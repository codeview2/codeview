####$$$$ cxxnet-master\cxxnet-master/.gitignore
*~
test*
*.o
*.conf
Local*
bin
mshadow
config.mk
layer.h
layer_impl-inl.hpp
.gitignore
*.so
im2bin
*.pyc
####$$$$ cxxnet-master\cxxnet-master/build.sh
#! /bin/bash
echo "Fetch mshadow..."
git clone https://github.com/dmlc/mshadow.git -b master
make $1
####$$$$ cxxnet-master\cxxnet-master/LICENSE
Copyright (c) 2014 Tianqi Chen and Bing Xu

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
    
   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
####$$$$ cxxnet-master\cxxnet-master/Makefile
ifndef config
ifdef CXXNET_CONFIG
	config = $(CXXNET_CONFIG)
else ifneq ("$(wildcard ./config.mk)","")
	config = config.mk
else
	config = make/config.mk
endif
endif

# use customized config file
include $(config)
include mshadow/make/mshadow.mk

# all tge possible warning tread
WARNFLAGS= -Wall
CFLAGS = -DMSHADOW_FORCE_STREAM $(WARNFLAGS)
CFLAGS += -g -O3 -I./mshadow/  -fopenmp -fPIC $(MSHADOW_CFLAGS)
LDFLAGS = -lz -pthread $(MSHADOW_LDFLAGS)
NVCCFLAGS = --use_fast_math -g -O3 -ccbin $(CXX) $(MSHADOW_NVCCFLAGS)

# setup opencv
ifeq ($(USE_OPENCV),1)
	CFLAGS+= -DCXXNET_USE_OPENCV=1
	LDFLAGS+= `pkg-config --libs opencv`
else
	CFLAGS+= -DCXXNET_USE_OPENCV=0
endif

ifeq ($(USE_OPENCV_DECODER),1)
	CFLAGS+= -DCXXNET_USE_OPENCV_DECODER=1
else
	CFLAGS+= -DCXXNET_USE_OPENCV_DECODER=0
endif

# customize cudnn path
ifneq ($(USE_CUDNN_PATH), NONE)
	CFLAGS += -I$(USE_CUDNN_PATH)
	LDFLAGS += -L$(USE_CUDNN_PATH)
endif
ifeq ($(USE_CUDNN), 1)
	CFLAGS += -DCXXNET_USE_CUDNN=1
	LDFLAGS += -lcudnn
endif

ifneq ($(ADD_CFLAGS), NONE)
	CFLAGS += $(ADD_CFLAGS)
endif
ifneq ($(ADD_LDFLAGS), NONE)
	LDFLAGS += $(ADD_LDFLAGS)
endif

# specify tensor path
BIN = bin/cxxnet
SLIB = wrapper/libcxxnetwrapper.so
OBJ = layer_cpu.o updater_cpu.o nnet_cpu.o data.o main.o nnet_ps_server.o
CUOBJ = layer_gpu.o  updater_gpu.o nnet_gpu.o
CUBIN =
ifeq ($(USE_CUDA), 0)
	CUDEP =
else
	CUDEP = $(CUOBJ)
endif

.PHONY: clean all

ifeq ($(USE_DIST_PS), 1)
BIN=bin/cxxnet.ps
endif

all: $(BIN) $(SLIB)

layer_cpu.o layer_gpu.o: src/layer/layer_impl.cpp src/layer/layer_impl.cu\
	src/layer/*.h src/layer/*.hpp src/utils/*.h src/plugin/*.hpp

updater_cpu.o updater_gpu.o: src/updater/updater_impl.cpp src/updater/updater_impl.cu\
	src/layer/layer.h src/updater/*.hpp src/updater/*.h src/utils/*.h

nnet_cpu.o nnet_gpu.o: src/nnet/nnet_impl.cpp src/nnet/nnet_impl.cu src/layer/layer.h\
	src/updater/updater.h src/utils/*.h src/nnet/*.hpp src/nnet/*.h

nnet_ps_server.o: src/nnet/nnet_ps_server.cpp src/utils/*.h src/nnet/*.hpp \
	src/nnet/*.h mshadow/mshadow-ps/*.h
data.o: src/io/data.cpp src/io/*.hpp

main.o: src/cxxnet_main.cpp

wrapper/libcxxnetwrapper.so: wrapper/cxxnet_wrapper.cpp $(OBJ) $(CUDEP)
bin/cxxnet: src/local_main.cpp $(OBJ) $(CUDEP)
bin/cxxnet.ps: $(OBJ) $(CUDEP) $(PS_LIB)

$(BIN) :
	$(CXX) $(CFLAGS)  -o $@ $(filter %.cpp %.o %.c %.a, $^) $(LDFLAGS)

$(OBJ) :
	$(CXX) -c $(CFLAGS) -o $@ $(firstword $(filter %.cpp %.c, $^) )

$(SLIB) :
	$(CXX) $(CFLAGS) -shared -o $@ $(filter %.cpp %.o %.c %.a %.cc, $^) $(LDFLAGS)

$(CUOBJ) :
	$(NVCC) -c -o $@ $(NVCCFLAGS) -Xcompiler "$(CFLAGS)" $(filter %.cu, $^)

$(CUBIN) :
	$(NVCC) -o $@ $(NVCCFLAGS) -Xcompiler "$(CFLAGS)" -Xlinker "$(LDFLAGS)" $(filter %.cu %.cpp %.o, $^)

clean:
	$(RM) $(OBJ) $(BIN) $(CUBIN) $(CUOBJ) $(SLIB) *~ */*~ */*/*~
####$$$$ cxxnet-master\cxxnet-master/README.md
cxxnet
======

CXXNET is a fast, concise, distributed deep learning framework.

Contributors: https://github.com/antinucleon/cxxnet/graphs/contributors

* [Documentation](doc)
* [Learning to use cxxnet by examples](example)
* User Group(TODO)

Feature Highlights
=====
* Lightweight: small but sharp knife
  - cxxnet contains concise implementation of state-of-art deep learning models
  - The project maintains a minimum dependency that makes it portable and easy to build
* Scale beyond single GPU and single machine
  - The library works on multiple GPUs, with nearly linear speedup
  - THe library works distributedly backed by disrtibuted parameter server
* Easy extensibility with no requirement on GPU programming
  - cxxnet is build on [mshadow](#backbone-library)
  - developer can write numpy-style template expressions to extend the library only once
  - mshadow will generate high performance CUDA and CPU code for users
  - It brings concise and readable code, with performance matching hand crafted kernels
* Convenient interface for other languages
  - Python interface for training from numpy array, and prediction/extraction to numpy array
  - Matlab interface (TODO)

### Backbone Library
CXXNET is built on [MShadow: Lightweight CPU/GPU Tensor Template Library](https://github.com/tqchen/mshadow)
* MShadow is an efficient, device invariant and simple tensor library
  - MShadow allows user to write expressions for machine learning while still provides
  - This means developer do not need to have knowledge on CUDA kernels to extend cxxnet.
* MShadow also provides a parameter interface for Multi-GPU and distributed deep learning
  - Improvements to cxxnet can naturally run on Multiple GPUs and being distributed

Build
=====
* Copy ```make/config.mk``` to root foler of the project
* Modify the config to adjust your enviroment settings
* Type ```./build.sh``` to build cxxnet
####$$$$ cxxnet-master\cxxnet-master\bin/README
This is where the binary file of cxxnet will be generated into
####$$$$ cxxnet-master\cxxnet-master\doc/global.md
#### Introduction
This page will introduce global setting in cxxnet, including:
* [Device Selection](#set-working-hardware)
* [Printing Control](#print-information)
* [Training Round](#set-round-of-training)
* [Saving Model and Continue Training](#saving-model-and-continue-training)
* [Prediction](#prediction)
* [Extract Feature](#extract-feature)
* [Fine-tune](#fine-tune)


#### Global setting
Global setting is to set parameters which are used globally. Global setting parameters are in any sections outside netconfig and iter. To understand better, you may find compare to the [MNIST.conf](example/MNIST.conf) can be helpful.

Indeed you can set any parameter in the global area. Local setting has higher privilege and is able to override the global setting if they are in same name.


#### Set Global Setting via Command Line
* Besides setting parameters via config file, cxxnet also support set parameters via command line. The syntax is like:
```bash
cxxnet_learner config.conf dev=gpu
```
Then the program will run in gpu mode. Note in command line mode, we must ensure ```dev=gpu``` contains no space so that they are passed in as single argument. The settings in command line will override settings in config file; this allows easy change of parameters via command line if you want to quick try different kinds of options.


#### Set working hardware
* To use CPU, set the field
```bash
dev = cpu
```
* To use GPU, set the field
```bash
dev = gpu
```
We can also set specific device (say device 1) by using
```bash
dev = gpu:1
```
* To use multi-GPU, set the field with the corresponding device id
```bash
dev = gpu:0,1,2,3
```
In default, it is `dev=gpu`


#### Print information
* To print training error evaluation, just set this field to 1
```bash
eval_train = 1
```
* in default this field is 0, which means cxxnet won't print anything about training error.
* To turn off all information while training, set this field to 1
```bash
silent = 1
```
* In default this field is 0
* To control print frequent, change this field
```bash
print_step = 100
```
* In default it will print every 100 batch


#### Set round of training
There are two field handle training round together: _**num_round**_ and _**max_round**_
* _**num_round**_ is used for number of round to train
* _**max_round**_ is used for maximum number of round to train from now on
```bash
num_round = 15
max_round = 15
```
This configuration will make cxxnet train for 15 rounds on the training data.

More examples,
```bash
num_round = 50
max_round = 2
```
If we have a model trained 40 rounds, then use this configuration continue to train, cxxnet will stop at the 42 round.


#### Saving model and continue training
* To save model while training round, set this field to saving frequent(a number)
```bash
save_model = 2
model_dir = path_of_dir_to_save_model
```
* In default, this field is 1, means cxxnet will save a model in every round
* To continue a training process, you need to set model_in as the input snapshot you want to continue from
```conf
model_in = path of model file
```
* Alternatively, if you save model every round (save_model=1), then you can use option continue, cxxnet will automatically search the latest model and start from that model
```conf
continue = 1
```
In default, if neither of the two values is set, cxxnet will start training from start.


#### Prediction
* In default, cxxnet treats the configuration file as a training configuration. To make it predict, you need to add extra data iterator and specify the task to be `pred` and model you want to use to do prediction. For example
```bash
# Data Iterator Setting
pred = pred.txt
iter = mnist
iterator_optition_1 = ..
iterator_optition_1 = ...
iter = end
# Global Setting
task = pred
model_in = ./models/0014.model
```
* In which the _*mode_in*_ is the path to the model which we need to use for prediction. The _*pred*_ field is the file we will save the result. The iterator configuration is same to traditional iterator.
* To get the softmax prediction directly, set the task to
```bash
task = pred_raw
```
#### Extract Feature
* To extract feature, you need to set task to ```extract_feature```with node name
```bash
task = extract_feature
extract_node_name = 45
```
If you didn't set name for layer, just use the number as name; If you set special name, use the name. For convenient, a special name ```top``` is used for extract topest layer behind loss layer.

#### Fine-tune
TODO
####$$$$ cxxnet-master\cxxnet-master\doc/io.md
#### Introduction
This page will introduce data input method in cxxnet. cxxnet use data iterator to provide data to the neural network.  Iterators do some preprocessing and generate batch for the neural network.

* We provide basic iterators for MNIST, CIFAR-10, Image, Binary Image.
* To boost performance, we provide thread buffer for loading.
  - Putting threadbuffer iterator after input iterator will open an independent thread to fetch from the input, this allows parallelism of learning process and data fetching.
  - We recommend you use thread buffer in all cases to avoid IO bottle neck.

Declarer the iterator in the form
```bash
iter = iterator_type
options 1 =
options 2 =
...
iter = end
```
* The basic iterator type is **mnist** , **image** , **imgbin**
* To use thread buffer, declare in this form
```bash
iter = iterator_type
options 1 =
options 2 =
...
iter = threadbuffer
iter = end
```
=
**Iterators**
* [MNSIT](#mnist-iterator)
* [Image and Image Binary](#image-and-image-binary-iterator)

=
##### Preprocessing Options
```bash
shuffle = 1
```
* **shuffle** set 1 to shuffle the **training data**. Note that this option **does not** apply to  **imgbin**.

=
##### MNIST Iterator
* Required fields
```bash
path_img = path to gz file of image
path_label = path to gz file of label
input_flat = 1
```
* **input_flat** means loading the data in shape 1,1,784 or 1,28,28
* You may check a full example [here](https://github.com/antinucleon/cxxnet/blob/master/example/MNIST/MNIST.conf)

=
##### Image and Image Binary Iterator
There are two ways to load images, image iterator that takes list of images in the disk, and image binary iterator that reads images from a packed binary file. Usually, I/O is a bottle neck, and image binary iterator makes training faster. However, we also provide image iterator for convenience


##### Image Iterator
* Required fields
```bash
image_list = path to the image list file
image_root = path to the image folder
```
###### Image list file
The **image_list** is a formatted file. The format is
```c++
image_index \t label \t file_name
```

A valid image list file is like the following (NO header):
```bash
1       0       cat.5396.jpg
2       0       cat.11780.jpg
3       1       dog.11254.jpg
4       0       cat.6791.jpg
5       0       cat.7937.jpg
6       1       dog.9329.jpg
```


* **image_root** is the path to the folder contains files in the image list file.

##### Image binary iterator
Image binary iterator aims to reduce to IO cost in random seek. It is especially useful when deal with large amount for data like in ImageNet.
* Required field
```bash
image_list = path to the image list file
image_bin = path to the image binary file
```
* The **image_list** file is described [above](#image-list-file)
* To generate **image_bin** file, you need to use the tool [im2bin](https://github.com/antinucleon/cxxnet/blob/master/tools/im2bin.cpp) in the tools folder.
* You may check an example [here](https://github.com/antinucleon/cxxnet/blob/master/example/ImageNet/ImageNet.conf)

#### Realtime Preprocessing Option for Image/Image Binary
```bash
rand_crop = 1
rand_mirror = 1
divideby = 256
image_mean = "img_mean.bin"
mean_value=255,255,255
min_crop_size=40
max_crop_size=80
max_aspect_ratio = 0.5
max_shear_ratio=0.3
max_rotate_angle=180
```

=
##### Common Parameters
* **divideby** normalize the data by dividing a value
* **image_mean** minus the image by the mean of all image. The value is the path of the mean image file. If the file doesn't exist, cxxnet will generate one.
* **mean_value** minus the image by the value specified in this field. Note that only one of **image_mean** and **mean_value** should be specified.

=
##### Random Augmenations
* **rand_crop** set 1 for randomly cropping image of size specified in **input_shape**. If set to 0, the iterator will only output the center crop.
* **rand_mirror** set 1 for random mirroring the **training data**
* **min_crop_size** and **max_crop_size** denotes the range of crop size. If they are not 0, the iterator will randomly pick _x_ in [min_crop_size, max_crop_size]. And then it will crop a region whose width and height are _x_. At last, the crop region is resize to **input_shape**.
* **max_aspect_ratio** denotes the max ratio of random aspect ratio augmentation. If it is not 0, the iterator will first random width in [min_crop_size, max_crop_size], and then random _aspect_ratio_ in [0, max_aspect_ratio]. The height is set to `y = max(min_crop_size, min(max_crop_size, x * (1 + aspect_ratio)))`. After cropping, the region is resized to **input_shape**.
* **max_shear_ratio** denotes the max random shearing ratio. In training, the image will be sheared randomly in [0, max_shear_ratio].
* **max_rotate_angle** denotes the random rotation angle. In training, the image will be rotated randomly in [-max_rotate_angle, max_rotate_angle].
* **rotate_list** specifies a list that input will rotate. e.g. `rotate_list=0,90,180,270` The input will only rotate randomly in the set.
* **max_random_contrast** denotes the range of random contrast variation. The output will be `y = (x - mean) * (1 + contrast)`, where `x` is the original image, and `contrast` is randomly picked in [-max_random_contrast, max_random_contrast]. **It will not take effect unless mean_value or mean_file specified.**
* **max_random_illumination** denotes the range of random illumination variation. The output will be `y = (x - mean) * contrast`, where `x` is the original image, and `illumination` is randomly picked in [-max_random_illumination, max_random_illumination]. **It will not take effect unless mean_value or mean_file specified.**

=
##### Deterministic Transformations
Deterministic transformations are usually used in test to generate diverse prediction results. Ensembling diverse prediction results could improve the performance.
* **crop_x_start** and **crop_y_start**  denotes the left corner of the crop.
* **mirror** denotes whether mirror the input.
* **rotate** denotes the angle will rotate.
####$$$$ cxxnet-master\cxxnet-master\doc/layer.md
#### Introduction
This page introduces the layer related configurations of cxxnet.

##### Layer Specification
All layer configurations comes into 
```bash
netconfig = start
layer[from->to] = layer_type:name
netconfig = end
```
* **from** is the from node name, 0 means input data 
* **to** is the to node name.
* **layer_type** is described below 
* _name_ is an optional, but if you need to finetune the network to other task, _name_ is a must, since it is used to indicate which layer to be copied.

##### Weight Initialization
_Fully_Connected_Layers_ and _Convolution_Layers_ require random weight initialization. We provide two initialization methods: gaussian and xaview:
```bash
random_type = gaussian
init_sigma = 0.01
```
We extra provide Xavier initialization method[1], by using the configuration
```bash
random_type = xavier
````

Global setting can be override in the layer configuration, eg
```bash
# global setting
random_type = gaussian
netconfig = start
wmat:lr  = 0.01
wmat:wd  = 0.0005
bias:wd  = 0.000
bias:lr  = 0.02
layer[0->1] = fullc:fc1
  # local setting start
  nhidden = 50
  random_type = xavier
  # local setting end 
layer[1->2] = relu
layer[2-3] = fullc
  # local setting start
  nhidden = 6
  init_sigma = 0.005
  wmat:lr = 0.1
  # local setting end
netconfig = end
```
By using this configuration, the `fc1` layer will use Xavier method to initialize, while fully connected layer without name will use Gaussian random number with `mu=0`, `sigma=0.005` to do initialization. Meanwhile fully connected layer without name will use a learning rate different with global.


=
#### Layer Types

=
**Connection Layer**
* [Flatten Layer](#flatten-layer)
* [Split Layer](#split-layer)
* [Concat Layer](#concat-layer)
* [Channel Concat Layer](#channel-concat-layer)

=
**Activation Layer**
* [Rectified Linear Layer](#rectified-linear)
* [Tanh Layer](#tanh)
* [Sigmoid Layer](#sigmoid)
* [Parametric ReLU layer](#parametric-rectified-linear)

=
**Loss Layer**
* [Softmax Layer](#softmax)
* [Euclidean Layer](#euclidean)
* [Elementwise Logistic Layer](#elementwise-logistic)

=
**Computation Layers**
* [Convolution Layer](#convolution-layer)
* [Fully Connected Layer](#fully-connected-layer) 

=
**Pooling Layers**
* [Max Pooling Layer](#max-pooling)
* [Sum Pooling Layer](#sum-pooling)
* [Average Pooling Layer](#average-pooling)

=
**Other Layers**
* [Dropout Layer](#dropout)
* [Local Response Normalization Layer](#local-response-normalization)
* [Batch Normalization Layer](#batch-normalization-layer)

=
#### Connection Layer

###### Flatten Layer
* **Flatten Layer** is used for flatten convolution layer. After flattening, we can use convolution output in the feed forward neural network. Namely, the shape of the output node is transformed to (batch, 1, 1, num_feature) instead of (batch, channel, width, height). Here is an example:
```bash
layer[15->16] = flatten
```

##### Split Layer
* **Split Layer** is used for one-to-multi connection. It duplicate the input node in forward pass, and accumulated the gradient from output nodes in backward pass.
```bash
layer[15->16,17] = split
```

##### Concat Layer
* **Concat Layer** is used to concatenate the last dimension (namely, _num_feature_) of the output of two nodes. It is usually used along with fully connected layer.
```bash
layer[18,19->20] = concat
```

##### Channel Concat Layer
* **Channel Concat Layer** is used to concatenate the second dimension (namely, _channel_) of the output of two nodes. It is usually used along with convolution layer.
```bash
layer[18,19->20] = ch_concat
```
=


#### Activation Layer
We provide common active layers including , _Rectified Linear_ (RELU), _Sigmoid_ , _Tanh_ and _Parametric_RELU_ (pRELU).

=
###### Rectified Linear
* The output of **Rectified Linear** is _max(0, x)_. This is the most commonly used activation function in modern deep learning method. 
```bash
layer[15->16] = relu
```

=
###### Tanh
* **Tanh** uses the _tanh_ as activation function. It transforms the input into range [-1, 1].
```bash
layer[15->16] = tanh
```

=
###### Sigmoid 
* **Sigmoid** uses the _sigmoid_ as activation function. It transforms the input into range [0, 1].
```bash
layer[15->16] = sigmoid
```

=
###### Parametric Rectified Linear
* **pRELU** is basically the implementation of [2]. In addition, we provide a parameter to add noise to the negative slope to reduce overfitting.
```bash
layer[15->16] = prelu
  random=0.5
```
* **random**[optional] denotes standard deviation of the gaussian distribution randomly added to the negative part of pRELU. In testing, this noise part is discarded.

=
##### Loss Layer
Loss layers are self-looped layer. It defines the loss function for training. 
* Common Parameters:
* grad_scale[optional]: scale the gradient generated by loss layer

=
###### Softmax
* **Softmax** Loss Layer is the implementation of multi-class softmax loss function.

= 
###### Euclidean
* **Euclidean** Loss Layer is the implementation of elementwise l2 loss function.

=
###### Elementwise Logistic
* **Elementwise Logistic** Loss Layer is the implementation of elementwise logistic loss function. It is suitable to multi-label classification problem.


=
#### Computation Layers

###### Fully Connected Layer
* **Fully Connection Layer** fully connection layer is the basic element in feed forward neural network.
```bash
layer[18->19] = fullc
  nhidden = 1024
```
* **nhidden** denotes the number of hidden units in the layer.

=
##### Convolution Layer
If built with CuDNN, the default convolution is CuDNN R2. If there is no CuDNN R2, convolution will be run on our own kernel. The configuration looks like
```bash
layer[0->1] = conv
  kernel_size = 11
  stride = 4
  nchannel = 96
  pad = 1
```
* **kernel_size** is the convolution kernel size
* **stride** is stride for convolution operation
* **nchannel** is the output channel
* **pad** is the number of pad
* **temp_col_max**[optional] is the maximum size of expanding in convolution operation. The default value is 64, means the maximum size of temp_col is 64MB. Adjusting this variable may boost speed in training especially the input size is small in the convolution network. Note that this will only take effect when not using CuDNN.

=
#### Pooling Layer
Currectly we provide three Pooling methods: _Sum Pooling_ , _Max Pooling_ and _Average Pooling_ .
All pooling layers shared same parameters: _stride_ and _kernel_size_

=
###### Sum Pooling
* **Sum Pooling** sums up the values in the pooling region as result , eg
```bash
layer[4->5] = sum_pooling
  kernel_size = 3
  stride = 2
```

###### Max Pooling
* **Max Pooling** takes the maximum value in the pooling region as result, eg
```bash
layer[4->5] = max_pooling
  kernel_size = 3
  stride = 2
```

###### Average Pooling
* **Average Pooling** averages the values in the pooling region as result , eg
```bash
layer[4->5] = avg_pooling
  kernel_size = 3
  stride = 2
```

=
#### Other Layers

###### Dropout
* Note that **Dropout** Layer is a self loop layer. You need to set _to_ equal the _from_, eg
```bash
layer[3->3] = dropout:dp
  threshold = 0.5
```
* **threshold** is the probability to drop an output.

=
###### Local Response Normalization
LRN normalizes the response of nearby kernels. Details can be found in the Alex's paper[3].

```bash
layer[3->4] = lrn
  local_size = 5
  alpha = 0.001
  beta = 0.75
  knorm = 1
``` 
* **local_size** denotes the nearby kernel size to be evaluated 
* **alpha, beta and knorm** is normalization param.

=
###### Batch Normalization Layer
BN layer is an implementation of [4]. The difference is that in testing, we only use the mini-batch statistics instead of global statistics in training data as in original paper. _It is an experimental layer that may not stable._

=
#### References
[1] Glorot Xavier, and Yoshua Bengio. "Understanding the difficulty of training deep feedforward neural networks." AISTATS. 2010.

[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification." arXiv preprint arXiv:1502.01852. 2015.

[3] Krizhevsky Alex, Ilya Sutskever, and Geoffrey E. Hinton. "Imagenet classification with deep convolutional neural networks." NIPS. 2012.

[4] Ioffe Sergey, and Christian Szegedy. "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift." arXiv preprint arXiv:1502.03167. 2015.####$$$$ cxxnet-master\cxxnet-master\doc/multigpu.md
Multi-GPU / Distributed Training
======
This page contains

[Set Multi-GPU in configuration file]()

[Make cxxnet work in distributed system]()

[How it works]()

[Reference]()

### Set Multi-GPU in configuration file
* To use multi-GPU, set the field with the corresponding device id
```bash
dev = gpu:0,1,2,3
```
or
```bash
dev = gpu:0-3
```
which indicate cxxnet will use the first four GPU to do the training task

### Make cxxnet work in distributed system


### How it works
Parameter Server is the backend of multi-gpu / distributed training part of cxxnet. For multi-gpu, the parameter is running on local machine so you don't need to set mannually.

For distributed case TODO



### Reference
####$$$$ cxxnet-master\cxxnet-master\doc/other.md
#### Introduction
This page will introduce other setting in cxxnet, including:
* [Device Selection](#set-working-hardware)
* [Printing Control](#print-information)
* [Training Round](#set-round-of-training)
* [Saving Model and Continue Training](#saving-model-and-continue-training)


#### Set working hardware
* To use CPU, set the field
```bash
dev = cpu
```
* To use GPU, set the field
```bash
dev = gpu
```
We can also set specific device (say device 1) by using
```bash
dev = gpu:1
```
* To use multi-GPU, set the field with the corresponding device id
```bash
dev = gpu:0,1,2,3
```
or
```bash
dev = gpu:0-3
```
In default, it is `dev=gpu`


#### Print information
* To print training error evaluation, just set this field to 1
```bash
eval_train = 1
```
* in default this field is 0, which means cxxnet won't print anything about training error.
* To turn off all information while training, set this field to 1
```bash
silent = 1
```
* In default this field is 0
* To control print frequent, change this field
```bash
print_step = 100
```
* In default it will print every 100 batch


#### Set round of training
There are two field handle training round together: _**num_round**_ and _**max_round**_
* _**num_round**_ is used for number of round to train
* _**max_round**_ is used for maximum number of round to train from now on
```bash
num_round = 15
max_round = 15
```
This configuration will make cxxnet train for 15 rounds on the training data.

More examples,
```bash
num_round = 50
max_round = 2
```
If we have a model trained 40 rounds, then use this configuration continue to train, cxxnet will stop at the 42 round.


#### Saving model and continue training
* To save model while training round, set this field to saving frequent(a number)
```bash
save_model = 2
model_dir = path_of_dir_to_save_model
```
* In default, this field is 1, means cxxnet will save a model in every round
* To continue a training process, you need to set model_in as the input snapshot you want to continue from
```conf
model_in = path of model file
```
* Alternatively, if you save model every round (save_model=1), then you can use option continue, cxxnet will automatically search the latest model and start from that model
```conf
continue = 1
```
In default, if neither of the two values is set, cxxnet will start training from start.####$$$$ cxxnet-master\cxxnet-master\doc/python.md
Python Interface example
====

CXXNET provides convenient interface for python user. We are able use Numpy array as training data source to train your network by using CXXNET in backend, we also are able extract feature into numpy array,

### Outline
* Import CXXNET
* Train a network by using ```train``` function
* Update the network
* Save and Load model
* Make prediction and evaluation
* Extract feature
* Advanced usage: ```Net``` object
* Advanced usage: ```Iterator``` object

Example script for MNIST can be found at [mnist.py](../example/MNIST/mnist.py)


### Import CXXNET
To import CXXNET into python, you need to add wrapper folder path to system path, eg
```python
CXXNET_WRAPPER_PATH = "/home/cxxnet/wrapper/"
import sys
sys.path.append(CXXNET_WRAPPER_PATH)
import cxxnet

```
### Train a network
CXXNET provides a simple method ```train``` to train a network. By following these steps, we can train a network in python.

#### Declare Network
Network structure is declared by configuration string, then we use the string to generate ```Net``` object for later task. The configuration format is same to original cxxnet, which can be found [here](layer.md)

For example, here is a simple network for MNIST.
```python
cfg = """
netconfig=start
layer[+1:fc1] = fullc:fc1
  nhidden = 100
  init_sigma = 0.01
layer[+1:sg1] = sigmoid:se1
layer[sg1->fc2] = fullc:fc2
  nhidden = 10
  init_sigma = 0.01
layer[+0] = softmax
netconfig=end

input_shape = 1,1,784
batch_size = 100

random_type = gaussian
"""
```
#### Declare Data
We can use both ```Iterator``` object and numpy ndarray as the training/evaluation data. We will discuss the ```Iterator``` in advanced feature, here we can use numpy ndarray directly.

```python
dtrain = data[0:1000, :] # training data
deval = data[1000:1500, :] # validation data
ltrain = label[0:1000, :]  # label for train
leval = label[1000:1500, :] #label for validation
```
For mini-batch data in numpy, we need some knowledge of ```Net``` object and ```update``` function in ```Net``` object so we will discuss it later.

#### Set Parameters
Parameters can be set in dictionary or list of tupple.
For example:
```python
param = {}
param['eta'] = 0.1
param['dev'] = 'cpu'
param['momentum'] = 0.9
param['metric'] = 'error'
```

#### Train this network, get the ```Net``` object
To train a net, there is a simple function ```cxxnet.train```. in our example, we can get a ```Net``` object in this way:
```python
num_round = 10
net = cxxnet.train(cfg=cfg, data=dtrain, label=ltrain, num_round, param)
```
Then in backend, cxxnet will be called and return the ```Net``` object for later advaced use.

### Update the network
We can use new data to update existing network. the data can be ```Iterator``` object or Numpy ndarray.
For example, when we have a ```Net``` object, we can use update to do mini-batch training like this:
```python
batch_size = 64
batch_num = data.shape[0] / batch_size
for i in xrange(batch_num):
  j = min((i + 1) * batch_size, data.shape[0])
  dbatch = data[i * batch_size : j, :]
  lbatch = label[i * batch_size : j, :]
  net.update(data=dbatch, label=lbatch)

```
To get ```Net``` object directly, we will discuss it in later advaced usage.
### Save and Load the network
We can save current network weights by calling
```python
file_name = "current.model"
net.save_model(file_name)
```
We can load existing model weight by calling
```python
file_name = "current.model"
net.load_model(file_name)
```
Please make sure both the ```Net``` object and existing model are in same network structure.

### Make Prediction and Evaluation
We can make prediction form net by using ```Iterator``` or Numpy ndarray. The prediction is stored in a numpy ndarray,
```python
dtest = data[1500:2000,:]
pred = net.predict(dtest)
```
For ```Iterator```, there is special function for evalutation in advanced usage. For numpy data, we can write an evaluation function to get the evaltion with prediction and label, eg
```python
werr = np.sum(label[:,0] !=  pred[:])
print "Error: %f" % werr
```
### Extract feature
To extract feature, we need both data and the node name which we will do extraction. A special node name format is ```top[-x]```, eg.
```python
raw_probability = net.extract(data, "top[-1]")
feature = net.extract(data, "fc7")
```
### Advanced usage: ```Net``` object
```Net``` object is able to be built by using ````train``` function, or we can initialize a network with configuration then train it by ourselves.

To get an ```Net``` object, we can do in this way
```python
cfg = """
netconfig=start
layer[+1:fc1] = fullc:fc1
  nhidden = 100
  init_sigma = 0.01
layer[+1:sg1] = sigmoid:se1
layer[sg1->fc2] = fullc:fc2
  nhidden = 10
  init_sigma = 0.01
layer[+0] = softmax
netconfig=end

input_shape = 1,1,784
batch_size = 100

random_type = gaussian
"""

net = cxxnet.Net(dev="gpu", cfg=cfg)
net.init_model()
```
Then we can use this net object just like previous net object to do update/predict/load/save/extract task.
We can also get/set weigth/bias for special layer in net.
eg:
```python
# get weight
weights = []
for layer in ['fc1', 'fc2']:
    for tag in ['wmat', 'bias']:
        weights.append((layer, tag, net.get_weight(layer, tag)))

# set weight
for layer, tag, wt in weights:
    net.set_weight(wt, layer, tag)
```
### Advanced usage: ```Iterator``` object
For large training task, for example, ImageNet training, we suggest to use CXXNET original iterator instead of training by numpy array directly because iterator is designed and implemented for best performance. To get an object, ```Iterator``` is very similar to ```Net```.
```python
data = cxxnet.DataIter("""
iter = mnist
    path_img = "./data/train-images-idx3-ubyte.gz"
    path_label = "./data/train-labels-idx1-ubyte.gz"
    shuffle = 1
iter = end
input_shape = 1,1,784
batch_size = 100
""")
print 'init train iter'

deval = cxxnet.DataIter("""
iter = mnist
    path_img = "./data/t10k-images-idx3-ubyte.gz"
    path_label = "./data/t10k-labels-idx1-ubyte.gz"
iter = end
input_shape = 1,1,784
batch_size = 100
""")
```
There is a special ```train``` function for input is iterator, it is like:
```
net = train(cfg, data, num_round, param, eval_data = deval)
```
For update/predict/extract, you can use iterator object directly just like use numpy ndarray. The ```Net``` will get current batch of iterator then do the task. For evaluation, there is a special function ```evaluate``` for iterator. The ```evaluate``` function only accept iterator as input and automatically evaluate all batches in the iterator.

```python
# first parameter is iterator
# second parameter is name for this evaluation
print net.evaluate(deval, "test")
```

To go through all data by batch by using iterator, there are 3 useful functions:
```python
#reset the iterator to beginning
deval.before_first()

# get next batch, return true if success, false for iterator reaches end
deval.next()

# check whether it is at head/tail of current iterator
deval.check_valid()

```
To get current batch data or label to a numpy ndarray, we can use:
```
dbatch = deval.get_data()
lbatch = deval.get_label()
```
Here is an example to go through all batches in iterator and update the network
```python
data.before_first()
while data.next():
  net.update(data)

```
####$$$$ cxxnet-master\cxxnet-master\doc/README.md
Document Home
====
This is the documentation for cxxnet

Links of Resources
* [Learning CXXNET by Examples](../example)
* [Python Interface](python.md)
* [Multi-GPU/Distributed Training](multigpu.md)

Configuration of CXXNET
====
This section introduces the how to setup configuation file of cxxnet.
In general, cxxnet configuration file contains four kinds of configurations in a single file:
* [Data Input Iterator Setting](io.md)
  - Set input data configurations.
* [Layer Setting](layer.md)
  - Configure network, and setup each layers.
* [Updater Setting](updater.md)
  - Set parameters(learning rate, momentum) for learning procedure
* [Tasks](tasks.md)
  - This page includes all the four tasks you could try by cxxnet.
* [Other Setting](other.md)
  - Set other parameters for neural network, related to device selection, running control.
####$$$$ cxxnet-master\cxxnet-master\doc/tasks.md
#### Introduction
This page will introduce the tasks supported in cxxnet, including:

* [Train](#train)
* [Predict](#predict)
* [Extract Features](#extract-features)
* [Finetune](#finetune)

####Train
* Train is the basic task for cxxnet. If you don't specify the task in global configuration, the task is train by default.
* To use ```train```, you must specify a data iterator to indicate the training data, which starts with ```data = train``` e.g.
```bash
data = train
iter = mnist
    iterator_optition_1 = ..
	iterator_optition_2 = ...
iter = end
```

You can also specify data iterators for evaluation if needed. The iterator should start with ```eval = iter_name``` e.g.
```bash
eval = test
iter = mnist
    path_img = "./data/t10k-images-idx3-ubyte.gz"
    path_label = "./data/t10k-labels-idx1-ubyte.gz"
iter = end
```
More details about iterator, please refer to [**Data Iterator**](io.md).

#### Predict
* In default, cxxnet treats the configuration file as a training configuration. To make it predict, you need to add extra data iterator and specify the task to be `pred` and model you want to use to do prediction. For example
```bash
# Data Iterator Setting
pred = pred.txt
iter = mnist
	iterator_optition_1 = ..
	iterator_optition_2 = ...
iter = end
# Global Setting
task = pred
model_in = ./models/0014.model
```
* In which the _*mode_in*_ is the path to the model which we need to use for prediction. The _*pred*_ field is the file we will save the result. The iterator configuration is same to traditional iterator.

#### Extract Features
* To extract feature, you need to set task to ```extract```with node name or distance to top. ```model_in``` is also required to specify the model to use.
```bash
task = extract
extract_node_name = 45
model_in = ./models/0014.model
```
```bash
task = extract_feature
extract_node_name = top[-1]
model_in = ./models/0014.model
# this will extract last node, namely the softmax prediction.
```

For convenient, a special name ```top``` is used for extract topest layer behind loss layer.


#### Finetune
To use finetune, you need to set ```task=finetune``` and ```model_in``` parameters in your global setting. Other parts are the same as task train. Note that finetune task will copy the parameters in the old network to the new one in the case that their layer names are exactly same. All other parts are initialized randomly. Note that ***You cannot copy a layer without a name.*** So it is a best practice that you add name for each layer, though it is not a must.


####$$$$ cxxnet-master\cxxnet-master\doc/updater.md
#### Introduction 
This page introduce updater setting for cxxnet

* [General SGD Updater](#updater)
* [Constant Learning Rate Scheduling](#constant-scheduling)
* [Exp Decay Learning Rate Scheduling](#exp-decay)
* [Poly Decay Learning Rate Scheduling](#poly-decay)
* [Factor Decay Learning Rate Scheduling](#factor-decay)

#### Updater
In default, the cxxnet will use the SGDUpdater.
The `eta`, `wd` and `momentum` can be set differently to `wmat` or `bias` (namely, the weight and bias in each layer), by configure 
```bias
wmat:eta = 0.1
bias:eta = 0.2
```
If not specify the target, the setting will take effect globally.  
* Basic configuration:
```bash
updater = sgd
eta = 0.01
momentum = 0.9
wd = 0.0005
```
* **eta** is known as learning rate, default is 0.01 
* **momentum** is momentum, default is 0.9
* **wd** is known as weight decay (l2 regularization), default is 0.005

* Global updater setting can be **overridden** in the layer configuration. eg.
```bash
# Global setting
eta = 0.01
momentum = 0.9
# Layer setting
netconfig=start
layer[0->1] = fullc:fc1
  nhidden = 100
  eta = 0.02
  momentum = 0.5
layer[1->2] = sigmoid:se1
layer[2->3] = fullc:fc1
  nhidden = 10
layer[3->3] = softmax
netconfig=end
```
In the layer `fc1`, the learning rate will be `0.02` and momentum will be `0.5`, but layer `fc2` will follow the global setting, whose learning rate will be `0.01` and momentum will be `0.9`


=
#### Learning Rate Scheduling
There are some advanced features for SGDUpdater, like learning rate scheduling. We provides four learning rate scheduling method: _constant_ , _expdecay_ and _polydecay_ and _factor_.

#### Common parameters
* **lr:start_epoch** start learning rate scheduling after iteration, default is 0. Before that, we use constant learning rate specified in _eta_.
* **lr:minimum_lr** minimum of learning rate, default is 0.0001
* **lr:step** the parameter used in each scheduling method, elaborate in each method

#### Constant Scheduling
* Example of **constant** scheduling: In this way the learning rate keep same
```bash
updater = sgd
eta = 0.01
momentum = 0.9
wd = 0.0005
lr:schedule = constant
```

#### Exp Decay
Exponential Learning rate decay adjust learning rate like this formula:
`new_learning_rate = base_learning_rate * pow(gamma, iteration / step )`
* Example of **expdecay** scheduling: In this way the learning rate drop in exponential way
```bash
updater = sgd
eta = 0.01
momentum = 0.9
wd = 0.0005
lr:schedule = expdecay
lr:start_iteration = 3000
lr:minimum_lr = 0.001
lr:gamma = 0.5
lr:step = 1000
```
* **lr:gamma** learning decay param, default is 0.5

#### Poly Decay
Polynomial learning rate decay adjusts the learning rate like this formula:
`new_learning_rate = base_learning_rate * pow( 1.0 + (iteration/step) * gamma, -alpha )` 
* Example of **polydecay** scheduling: In this way the learning rate drop in polynomial way
```bash
updater = sgd
eta = 0.01
momentum = 0.9
wd = 0.0005
lr:schedule = polydecay
lr:start_epoch = 3000
lr:minimum_lr = 0.001
lr:alpha = 0.5
lr:gamma = 0.1
lr:step = 1000
```
* **lr:gamma** learning decay param, default is 0.5
* **lr:alpha** learning decay param, default is 0.5

#### Factor Decay
Factor Decay multiplies the learning rate by _factor_ each _step_ iterations. It is also called step scheduling.
* Example of **factor** scheduling:
```bash
updater = sgd
eta = 0.01
momentum = 0.9
wd = 0.0005
lr:schedule = factor
lr:minimum_lr = 0.001
lr:factor = 0.1
lr:step = 10000
```
* **lr:factor** learning decay param, default is 0.1
####$$$$ cxxnet-master\cxxnet-master\example/README.md
CXXNET Examples
====
This folder contains all the code examples using cxxnet
* Contribution of examples, benchmarks is more than welcome!
* If you like to share how you use xgboost to solve your problem, send a pull request:)

List of examples
====
* [Feature Walk-through by using MNIST](MNIST)
  - This is a basic steps of cxxnet
* [Kaggle National Data Science Bowl Example](kaggle_bowl)
  - This is an example to show you how to solve a real kaggle problem by using cxxnet.
  - This example also show basic steps to using convolution neural network.
* [ImageNet Example](ImageNet)
  - This is a step by step example to show how to use cxxnet train an AlexNet for ImageNet task.
  - We also provides better reference pre-trained model with all configurations.
####$$$$ cxxnet-master\cxxnet-master\example\ImageNet/ImageNet.conf
# Configuration for ImageNet  
# Acknowledgement:
#  Ref: http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf
#  The scheduling parameters is adapted from Caffe(http://caffe.berkeleyvision.org/)

data = train
iter = imgbin
  image_list = "../../NameList.train"
  image_bin  = "../../TRAIN.BIN"
  image_root = "../../data/resize256/"
  image_mean = "models/image_net_mean.bin"
  rand_crop=1
  rand_mirror=1
iter = threadbuffer
iter = end

eval = test
iter = imgbin
  image_list = "../../NameList.test"
  image_bin = "../../TEST.BIN"
  image_root = "../../data/resize256/"
  image_mean = "models/image_net_mean.bin"
# no random crop and mirror in test
iter = end

netconfig=start
layer[0->1] = conv
  kernel_size = 11
  stride = 4
  nchannel = 96
layer[1->2] = relu
layer[2->3] = max_pooling
  kernel_size = 3
  stride = 2
layer[3->4] = lrn
  local_size = 5
  alpha = 0.001
  beta = 0.75
  knorm = 1
###############
layer[4->5] = conv
  ngroup = 2
  nchannel = 256
  kernel_size = 5
  pad = 2
layer[5->6] = relu
layer[6->7] = max_pooling
  kernel_size = 3
  stride = 2
layer[7->8] = lrn
  local_size = 5
  alpha = 0.001
  beta = 0.75
  knorm = 1
#############
layer[8->9] = conv
  nchannel = 384
  kernel_size = 3
  pad = 1
layer[9->10]= relu
layer[10->11] = conv
  nchannel = 384
  ngroup = 2
  kernel_size = 3
  pad = 1
layer[11->12] = relu
layer[12->13] = conv
  nchannel = 256
  ngroup = 2
  kernel_size = 3
  pad = 1
  init_bias = 1.0
layer[13->14] = relu
layer[14->15] = max_pooling
  kernel_size = 3
  stride = 2
layer[15->16] = flatten
layer[16->17] = fullc
  nhidden = 4096
  init_sigma = 0.005
  init_bias = 1.0
layer[17->18] = relu
layer[18->18] = dropout
  threshold = 0.5
layer[18->19] = fullc
  nhidden = 4096
  init_sigma = 0.005
  init_bias = 1.0
layer[19->20] = relu
layer[20->20] = dropout
  threshold = 0.5
layer[20->21] = fullc
  nhidden = 1000
layer[21->21] = softmax
netconfig=end

# evaluation metric
metric = error
metric = rec@1
metric = rec@5

max_round = 45
num_round = 45

# input shape not including batch
input_shape = 3,227,227

batch_size = 256

# global parameters in any sectiion outside netconfig, and iter
momentum = 0.9
wmat:lr  = 0.01
wmat:wd  = 0.0005

bias:wd  = 0.000
bias:lr  = 0.02

# all the learning rate schedule starts with lr
lr:schedule = expdecay
lr:gamma = 0.1
lr:step = 100000

save_model=1
model_dir=models

# random config
random_type = xavier


# new line
####$$$$ cxxnet-master\cxxnet-master\example\ImageNet/README.md
ImageNet Tutorial
=====

This is a step by step example of AlexNet for ImageNet Challenge.

### Introduction
This tutorial will guide you train your own super vision model. The default configure will take more than 3GB GPU RAM, so make batch size smaller or larger according to your GPU RAM size.

* Normally, smaller batch_size means more noise in gradient, and maybe a smaller learning rate is needed.
* If you want to still use a large batch with not enough RAM, You can set ```update_period=2``` and ```batch_size=128```, this means the parameter update is done every 2 batches, which is equivalent to ```batch_size=256```

### 0.Before you start
Make sure you have downloaded the ImageNet training data. Resize the picture into size 256 * 256 *3 for later we will crop random 227 * 227 * 3 image while training.

### 1.Make the image list
After you get the data, you need to make a [image list file](../../doc/io.md#image-list-file) first.  The format is
```
integer_image_index \t label_index \t path_to_image
```
In general, the program will take a list of names of all image, shuffle them, then separate them into training files name list and testing file name list. Write down the list in the format.

A sample file is provided here
```bash
895099  464     n04467665_17283.JPEG
10025081        412     ILSVRC2010_val_00025082.JPEG
74181   789     n01915811_2739.JPEG
10035553        859     ILSVRC2010_val_00035554.JPEG
10048727        929     ILSVRC2010_val_00048728.JPEG
94028   924     n01980166_4956.JPEG
1080682 650     n11807979_571.JPEG
972457  633     n07723039_1627.JPEG
7534    11      n01630670_4486.JPEG
1191261 249     n12407079_5106.JPEG

```

### 2.Make the binary file
Although you can use image iterator now. the disk random seek will make the training process extremely slow. So **you'd better generate binary file for training and use imgbin iterator** .

To generate binary image, you need to use *im2bin* in the tool folder. The im2bin will take the path of _image list file_ you generated just now, _root path_ of the images and the _output file path_ as input. These processes usually take several hours, so be patient. :)

A sample command:
```bash
im2bin ./train.lst ./resized256_images/ TRAIN.BIN
```
### 3.Set correct configuration file
Change the iterator path in the [ImageNet.conf](ImageNet.conf) to point to your _image list file_ and _image binary file_ correctly, then just run as MNIST example. After about 20 round, you can see some reasonable result.
By calling
```bash
cxxnet ./ImageNet.conf 2>eval.log
```
You can save the evaluation result into the file `eval.log`



=
### Acknowledgment
* Reference: Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. "ImageNet Classification with Deep Convolutional Neural Networks." NIPS. Vol. 1. No. 2. 2012.
* The network parameter scheduling is adapted from configuration provided by [Caffe](http://caffe.berkeleyvision.org/)
####$$$$ cxxnet-master\cxxnet-master\example\kaggle_bowl/bowl.conf
# Configuration for ImageNet
# Acknowledgement:
#  Ref: http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf
#  The scheduling parameters is adapted from Caffe(http://caffe.berkeleyvision.org/)

dev = cpu

data = train
iter = imgbin
  image_list = "./tr.lst"
  image_bin  = "./tr.bin"
  image_mean = "models/image_mean.bin"
  rand_mirror=1
  rand_crop=1
  max_rotate_angle=180
  max_aspect_ratio = 0.5
  max_shear_ratio = 0.3
  min_crop_size=32
  max_crop_size=48
iter = threadbuffer
iter = end

eval = val
iter = imgbin
  image_list = "./va.lst"
  image_bin  = "./va.bin"
  image_mean = "models/image_mean.bin"
iter = threadbuffer
iter = end


netconfig=start
layer[+1] = conv
  kernel_size = 4
  stride = 1
  nchannel = 48
  pad = 2
layer[+1] = relu
layer[+1] = max_pooling
  kernel_size = 3
  stride = 2
###############
layer[+1] = conv
  nchannel = 96
  kernel_size = 3
  stride = 1
  pad = 1
layer[+1] = relu
layer[+1] = conv
  nchannel = 96
  kernel_size = 3
  stride = 1
  pad = 1
layer[+1] = relu
layer[+1] = max_pooling
  kernel_size = 3
  stride = 2
##############
layer[+1] = conv
  nchannel = 128
  kernel_size = 2
  stride = 1
layer[+1] = relu
layer[+1] = conv
  nchannel = 128
  kernel_size = 3
  stride = 1
layer[+1] = max_pooling
  kernel_size = 3
  stride = 2
layer[+1] = flatten
layer[+1] = fullc
  nhidden = 256
layer[+0] = dropout
  threshold = 0.5
layer[+1] = fullc
  nhidden = 121
layer[+0] = softmax
netconfig=end

# evaluation metric
metric = error

dev = gpu:1
max_round = 100
num_round = 100

# input shape not including batch
input_shape = 3,40,40

batch_size = 64

# global parameters in any sectiion outside netconfig, and iter
momentum = 0.9
wmat:lr  = 0.001
wmat:wd  = 0.0005

bias:wd  = 0.000
bias:lr  = 0.002

# all the learning rate schedule starts with lr
lr:schedule = expdecay
lr:gamma = 0.1
lr:step = 20000

save_model=1
model_dir=models
print_step = 1
# random config
random_type = xavier
init_sigma = 0.01

# new line
####$$$$ cxxnet-master\cxxnet-master\example\kaggle_bowl/gen_img_list.py
import csv
import os
import sys
import random

if len(sys.argv) < 4:
    print "Usage: gen_img_list.py train/test sample_submission.csv train_folder img.lst"
    exit(1)

random.seed(888)

task = sys.argv[1]
fc = csv.reader(file(sys.argv[2]))
fi = sys.argv[3]
fo = csv.writer(open(sys.argv[4], "w"), delimiter='\t', lineterminator='\n')

# make class map
head = fc.next()
head = head[1:]

# make image list
img_lst = []
cnt = 0
if task == "train":
    for i in xrange(len(head)):
        path = fi + head[i]
        lst = os.listdir(fi + head[i])
        for img in lst:
            img_lst.append((cnt, i, path + '/' + img))
            cnt += 1
else:
    lst = os.listdir(fi)
    for img in lst:
        img_lst.append((cnt, 0, fi + img))
        cnt += 1

# shuffle
random.shuffle(img_lst)

#wirte
for item in img_lst:
    fo.writerow(item)
####$$$$ cxxnet-master\cxxnet-master\example\kaggle_bowl/gen_test.py
import os
import sys
import subprocess

if len(sys.argv) < 3:
    print "Usage: python gen_test.py input_folder output_folder"
    exit(1)

fi = sys.argv[1]
fo = sys.argv[2]

cmd = "convert -resize 48x48\! "
imgs = os.listdir(fi)


for img in imgs:
    md = ""
    md += cmd
    md += fi + img
    md += " " + fo + img
    os.system(md)



####$$$$ cxxnet-master\cxxnet-master\example\kaggle_bowl/gen_train.py
import os
import sys
import subprocess

if len(sys.argv) < 3:
    print "Usage: python gen_train.py input_folder output_folder"
    exit(1)

fi = sys.argv[1]
fo = sys.argv[2]

cmd = "convert -resize 48x48\! "
classes = os.listdir(fi)

os.chdir(fo)
for cls in classes:
    try:
        os.mkdir(cls)
    except:
        pass
    imgs = os.listdir(fi + cls)
    for img in imgs:
        md = ""
        md += cmd
        md += fi + cls + "/" + img
        md += " " + fo + cls + "/" + img
        os.system(md)



####$$$$ cxxnet-master\cxxnet-master\example\kaggle_bowl/gen_tr_va.sh
sed -n '1,20000p' $1 > tr.lst
sed -n '20000, 40000p' $1 > va.lst
####$$$$ cxxnet-master\cxxnet-master\example\kaggle_bowl/make_submission.py
import csv
import sys

if len(sys.argv) < 4:
    print "Usage: python make_submission.py sample_submission.csv test.lst text.txt out.csv"
    exit(1)

fc = csv.reader(file(sys.argv[1]))
fl = csv.reader(file(sys.argv[2]), delimiter='\t', lineterminator='\n')
fi = csv.reader(file(sys.argv[3]), delimiter=' ', lineterminator='\n')
fo = csv.writer(open(sys.argv[4], "w"), lineterminator='\n')

head = fc.next()
fo.writerow(head)

head = head[1:]

img_lst = []
for line in fl:
    path = line[-1]
    path = path.split('/')
    path = path[-1]
    img_lst.append(path)

idx = 0
for line in fi:
    row = [img_lst[idx]]
    idx += 1
    line = line[:-1]
    row.extend(line)
    fo.writerow(row)

    

####$$$$ cxxnet-master\cxxnet-master\example\kaggle_bowl/pred.conf
# Configuration for ImageNet
# Acknowledgement:
#  Ref: http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf
#  The scheduling parameters is adapted from Caffe(http://caffe.berkeleyvision.org/)

data = train
iter = imgbin
  image_list = "./train.lst"
  image_bin  = "./train.bin"
  image_mean = "models/image_mean.bin"
  rand_mirror=1
iter = threadbuffer
iter = end

pred = test.txt
iter = imgbin
  image_list = "./test.lst"
  image_bin  = "./test.bin"
  image_mean = "models/image_mean.bin"
iter = threadbuffer
iter = end

task = pred_raw
model_in = ./models/0045.model


netconfig=start
layer[0->1] = conv
  kernel_size = 5
  stride = 4
  nchannel = 96
  pad = 2
layer[1->2] = relu
layer[2->3] = max_pooling
  kernel_size = 3
  stride = 2
###############
layer[3->4] = conv
  nchannel = 128
  kernel_size = 3
  pad = 2
layer[4->5] = relu
#############
layer[5->6] = conv
  nchannel = 128
  kernel_size = 3
  pad = 1
layer[6->7] = relu
layer[7->8] = max_pooling
  kernel_size = 3
  stride = 2
layer[8->9] = flatten
layer[9->10] = fullc
  nhidden = 512
layer[10->11] = relu
layer[11->11] = dropout
  threshold = 0.5
layer[11->12] = fullc
  nhidden = 512
layer[12->13] = relu
layer[13->13] = dropout
  threshold = 0.5
layer[13->14] = fullc
  nhidden = 121
layer[14->14] = softmax
netconfig=end

# evaluation metric
metric = error

max_round = 45
num_round = 45

# input shape not including batch
input_shape = 3,48,48

batch_size = 100

# global parameters in any sectiion outside netconfig, and iter
momentum = 0.9
wmat:lr  = 0.01
wmat:wd  = 0.0005

bias:wd  = 0.000
bias:lr  = 0.02

# all the learning rate schedule starts with lr
lr:schedule = expdecay
lr:gamma = 0.1
lr:step = 10000

save_model=1
model_dir=models

# random config
random_type = xavier
init_sigma = 0.01

# new line
####$$$$ cxxnet-master\cxxnet-master\example\kaggle_bowl/README.md
* Resize all image to 48 X 48
```
mkdir /home/cxxnet/example/kaggle_bowl/data
python gen_train.py /home/data/bowl/train/ /home/cxxnet/example/kaggle_bowl/data/train/
python gen_test.py /home/data/bowl/test/ /home/cxxnet/example/kaggle_bowl/data/test/
```

* Generate img list
```
python gen_img_list.py train /home/data/bowl/sampleSubmission.csv data/train/ train.lst
python gen_img_list.py test /home/data/bowl/sampleSubmission.csv data/test/ test.lst
```

* Generate binary image file
First build im2bin at ```../../tools```, then run
```
../../tools/im2bin train.lst ./ train.bin
../../tools/im2bin test.lst ./ test.bin
```

* Run CXXNET
```
mkdir models
../../bin/cxxnet bowl.conf
```
It take about 5 minute to train a deep conv net model on Geforece 780

* Run Prediction
```
../../bin/cxxnet pred.conf
```
It will write softmax result in test.txt

* Make a submission file

```
python make_submission.py /home/data/bowl/sampleSubmission.csv test.lst test.txt out.csv
```

* Submit out.csv, you will get a result

* Validation

Run
```
sh gen_tr_va.sh train.lst
```
Then you will have ```tr.lst``` and ```va.lst``` as validation set list.

####$$$$ cxxnet-master\cxxnet-master\example\MNIST/.gitignore
data
models
####$$$$ cxxnet-master\cxxnet-master\example\MNIST/MNIST.conf
# example configure file for mnist
# training iterator
data = train
iter = mnist
    path_img = "./data/train-images-idx3-ubyte.gz"
    path_label = "./data/train-labels-idx1-ubyte.gz"
    shuffle = 1
iter = end
# evaluation iterator
eval = test
iter = mnist
    path_img = "./data/t10k-images-idx3-ubyte.gz"
    path_label = "./data/t10k-labels-idx1-ubyte.gz"
iter = end

netconfig=start
layer[+1:fc1] = fullc:fc1
  nhidden = 100
  init_sigma = 0.01
layer[+1:sg1] = sigmoid:se1
layer[sg1->fc2] = fullc:fc2
  nhidden = 10
  init_sigma = 0.01
layer[+0] = softmax
netconfig=end

# input shape not including batch
input_shape = 1,1,784
batch_size = 100

## global parameters
dev = cpu
save_model = 1
max_round = 15
num_round = 15
train_eval = 15
random_type = gaussian
## learning parameters
eta = 0.1
momentum = 0.9
wd  = 0.0
# evaluation metric
metric[label] = error
# end of config
####$$$$ cxxnet-master\cxxnet-master\example\MNIST/mnist.py
import sys
sys.path.append('../../wrapper/')
import cxxnet
import numpy as np

data = cxxnet.DataIter("""
iter = mnist
    path_img = "./data/train-images-idx3-ubyte.gz"
    path_label = "./data/train-labels-idx1-ubyte.gz"
    shuffle = 1
iter = end
input_shape = 1,1,784
batch_size = 100
""")
print 'init data iter'

deval = cxxnet.DataIter("""
iter = mnist
    path_img = "./data/t10k-images-idx3-ubyte.gz"
    path_label = "./data/t10k-labels-idx1-ubyte.gz"
iter = end
input_shape = 1,1,784
batch_size = 100
""")

cfg = """
netconfig=start
layer[+1:fc1] = fullc:fc1
  nhidden = 100
  init_sigma = 0.01
layer[+1:sg1] = sigmoid:se1
layer[sg1->fc2] = fullc:fc2
  nhidden = 10
  init_sigma = 0.01
layer[+0] = softmax
netconfig=end

input_shape = 1,1,784
batch_size = 100

random_type = gaussian
"""

param = {}
param['eta'] = 0.1
param['dev'] = 'cpu'
param['momentum'] = 0.9
param['metric[label]'] = 'error'

net = cxxnet.train(cfg, data, 1, param, eval_data = deval)

weights = []
for layer in ['fc1', 'fc2']:
    for tag in ['wmat', 'bias']:
        weights.append((layer, tag, net.get_weight(layer, tag)))

data.before_first()
data.next()
# extract 
print 'predict'
pred = net.predict(data)
print 'predict finish'
dbatch = data.get_data()
print dbatch.shape
print 'get data'
pred2 = net.predict(dbatch)

print np.sum(np.abs(pred - pred2))
print np.sum(np.abs(net.extract(data, 'sg1') - net.extract(dbatch, 'sg1')))

# evaluate
deval.before_first()
werr = 0
wcnt = 0
while deval.next():
    label = deval.get_label()
    pred = net.predict(deval)
    werr += np.sum(label[:,0] !=  pred[:])
    wcnt += len(label[:,0])
print 'eval-error=%f' % (float(werr) / wcnt)

# training
data.before_first()
while data.next():
    label = data.get_label()    
    batch = data.get_data()
    net.update(batch, label)

# evaluate
deval.before_first()
werr = 0
wcnt = 0
while deval.next():
    label = deval.get_label()
    pred = net.predict(deval)
    werr += np.sum(label[:,0] !=  pred[:])
    wcnt += len(label[:,0])
print 'eval-error2=%f' % (float(werr) / wcnt)

for layer, tag, wt in weights:
    net.set_weight(wt, layer, tag)

# evaluate
deval.before_first()
werr = 0
wcnt = 0
while deval.next():
    label = deval.get_label()
    pred = net.predict(deval)
    werr += np.sum(label[:,0] !=  pred[:])
    wcnt += len(label[:,0])
print 'eval-error-after-setback=%f' % (float(werr) / wcnt)
####$$$$ cxxnet-master\cxxnet-master\example\MNIST/MNIST_CONV.conf
# example configure file for mnist
# training iterator
data = train
iter = mnist
    path_img = "./data/train-images-idx3-ubyte.gz"
    path_label = "./data/train-labels-idx1-ubyte.gz"
    input_flat = 0
    shuffle = 1
iter = end
# evaluation iterator
eval = test
iter = mnist
    input_flat = 0
    path_img = "./data/t10k-images-idx3-ubyte.gz"
    path_label = "./data/t10k-labels-idx1-ubyte.gz"
iter = end

netconfig=start
layer[0->1] = conv:cv1
  kernel_size = 3
  pad = 1
  stride = 2
  nchannel = 32
  random_type = xavier
  no_bias=0
layer[1->2] = max_pooling
  kernel_size = 3
  stride = 2
layer[2->3] = flatten
layer[3->3] = dropout
  threshold = 0.5
layer[3->4] = fullc:fc1
  nhidden = 100
  init_sigma = 0.01
layer[4->5] = sigmoid:se1
layer[5->6] = fullc:fc2
  nhidden = 10
  init_sigma = 0.01
layer[6->6] = softmax
netconfig=end

# input shape not including batch
input_shape = 1,28,28
batch_size = 100

## global parameters
dev = gpu
save_model = 15
max_round = 15
num_round = 15
train_eval = 1
random_type = gaussian
## learning parameters
eta = 0.1
momentum = 0.9
wd  = 0.0
# evaluation metric
metric = error
eval_train = 1
# end of config
####$$$$ cxxnet-master\cxxnet-master\example\MNIST/mpi.conf
num_servers=2
num_workers=2
bin=../../bin/cxxnet.ps
arg="MNIST.conf -app_file MNIST.conf update_on_server=1 param_server=dist slient=1"
network_interface=eth0
network_port=9000
# hostfile=../../../../hosts
####$$$$ cxxnet-master\cxxnet-master\example\MNIST/README.md
This tutorial will show you how to use cxxnet train a feed forward neural network, then show you how to modify the feed forward network to a simple convolution neural network.

### Feed Forward Example
===
Reference configuration example [MNIST.conf]
(https://github.com/antinucleon/cxxnet/blob/master/example/MNIST/MNIST.conf)

### Before you start
* Run the command `./run.sh MNIST.conf` in the folder `example/MNIST/` to get data. 

### Setup data iterator configuration 
cxxnet use iterator to provide data batch to the network trainer. First we need to set the data type (_eval_ or _data_). Then we need to specify iterator type (_mnist_, _cifar_, _image_, _imgbin_, etc). Then set some attribute to the iterator including shuffle, file path and so on. 
Here is an example for MNIST
#### Setup training iterator
This part is about [**Data Iterator**](../../doc/io.md) setting

* Change _**path_image**_ to the path of training image file, change _**path_label**_  to the path of training label file you download just now
```bash
data = train
iter = mnist
    path_img = "./data/train-images-idx3-ubyte.gz"
    path_label = "./data/train-labels-idx1-ubyte.gz"
    shuffle = 1
iter = end
```
#### Setup test iterator
* Change _**path_image**_ to the path of test image file, change _**path_label**_ to the path of test label file you download just now
```bash
eval = test
iter = mnist
    path_img = "./data/t10k-images-idx3-ubyte.gz"
    path_label = "./data/t10k-labels-idx1-ubyte.gz"
iter = end
```
#### Setup network structure
* This part is about [**Layer Setting**](../../doc/layer.md)

* Network structure start with declaration _"netconfig=start"_ and end with _"netconfig=end"_. Note that there are two types of entities in cxxnet: node and layer. **Node** stores the intermediate results in the network, while **layer** denotes different kinds of transformations. Though the names of node and layer can be same, but they refer to different entities. They layer is declared in the format _"**layer[** from_node_name **->** to_node_name **]** = **layer_type**:name"_ Then comes the parameters of the layer. 
Here is an example for MNIST
```bash
netconfig=start
layer[0->1] = fullc:fc1
  nhidden = 100
  init_sigma = 0.01
layer[1->2] = sigmoid:se1
layer[2->3] = fullc:fc1
  nhidden = 10
  init_sigma = 0.01
layer[3->3] = softmax
netconfig=end
```
Notice some special layer like _softmax_ and _dropout_ use self-loop, which means the input node equals output node.
#### Setup input size and batch size
In this section, we need to set the input shape and batch size. The input shape should be 3 numbers, split by ','; The 3 numbers is channel, height and width for 3D input or 1, 1, dim for 1D vector input. In this example it is 
```bash
input_shape = 1,1,784
batch_size = 100
```
#### Setup global parameters
This part is about [**Global Setting**](../../doc/other.md)

Global parameters are used for setting the trainer behavior. In this example, we use the following configuration. 
```bash
dev = cpu
save_model = 15
max_round = 15
num_round = 15
train_eval = 1
random_type = gaussian
init_sigma = 0.01
```
First set working device **dev** ( _cpu_ or _gpu_ ); frequent to save mode **save_model** ; training round **num_round** and **max_round** and whether to print training set evaluation **train_eval**. The **random_type** defines weight initialization method. We provides ( _gaussian_ method and _xavier_ method)

#### Setup learning parameters
This part is about [**Updater Setting**](../../doc/updater.md)

learning parameter change the updater behavior. _eta_ is known as learning rate, and _wd_ is known as weight decay. And _momentum_ will help train faster.
```bash
eta = 0.1
momentum = 0.9
wd  = 0.0
```
Alternatively, we can set parameters specifically for bias and weight connections, using following configs.
```bash
wmat:eta = 0.1
bias:eta = 0.2
momentum = 0.9
wd  = 0.0
```
Here we set ```0.1``` learning rate for weight connections, and ```0.2``` learning rate for bias. Adding prefix ```wmat:``` to any parameter settings and they will be only applied to weight connections. While ```bias:```  stands for update parameters of bias 

#### Metric method
For classification, we use _error_ to metric performance. So set
```bash
metric = error
```
We also provide _logloss_, _rec@n_, _rmse_.
#### Running experiment
Make a folder named _"models"_ for saving the training model in the same folder you calling cxxnet_learner  
Then just run
```bash
../../bin/cxxnet ./MNIST.conf
```
or
```bash
./run.sh ./MNIST.conf
```
Then you will get a nearly 98% correct result in just several seconds.
#### Do prediction
Add these info in the configure file _MNIST.conf_
```bash
# Data iterator setting
pred = pred.txt
iter = mnist
    path_img = "./data/t10k-images-idx3-ubyte.gz"
    path_label = "./data/t10k-labels-idx1-ubyte.gz"
iter = end
# Global Setting
task = pred
model_in = ./models/0014.model
```

run
```bash
../../bin/cxxnet MNIST.conf
```
The prediction result will be stored in the pred.txt. Other useful tasks include predict_raw (output the probability yields by the last classifier) and extract_feature (extract a given node specified in extract_layer_name). Please refer to Imagenet example for more details 

### Continue Training
===

Now we get a model file `models/0014.model`. 
Add these configuration into the original configuration file:
```bash
max_round = 3
model_in = ./models/0014.model
```
Then run `../../bin/cxxnet MNIST.conf`, it will continue train extra 3 round based on previous model.
If use `continue=1` instead of `model_in`, cxxnet will search the model folder and use the model file with largest number.

### Convolution Example
===
Reference configuration example [MNIST_CONV.conf](https://github.com/antinucleon/cxxnet/blob/master/example/MNIST/MNIST_CONV.conf)

Use convolution layer is easy in cxxnet. Based on previous configuration, make the following changes
* Data Iterator, set `input_flat = 0` to make input in 3D: (1,28,28)
```bash
# Data iterator setting
data = train
iter = mnist
    path_img = "./data/train-images-idx3-ubyte.gz"
    path_label = "./data/train-labels-idx1-ubyte.gz"
    input_flat = 0
    shuffle = 1
iter = end

eval = test
iter = mnist
    input_flat = 0
    path_img = "./data/t10k-images-idx3-ubyte.gz"
    path_label = "./data/t10k-labels-idx1-ubyte.gz"
iter = end

```
* Update a convolution network structure
```bash
# Network structure setting
netconfig=start
layer[0->1] = conv:cv1
  kernel_size = 3
  pad = 1
  stride = 2
  nchannel = 32
  random_type = xavier
layer[1->2] = max_pooling
  kernel_size = 3
  stride = 2
layer[2->3] = flatten
layer[3->3] = dropout
  threshold = 0.5
layer[3->4] = fullc:fc1
  nhidden = 100
  init_sigma = 0.01
layer[4->5] = sigmoid:se1
layer[5->6] = fullc:fc1
  nhidden = 10
  init_sigma = 0.01
layer[6->6] = softmax
netconfig=end
```

* Set trainer input shape
```bash
input_shape = 1,28,28
```
* Try to use GPU train the conv net, set `dev=gpu`
```bash
dev = gpu
```
* Then run
```bash
../../bin/cxxnet ./MNIST_CONV.conf
```
or 
```bash
./run.sh ./MNIST_CONV.conf
```
You will get a result near 99% in a few seconds.
####$$$$ cxxnet-master\cxxnet-master\example\MNIST/run.sh
#!/bin/bash

if [ ! -d "data" ]; then
    mkdir data
fi

cd data

if [ ! -f "train-images-idx3-ubyte.gz" ]; then
    wget http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
fi

if [ ! -f "train-labels-idx1-ubyte.gz" ]; then
    wget http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
fi

if [ ! -f "t10k-images-idx3-ubyte.gz" ]; then
    wget http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
fi

if [ ! -f "t10k-labels-idx1-ubyte.gz" ]; then
    wget http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
fi

cd ..

if [ ! -d "models" ]; then
    mkdir models
fi

../../bin/cxxnet $1
####$$$$ cxxnet-master\cxxnet-master\make/config.mk
#-----------------------------------------------------
#  cxxnet: the configuration compile script
#
#  This is the default configuration setup for cxxnet
#  If you want to change configuration, do the following steps:
#
#  - copy this file to the root folder
#  - modify the configuration you want
#  - type make or make -j n for parallel build
#----------------------------------------------------

# choice of compiler
export CC = gcc
export CXX = g++
export NVCC = nvcc

# whether use CUDA during compile
USE_CUDA = 1

# add the path to CUDA libary to link and compile flag
# if you have already add them to enviroment variable, leave it as NONE
USE_CUDA_PATH = NONE

# whether use opencv during compilation
# you can disable it, however, you will not able to use
# imbin iterator
USE_OPENCV = 1
USE_OPENCV_DECODER = 1
# whether use CUDNN R3 library
USE_CUDNN = 0
# add the path to CUDNN libary to link and compile flag
# if you do not need that, or do not have that, leave it as NONE
USE_CUDNN_PATH = NONE

#
# choose the version of blas you want to use
# can be: mkl, blas, atlas, openblas
USE_STATIC_MKL = NONE
USE_BLAS = mkl
#
# add path to intel libary, you may need it
# for MKL, if you did not add the path to enviroment variable
#
USE_INTEL_PATH = NONE

# whether compile with parameter server
USE_DIST_PS = 0
PS_PATH = NONE
PS_THIRD_PATH = NONE

# the additional link flags you want to add
ADD_LDFLAGS = -ljpeg

# the additional compile flags you want to add
ADD_CFLAGS =
#
# If use MKL, choose static link automaticly to fix python wrapper
#
ifeq ($(USE_BLAS), mkl)
	USE_STATIC_MKL = 1
endif

####$$$$ cxxnet-master\cxxnet-master\make/README.md
Make configuration for CXXNET
====

cxxnet is designed to require less third party library. The minimal requirement is MKL/CBLAS/OpenBLAS and MShadow(which can be downloaded automatically).  Other dependence can be set by editing  [make/config.mk](make/config.mk) before make.
####$$$$ cxxnet-master\cxxnet-master\src/cxxnet_main.cpp
#define _CRT_SECURE_NO_WARNINGS
#define _CRT_SECURE_NO_DEPRECATE

#include <ctime>
#include <string>
#include <cstring>
#include <vector>
#include <climits>
#include "nnet/nnet.h"
#include "io/data.h"
#include "utils/config.h"

namespace cxxnet{


class CXXNetLearnTask {
 public:
  CXXNetLearnTask(void) {
    this->task = "train";
    this->net_type = 0;
    this->net_trainer = NULL;
    this->itr_train = NULL;
    this->itr_pred  = NULL;
    name_model_dir = "models";
    num_round = 10;
    test_io = 0;
    silent = start_counter = 0;
    max_round = INT_MAX;
    continue_training = 0;
    save_period = 1;
    name_model_in = "NULL";
    name_pred     = "pred.txt";
    print_step    = 100;
    reset_net_type = -1;
    extract_node_name = "";
    output_format = 1;
#if MSHADOW_USE_CUDA
    this->SetParam("dev", "gpu");
#else
    this->SetParam("dev", "cpu");
#endif
  }
  ~CXXNetLearnTask(void) {
    if (net_trainer != NULL) {
      delete net_trainer;
      // shut down tensor engine if it is GPU based
      //if (device == "gpu") mshadow::ShutdownTensorEngine();
    }

    if (itr_train != NULL)   delete itr_train;
    if (itr_pred  != NULL)   delete itr_pred;
    for (size_t i = 0; i < itr_evals.size(); ++ i) {
      delete itr_evals[i];
    }
  }
 public:
  inline int Run(int argc, char *argv[]) {
    if (argc < 2) {
      printf("Usage: <config>\n");
      return 0;
    }

    utils::ConfigIterator itr(argv[1]);
    while (itr.Next()) {
      this->SetParam(itr.name(), itr.val());
    }
    for (int i = 2; i < argc; i ++) {
      char name[256], val[256];
      if (sscanf(argv[i], "%[^=]=%s", name, val) == 2) {
        this->SetParam(name, val);
      }
    }
    this->Init();
    if (!silent) {
      printf("initializing end, start working\n");
    }
    if (task == "train" || task == "finetune") this->TaskTrain();
    if (task == "pred")   this->TaskPredict();
    if (task == "extract") this->TaskExtractFeature();
    return 0;
  }

  inline void SetParam(const char *name , const char *val) {
    if (!strcmp(val, "default")) return;
    if (!strcmp(name,"net_type"))            net_type = atoi(val);
    if (!strcmp(name,"reset_net_type"))      reset_net_type = atoi(val);
    if (!strcmp(name,"print_step"))          print_step = atoi(val);
    if (!strcmp(name,"continue"))            continue_training = atoi(val);
    if (!strcmp(name,"save_model"))        save_period = atoi(val);
    if (!strcmp(name,"start_counter"))      start_counter = atoi(val);
    if (!strcmp(name,"model_in"))           name_model_in = val;
    if (!strcmp(name,"model_dir"))          name_model_dir= val;
    if (!strcmp(name,"num_round" ))         num_round     = atoi(val);
    if (!strcmp(name,"max_round"))           max_round = atoi(val);
    if (!strcmp(name, "silent"))            silent        = atoi(val);
    if (!strcmp(name, "task"))              task = val;
    if (!strcmp(name, "dev"))               device = val;
    if (!strcmp(name, "test_io"))           test_io = atoi(val);
    if (!strcmp(name, "extract_node_name"))         extract_node_name = val;
    if (!strcmp(name, "output_format")) {
      if  (!strcmp(val, "txt")) output_format = 1;
      else output_format = 0;
    }
    cfg.push_back(std::make_pair(std::string(name), std::string(val)));
  }
 private:
  // configure trainer
  inline void Init(void) {
    if (task == "train" && continue_training) {
      if (SyncLastestModel() == 0) {
        utils::Error("Init: Cannot find models for continue training. \
          Please specify it by model_in instead.");
      } else {
        printf("Init: Continue training from round %d\n", start_counter);
        this->CreateIterators();
        return;
      }
    }
    continue_training = 0;
    if (name_model_in == "NULL") {
      utils::Assert(task == "train", "must specify model_in if not training");
      net_trainer = this->CreateNet();
      net_trainer->InitModel();
    } else {
      if (task == "finetune") {
        this->CopyModel();
      } else {
        this->LoadModel();
      }
    }
  
    this->CreateIterators();
  }
  // load in latest model from model_folder
  inline int SyncLastestModel(void) {
    FILE *fi = NULL, *last = NULL;
    char name[ 256 ];
    int s_counter = start_counter;
    do{
      if (last != NULL) fclose(last);
      last = fi;
      sprintf(name,"%s/%04d.model", name_model_dir.c_str(), s_counter ++);
      fi = fopen64(name, "rb");
    }while (fi != NULL);

    if (last != NULL) {
      utils::Assert(fread(&net_type, sizeof(int), 1, last) > 0, "loading model");
      net_trainer = this->CreateNet();
      utils::FileStream fs(last);
      net_trainer->LoadModel(fs);
      start_counter = s_counter - 1;
      fclose(last);
      return 1;
    }else{
      return 0;
    }
  }
  // load model from file
  inline void LoadModel(void) {
    const char* pos = strrchr(name_model_in.c_str(), '/');
    if (pos != NULL && sscanf(pos + 1, "%d", &start_counter) != 1){
      printf("WARNING: Cannot infer start_counter from model name. Specify it in config if needed\n");
    }
    FILE *fi = utils::FopenCheck(name_model_in.c_str(), "rb");
    utils::Assert(fread(&net_type, sizeof(int), 1, fi) > 0, "loading model");
    net_trainer = this->CreateNet();
    utils::FileStream fs(fi);
    net_trainer->LoadModel(fs);
    fclose(fi);
    ++start_counter;
  }
  // save model into file
  inline void SaveModel(void) {
    char name[256];
    sprintf(name,"%s/%04d.model" , name_model_dir.c_str(), start_counter ++);
    if (save_period == 0 || start_counter % save_period != 0) return;
    FILE *fo  = utils::FopenCheck(name, "wb");
    fwrite(&net_type, sizeof(int), 1, fo);
    utils::FileStream fs(fo);
    net_trainer->SaveModel(fs);
    fclose(fo);
  }
  // create a neural net
  inline nnet::INetTrainer* CreateNet(void) {
    if (reset_net_type != -1) {
      net_type = reset_net_type;
    }
    nnet::INetTrainer *net;
    if (!strncmp(device.c_str(), "gpu", 3)) {
#if MSHADOW_USE_CUDA
      net = nnet::CreateNet<mshadow::gpu>(net_type);
#else
      net = NULL;
      utils::Error("MSHADOW_USE_CUDA was not enabled");
#endif
    } else {
      net = nnet::CreateNet<mshadow::cpu>(net_type);
    }

    for (size_t i = 0; i < cfg.size(); ++ i) {
      net->SetParam(cfg[i].first.c_str(), cfg[i].second.c_str());
    }
    return net;
  }
  inline void InitIter(IIterator<DataBatch>* itr,
                        const std::vector< std::pair< std::string, std::string> > &defcfg) {
    for (size_t i = 0; i < defcfg.size(); ++ i) {
      itr->SetParam(defcfg[i].first.c_str(), defcfg[i].second.c_str());
    }
    itr->Init();

  }
  // iterators
  inline void CreateIterators(void) {
    int flag = 0;
    std::string evname;
    std::vector< std::pair< std::string, std::string> > itcfg;
    std::vector< std::pair< std::string, std::string> > defcfg;
    for (size_t i = 0; i < cfg.size(); ++ i) {
      const char *name = cfg[i].first.c_str();
      const char *val  = cfg[i].second.c_str();
      if (!strcmp(name, "data")) {
        flag = 1; continue;
      }
      if (!strcmp(name, "eval")) {
        evname = std::string(val);
        flag = 2; continue;
      }
      if (!strcmp(name, "pred")) {
        flag = 3; name_pred = val; continue;
      }
      if (!strcmp(name, "iter") && !strcmp(val, "end")) {
        utils::Assert(flag != 0, "wrong configuration file");
        if (flag == 1 && task != "pred") {
          utils::Assert(itr_train == NULL, "can only have one data");
          itr_train = cxxnet::CreateIterator(itcfg);
        }
        if (flag == 2 && task != "pred") {
          itr_evals.push_back(cxxnet::CreateIterator(itcfg));
          eval_names.push_back(evname);
        }
        if (flag == 3 && (task == "pred" || task == "pred_raw" ||
                          task == "extract")) {
          utils::Assert(itr_pred == NULL, "can only have one data:test");
          itr_pred = cxxnet::CreateIterator(itcfg);
        }
        flag = 0; itcfg.clear();
      }
      if (flag == 0) {
        defcfg.push_back(cfg[i]);
      }else{
        itcfg.push_back(cfg[i]);
      }
    }
    if (itr_train != NULL) {
      this->InitIter(itr_train, defcfg);
    }
    if (itr_pred != NULL) {
      this->InitIter(itr_pred, defcfg);
    }
    for (size_t i = 0; i < itr_evals.size(); ++ i) {
      this->InitIter(itr_evals[i], defcfg);
    }
  }
 private:
  inline void TaskPredict(void) {
    utils::Assert(itr_pred != NULL, "must specify a predict iterator to generate predictions");
    printf("start predicting...\n");
    FILE *fo = utils::FopenCheck(name_pred.c_str(), "w");
    itr_pred->BeforeFirst();
    mshadow::TensorContainer<mshadow::cpu, 1> pred;
    while (itr_pred->Next()) {
      const DataBatch& batch = itr_pred->Value();
      net_trainer->Predict(&pred, batch);
      utils::Assert(batch.num_batch_padd < batch.batch_size, "num batch pad must be smaller");
      mshadow::index_t sz = pred.size(0) - batch.num_batch_padd;
      for (mshadow::index_t j = 0; j < sz; ++j) {
        fprintf(fo, "%g\n", pred[j]);
      }
    }
    fclose(fo);
    printf("finished prediction, write into %s\n", name_pred.c_str());
  }
  inline void TaskExtractFeature() {
    long nrow = 0;
    mshadow::Shape<3> dshape;
    utils::Check(itr_pred != NULL,
                 "must specify a predict iterator to generate predictions");
    printf("start predicting...\n");
    FILE *fo = utils::FopenCheck(name_pred.c_str(), "wb");
    std::string name_meta = name_pred + ".meta";
    FILE *fm = utils::FopenCheck(name_meta.c_str(), "w");
    itr_pred->BeforeFirst();

    time_t start    = time(NULL);
    int sample_counter = 0;
    mshadow::TensorContainer<mshadow::cpu, 4> pred;
    while (itr_pred->Next()) {
      const DataBatch &batch = itr_pred->Value();
      if (extract_node_name != ""){
        net_trainer->ExtractFeature(&pred, batch, extract_node_name.c_str());
      } else {
        utils::Error("extract node name must be specified in task extract_feature.");
      }
      utils::Assert(batch.num_batch_padd < batch.batch_size, "num batch pad must be smaller");
      mshadow::index_t sz = pred.size(0) - batch.num_batch_padd;
      nrow += sz;
      for (mshadow::index_t j = 0; j < sz; ++j) {
        mshadow::Tensor<mshadow::cpu, 2> d = pred[j].FlatTo2D();
        for (mshadow::index_t k = 0; k < d.size(0); ++k) {
          if (output_format) {
            for (mshadow::index_t m = 0; m < d.size(1); ++m) {
              fprintf(fo, "%g ", d[k].dptr_[m]);
            }
          } else {
            fwrite(d[k].dptr_, sizeof(float), d.size(1), fo);
          }
        }
        if (output_format) {
          fprintf(fo, "\n");
        }
      }
      if (sz != 0) {
        dshape = pred[0].shape_;
      }
      if (++ sample_counter  % print_step == 0) {
        long elapsed = (long)(time(NULL) - start);
        if (!silent) {
          printf("\r                                                               \r");
          printf("batch:[%8d] %ld sec elapsed", sample_counter, elapsed);
          fflush(stdout);
        }
      }
    }
    long elapsed = (long)(time(NULL) - start);
    printf("\r                                                               \r");
    printf("batch:[%8d] %ld sec elapsed\n", sample_counter, elapsed);

    fclose(fo);
    fprintf(fm, "%ld,%u,%u,%u\n", nrow, dshape[0], dshape[1], dshape[2]);
    fclose(fm);
    printf("finished prediction, write into %s\n", name_pred.c_str());
  }
  inline void TaskTrain(void) {
    time_t start    = time(NULL);
    unsigned long elapsed = 0;
    if (continue_training == 0 && name_model_in == "NULL") {
      this->SaveModel();
    } else {
      if (!silent) {
        printf("continuing from round %d", start_counter-1);
        fflush(stdout);
      }
      for (size_t i = 0; i < itr_evals.size(); ++i) {
        std::string res = net_trainer->Evaluate(itr_evals[i], eval_names[i].c_str());
        fprintf(stderr, "%s", res.c_str());
      }
      fprintf(stderr, "\n");
      fflush(stderr);
    }
    
    if (itr_train != NULL) {
      if (test_io != 0) {
        printf("start I/O test\n");
      }
      int cc = max_round;
      while (start_counter <= num_round && cc --) {
        if (!silent) {
          printf("update round %d", start_counter -1); fflush(stdout);
        }
        int sample_counter = 0;
        net_trainer->StartRound(start_counter);
        itr_train->BeforeFirst();
        while (itr_train->Next()) {
          if (test_io == 0) {
            net_trainer->Update(itr_train->Value());
          }
          if (++ sample_counter  % print_step == 0) {
            elapsed = (long)(time(NULL) - start);
            if (!silent) {
              printf("\r                                                               \r");
              printf("round %8d:[%8d] %ld sec elapsed", start_counter-1,
                     sample_counter, elapsed);
              fflush(stdout);
            }
          }
        }

        if (test_io == 0) {
          // code handling evaluation
          fprintf(stderr, "[%d]", start_counter);
          // handle only with eval_train = 1, but not val data
          if (itr_evals.size() == 0) {
            std::string res = net_trainer->Evaluate(NULL, "train");
            fprintf(stderr, "%s", res.c_str());
          }
          for (size_t i = 0; i < itr_evals.size(); ++i) {
            std::string res = net_trainer->Evaluate(itr_evals[i], eval_names[i].c_str());
            fprintf(stderr, "%s", res.c_str());
          }
          fprintf(stderr, "\n");
          fflush(stderr);
        }
        elapsed = (unsigned long)(time(NULL) - start);
        this->SaveModel();
      }
      
      if (!silent) {
        printf("\nupdating end, %lu sec in all\n", elapsed);
      }
    }
  }

  inline void CopyModel(void){
    FILE *fi = utils::FopenCheck(name_model_in.c_str(), "rb");
    utils::Assert(fread(&net_type, sizeof(int), 1, fi) > 0, "loading model");
    net_trainer = this->CreateNet();
    utils::FileStream fs(fi);
    net_trainer->CopyModelFrom(fs);
    fclose(fi);
  }
 private:
  /*! \brief type of net implementation */
  int net_type;
  /*! \brief whether to force reset network implementation */
  int reset_net_type;
  /*! \brief trainer */
  nnet::INetTrainer *net_trainer;
  /*! \brief training iterator, prediction iterator */
  IIterator<DataBatch>* itr_train, *itr_pred;
  /*! \brief validation iterators */
  std::vector<IIterator<DataBatch>* > itr_evals;
  /*! \brief evaluation names */
  std::vector<std::string> eval_names;
 private:
  /*! \brief all the configurations */
  std::vector<std::pair<std::string, std::string> > cfg;
 private:
  /*! \brief whether test io only */
  int test_io;
  /*! \brief  how may samples before print information */
  int print_step;
  /*! \brief number of round to train */
  int num_round;
  /*! \brief maximum number of round to train */
  int max_round;
  /*! \brief continue from model folder */
  int continue_training;
  /*! \brief  whether to save model after each round */
  int save_period;
  /*! \brief  start counter of model */
  int start_counter;
  /*! \brief  whether to be silent */
  int silent;
  /*! \brief  device of the trainer */
  std::string device;
  /*! \brief  task of the job */
  std::string task;
  /*! \brief  input model name */
  std::string name_model_in;
  /*! \brief training data */
  std::string name_data;
  /*! \brief folder name of output */
  std::string name_model_dir;
  /*! \brief file name to write prediction */
  std::string name_pred;
  /*! \brief the layer name to be extracted */
  std::string extract_node_name;
  /*! \brief output format of network */
  int output_format;
 };
}  // namespace cxxnet

// general main for PS
int WorkerNodeMain(int argc, char *argv[]) {
  cxxnet::CXXNetLearnTask tsk;
  return tsk.Run(argc, argv);
}
####$$$$ cxxnet-master\cxxnet-master\src/global.h
#ifndef CXXNET_GLOBAL_H_
/*!
 * \file global.h
 * \brief global configuration of cxxnet, this controls how cxxnet is compiled
 * \author Tianqi Chen
 */
/*! \brief whether to adapt caffe layers */
#ifndef CXXNET_USE_CAFFE_ADAPTOR
#define CXXNET_USE_CAFFE_ADAPTOR 0
#endif

/*! 
 *\brief whether to use opencv support, 
 *  without it, we will not be able to use load jpg image iterarators
 */
#ifndef CXXNET_USE_OPENCV
#define CXXNET_USE_OPENCV 1
#endif

/*!
 *\brief whether to use cudnn library for convolution
 */
#ifndef CXXNET_USE_CUDNN
#define CXXNET_USE_CUDNN 0
#endif

/*! \brief namespace of cxxnet */
namespace cxxnet {
typedef mshadow::cpu cpu;
typedef mshadow::gpu gpu;
typedef mshadow::index_t index_t;
typedef mshadow::default_real_t real_t;
} // namespace cxxnet

#endif  // CXXNET_GLOBAL_H_
####$$$$ cxxnet-master\cxxnet-master\src/local_main.cpp
/*!
 * \file local_main.cpp
 * \brief local main file that redirect directly to PSMain
 * \author Tianqi Chen
 */

int WorkerNodeMain(int argc, char *argv[]);

int main(int argc, char *argv[]) {
  return WorkerNodeMain(argc, argv);
}
####$$$$ cxxnet-master\cxxnet-master\src/README.md
Coding Guide
======
This file is intended to be notes about code structure in cxxnet

Project Logical Layout
=======
* Dependency order: nnet->updater->layer
  - All module depends on global.h and utils
  - io is an independent module
* layer is implementation of neural net layers and defines forward and backward propagation
* updater is the parameter updating module, it defines update rule of weights
* nnet is the neural net structure that combines layers together to form a neural net
* io is the input module to handle reading various data and preprocessing

File Naming Convention
======= 
* .h files are data structures and interface
  - In each folder, there is one .h file that have same name as the folder, this file defines everything needed for other module to use this module
  - Interface headers: layer/layer.h, updater/updater.h
* -inl.hpp files are implementations of interface, like cpp file in most project.
  - You only need to understand the interface file to understand the usage of that layer
* In each folder, there can be a .cpp file, and .cu file that that compiles the module of that layer
  - the .cpp file and .cu file does not contain implementation, but reuse common implementation in file ends with _impl-inl.hpp
####$$$$ cxxnet-master\cxxnet-master\src\io/data.cpp
#define _CRT_SECURE_NO_WARNINGS
#define _CRT_SECURE_NO_DEPRECATE

#include <string>
#include <vector>
#include <mshadow/tensor.h>
#include "./data.h"
#include "../global.h"
#include "../utils/utils.h"
#include "../utils/io.h"
#include "iter_mnist-inl.hpp"
#include "iter_augment_proc-inl.hpp"
#include "iter_batch_proc-inl.hpp"
#include "iter_mem_buffer-inl.hpp"
#include "iter_attach_txt-inl.hpp"
#if CXXNET_USE_OPENCV
#include "iter_thread_imbin-inl.hpp"
#include "iter_thread_imbin_x-inl.hpp"
#include "iter_img-inl.hpp"
#endif

namespace cxxnet {
IIterator<DataBatch> *CreateIterator(const std::vector< std::pair<std::string, std::string> > &cfg) {
  size_t i = 0;
  IIterator<DataBatch> *it = NULL;
  for (; i < cfg.size(); ++i) {
    const char *name = cfg[i].first.c_str();
    const char *val  = cfg[i].second.c_str();
    if (!strcmp(name, "iter")) {
      if (!strcmp(val, "mnist")) {
        utils::Check(it == NULL, "mnist can not chain over other iterator");
        it = new MNISTIterator(); continue;
      }
      #if CXXNET_USE_OPENCV
      if (!strcmp(val, "imgbin")) {
        utils::Assert(it == NULL, "image binary can not chain over other iterator");
        it = new BatchAdaptIterator(new AugmentIterator(new ThreadImagePageIterator()));
        continue;
      }
      if (!strcmp(val, "imgbinx")) {
        utils::Assert(it == NULL, "image binary can not chain over other iterator");
        it = new BatchAdaptIterator(new AugmentIterator(new ThreadImagePageIteratorX()));
        continue;
      }
      if (!strcmp(val, "img")) {
        utils::Assert(it == NULL, "image list iterator can not chain over other iterator");
        it = new BatchAdaptIterator(new AugmentIterator(new ImageIterator()));
        continue;
      }
      #endif
      if (!strcmp(val, "threadbuffer")) {
        utils::Assert(it != NULL, "must specify input of threadbuffer");
        it = new ThreadBufferIterator(it);
        continue;
      }
      if (!strcmp(val, "membuffer")) {
        utils::Assert(it != NULL, "must specify input of memory buffer");
        it = new DenseBufferIterator(it);
        continue;
      }
      if (!strcmp(val, "attachtxt")) {
        utils::Assert(it != NULL, "must specify input of attach txt buffer");
        it = new AttachTxtIterator(it);
        continue;
      }
      utils::Error("unknown iterator type %s", val);
    }
    if (it != NULL) {
      it->SetParam(name, val);
    }
  }
  utils::Assert(it != NULL, "must specify iterator by iter=itername");
  return it;
}
} // namespace cxxnet
####$$$$ cxxnet-master\cxxnet-master\src\io/data.h
#ifndef CXXNET_DATA_H_
#define CXXNET_DATA_H_
/*!
 * \file data.h
 * \brief data type and iterator abstraction
 * \author Bing Xu, Tianqi Chen
 */
#include <vector>
#include <string>
#include <mshadow/tensor.h>
#include "../utils/utils.h"

namespace cxxnet {
/*!
 * \brief iterator type
 * \tparam DType data type
 */
template<typename DType>
class IIterator {
public:
  /*!
   * \brief set the parameter
   * \param name name of parameter
   * \param val  value of parameter
   */
  virtual void SetParam(const char *name, const char *val) = 0;
  /*! \brief initalize the iterator so that we can use the iterator */
  virtual void Init(void) = 0;
  /*! \brief set before first of the item */
  virtual void BeforeFirst(void) = 0;
  /*! \brief move to next item */
  virtual bool Next(void) = 0;
  /*! \brief get current data */
  virtual const DType &Value(void) const = 0;
public:
  /*! \brief constructor */
  virtual ~IIterator(void) {}
}; // class IIterator

/*! \brief a single data instance */
struct DataInst {
  /*! \brief unique id for instance */
  unsigned index;
  /*! \brief label information */
  mshadow::Tensor<mshadow::cpu, 1> label;
  /*! \brief content of data */
  mshadow::Tensor<mshadow::cpu, 3> data;
}; // struct DataInst

/*! \brief a sparse data instance, in sparse vector */
struct SparseInst {
  /*! \brief an entry of sparse vector */
  struct Entry {
    /*! \brief feature index */
    unsigned findex;
    /*! \brief feature value */
    float fvalue;
    Entry(unsigned findex, float fvalue): findex(findex), fvalue(fvalue) {}
  }; // struct Entry
  /*! \brief label information */
  mshadow::Tensor<mshadow::cpu, 1> label;
  /*! \brief unique id for instance */
  unsigned index;
  /*! \brief length of the instance */
  unsigned length;
  /*! \brief pointer to the elements*/
  const Entry *data;
  /*! \brief get i-th pair in the sparse vector*/
  inline const Entry &operator[](size_t i)const {
    return data[i];
  }
}; // struct SparseInst

/*!
 * \brief a standard batch of data commonly used by iterator
 *        this could be a dense batch or sparse matrix,
 *        the iterator configuration should be aware what kind of batch can be generated and feed in to each type of neuralnet
 */
struct DataBatch {
public:
  /*! \brief unique id for instance, can be NULL, sometimes is useful */
  unsigned *inst_index;
  /*! \brief number of instance */
  mshadow::index_t batch_size;
  /*! \brief number of padding elements in this batch,
       this is used to indicate the last elements in the batch are only padded up to match the batch, and should be discarded */
  mshadow::index_t num_batch_padd;
public:
  /*! \brief label information of the data*/
  mshadow::Tensor<mshadow::cpu, 2> label;
  /*! \brief content of dense data, if this DataBatch is dense */
  mshadow::Tensor<mshadow::cpu, 4> data;
  /*! \brief extra data to be fed to the network */
  std::vector<mshadow::Tensor<mshadow::cpu, 4> > extra_data;
public:
  // sparse part of the DataBatch, in CSR format
  /*! \brief array[batch_size+1], row pointer of each of the elements */
  const size_t *sparse_row_ptr;
  /*! \brief array[row_ptr.back()], content of the sparse element */
  const SparseInst::Entry *sparse_data;
public:
  /*! \brief constructor */
  DataBatch(void) {
    label.dptr_ = NULL;
    inst_index = NULL;
    data.dptr_ = NULL;
    batch_size = 0; num_batch_padd = 0;
    sparse_row_ptr = NULL;
    sparse_data = NULL;
  }
  /*! \brief auxiliary  functionto allocate space, if needed */
  inline void AllocSpaceDense(mshadow::Shape<4> shape,
                              mshadow::index_t batch_size,
                              mshadow::index_t label_width,
                              bool pad = false) {
    data = mshadow::NewTensor<mshadow::cpu>(shape, 0.0f, pad);
    mshadow::Shape<2> lshape = mshadow::Shape2(batch_size, label_width);
    label = mshadow::NewTensor<mshadow::cpu>(lshape, 0.0f, pad);
    inst_index = new unsigned[batch_size];
    this->batch_size = batch_size;
  }
  /*! \brief auxiliary  functionto allocate space, if needed */
  inline void AllocSpaceDense(mshadow::Shape<4> shape,
                              mshadow::index_t batch_size,
                              mshadow::index_t label_width,
                              const std::vector<mshadow::Shape<4> >& extra_shape,
                              bool pad = false) {
    AllocSpaceDense(shape, batch_size, label_width, pad);
    for (mshadow::index_t i = 0; i < extra_shape.size(); ++i){
      extra_data.push_back(mshadow::NewTensor<mshadow::cpu>(extra_shape[i], 0.0f, pad));
    }
  }
  /*! \brief auxiliary function to free space, if needed, dense only */
  inline void FreeSpaceDense(void) {
    if (label.dptr_ != NULL) {
      delete [] inst_index;
      mshadow::FreeSpace(&label);
      mshadow::FreeSpace(&data);
      label.dptr_ = NULL;
    }
    for (mshadow::index_t i = 0; i < extra_data.size(); ++i){
      mshadow::FreeSpace(&extra_data[i]);
    }
  }
  /*! \brief copy dense content from existing data, dense only */
  inline void CopyFromDense(const DataBatch &src) {
    utils::Assert(batch_size == src.batch_size, "DataBatch: the batch size is not set correctly");
    num_batch_padd = src.num_batch_padd;
    utils::Check(src.inst_index != NULL, "CopyFromDense need to copy instance index");
    memcpy(inst_index, src.inst_index, batch_size * sizeof(unsigned));
    utils::Assert(data.shape_ == src.data.shape_, "DataBatch: data shape mismatch");
    utils::Assert(label.shape_ == src.label.shape_, "DataBatch: label shape mismatch");
    mshadow::Copy(label, src.label);
    mshadow::Copy(data, src.data);
    utils::Assert(extra_data.size() == src.extra_data.size(),
      "DataBatch: extra data number mismatch");
    for (mshadow::index_t i = 0; i < extra_data.size(); ++i){
      utils::Assert(label.shape_ == src.label.shape_,
        "DataBatch: extra data %d shape mismatch", i);
      mshadow::Copy(extra_data[i], src.extra_data[i]);
    }
  }
public:
  /*! \brief helper function to check if a element is sparse */
  inline bool is_sparse(void) const {
    return sparse_row_ptr != NULL;
  }
  /*! \brief get rid'th row from the sparse element, the data must be in sparse format */
  inline SparseInst GetRowSparse(unsigned rid) const {
    SparseInst inst;
    inst.data = sparse_data + sparse_row_ptr[rid];
    inst.length = sparse_row_ptr[rid + 1] - sparse_row_ptr[rid];
    inst.label = label[rid];
    if (inst_index != NULL) {
      inst.index = inst_index[rid];
    } else {
      inst.index = 0;
    }
    return inst;
  }
}; // struct DataBatch
/*!
 * \brief create iterator from configure settings
 * \param cfg configure settings key=vale pair
 */
IIterator<DataBatch> *CreateIterator(const std::vector<std::pair<std::string, std::string> > &cfg);
}  // namespace cxxnet
#endif  // CXXNET_DATA_H_
####$$$$ cxxnet-master\cxxnet-master\src\io/image_augmenter-inl.hpp
#ifndef IMAGE_AUGMENTER_OPENCV_HPP_
#define IMAGE_AUGMENTER_OPENCV_HPP_
/*!
 * \file image_augmenter_opencv.hpp
 * \brief threaded version of page iterator
 * \author Naiyan Wang, Tianqi Chen
 */
#include <opencv2/opencv.hpp>
#include "../utils/random.h"

namespace cxxnet {
/*! \brief helper class to do image augmentation */
class ImageAugmenter {
 public:
  // contructor
  ImageAugmenter(void)
      : tmpres(false),
        rotateM(2, 3, CV_32F) {
    rand_crop_ = 0;
    crop_y_start_ = -1;
    crop_x_start_ = -1;
    max_rotate_angle_ = 0.0f;
    max_aspect_ratio_ = 0.0f;
    max_shear_ratio_ = 0.0f;
    min_crop_size_ = -1;
    max_crop_size_ = -1;
    rotate_ = -1.0f;
    max_random_scale_ = 1.0f;
    min_random_scale_ = 1.0f;
    min_img_size_ = 0.0f;
    max_img_size_ = 1e10f;
    fill_value_ = 255;
  }
  virtual ~ImageAugmenter() {
  }
  virtual void SetParam(const char *name, const char *val) {
    if (!strcmp(name, "input_shape")) {
      utils::Check(sscanf(val, "%u,%u,%u", &shape_[0], &shape_[1], &shape_[2]) == 3,
                   "input_shape must be three consecutive integers without space example: 1,1,200 ");
    }
    if (!strcmp(name, "rand_crop")) rand_crop_ = atoi(val);
    if (!strcmp(name, "crop_y_start")) crop_y_start_ = atoi(val);
    if (!strcmp(name, "crop_x_start")) crop_x_start_ = atoi(val);
    if (!strcmp(name, "max_rotate_angle")) max_rotate_angle_ = atof(val);
    if (!strcmp(name, "max_shear_ratio")) max_shear_ratio_ = atof(val);
    if (!strcmp(name, "max_aspect_ratio")) max_aspect_ratio_ = atof(val);
    if (!strcmp(name, "min_crop_size")) min_crop_size_ = atoi(val);
    if (!strcmp(name, "max_crop_size")) max_crop_size_ = atoi(val);
    if (!strcmp(name, "min_random_scale")) min_random_scale_ = atof(val);
    if (!strcmp(name, "max_random_scale")) max_random_scale_ = atof(val);
    if (!strcmp(name, "min_img_size")) min_img_size_ = atof(val);
    if (!strcmp(name, "max_img_size")) max_img_size_ = atof(val);
    if (!strcmp(name, "fill_value")) fill_value_ = atoi(val);
    if (!strcmp(name, "mirror")) mirror_ = atoi(val);
    if (!strcmp(name, "rotate")) rotate_ = atoi(val);
    if (!strcmp(name, "rotate_list")) {
      const char *end = val + strlen(val);
      char buf[128];
      while (val < end) {
        sscanf(val, "%[^,]", buf);
        val += strlen(buf) + 1;
        rotate_list_.push_back(atoi(buf));
      }
    }
  }
  /*!
   * \brief augment src image, store result into dst
   *   this function is not thread safe, and will only be called by one thread
   *   however, it will tries to re-use memory space as much as possible
   * \param src the source image
   * \param source of random number
   * \param dst the pointer to the place where we want to store the result
   */
  virtual cv::Mat Process(const cv::Mat &src,
                          utils::RandomSampler *prnd) {
    // shear
    float s = prnd->NextDouble() * max_shear_ratio_ * 2 - max_shear_ratio_;
    // rotate
    int angle = prnd->NextUInt32(max_rotate_angle_ * 2) - max_rotate_angle_;
    if (rotate_ > 0) angle = rotate_;
    if (rotate_list_.size() > 0) {
      angle = rotate_list_[prnd->NextUInt32(rotate_list_.size() - 1)];
    }
    float a = cos(angle / 180.0 * M_PI);
    float b = sin(angle / 180.0 * M_PI);
    // scale
    float scale = prnd->NextDouble() * (max_random_scale_ - min_random_scale_) + min_random_scale_;
    // aspect ratio
    float ratio = prnd->NextDouble() * max_aspect_ratio_ * 2 - max_aspect_ratio_ + 1;
    float hs = 2 * scale / (1 + ratio);
    float ws = ratio * hs;
    // new width and height
    float new_width = std::max(min_img_size_, std::min(max_img_size_, scale * src.cols));
    float new_height = std::max(min_img_size_, std::min(max_img_size_, scale * src.rows));
    //printf("%f %f %f %f %f %f %f %f %f\n", s, a, b, scale, ratio, hs, ws, new_width, new_height);
    cv::Mat M(2, 3, CV_32F);
    M.at<float>(0, 0) = hs * a - s * b * ws;
    M.at<float>(1, 0) = -b * ws;
    M.at<float>(0, 1) = hs * b + s * a * ws;
    M.at<float>(1, 1) = a * ws;
    float ori_center_width = M.at<float>(0, 0) * src.cols + M.at<float>(0, 1) * src.rows;
    float ori_center_height = M.at<float>(1, 0) * src.cols + M.at<float>(1, 1) * src.rows;
    M.at<float>(0, 2) = (new_width - ori_center_width) / 2;
    M.at<float>(1, 2) = (new_height - ori_center_height) / 2;
    cv::warpAffine(src, temp, M, cv::Size(new_width, new_height),
                     cv::INTER_CUBIC,
                     cv::BORDER_CONSTANT,
                     cv::Scalar(fill_value_, fill_value_, fill_value_));
    cv::Mat res = temp;
    mshadow::index_t y = res.rows - shape_[2];
    mshadow::index_t x = res.cols - shape_[1];
    if (rand_crop_ != 0) {
      y = prnd->NextUInt32(y + 1);
      x = prnd->NextUInt32(x + 1);
    } else {
      y /= 2; x /= 2;
    }
    cv::Rect roi(x, y, shape_[1], shape_[2]);
    res = res(roi);
    return res;
  }
  /*!
   * \brief augment src image, store result into dst
   *   this function is not thread safe, and will only be called by one thread
   *   however, it will tries to re-use memory space as much as possible
   * \param src the source image
   * \param source of random number
   * \param dst the pointer to the place where we want to store the result
   */
  virtual mshadow::Tensor<cpu, 3> Process(mshadow::Tensor<cpu, 3> data,
                                          utils::RandomSampler *prnd) {
    if (!NeedProcess()) return data;
    cv::Mat res(data.size(1), data.size(2), CV_8UC3);
    for (index_t i = 0; i < data.size(1); ++i) {
      for (index_t j = 0; j < data.size(2); ++j) {
        res.at<cv::Vec3b>(i, j)[0] = data[2][i][j];
        res.at<cv::Vec3b>(i, j)[1] = data[1][i][j];
        res.at<cv::Vec3b>(i, j)[2] = data[0][i][j];
      }
    }
    res = this->Process(res, prnd);
    tmpres.Resize(mshadow::Shape3(3, res.rows, res.cols));
    for (index_t i = 0; i < tmpres.size(1); ++i) {
      for (index_t j = 0; j < tmpres.size(2); ++j) {
        cv::Vec3b bgr = res.at<cv::Vec3b>(i, j);
        tmpres[0][i][j] = bgr[2];
        tmpres[1][i][j] = bgr[1];
        tmpres[2][i][j] = bgr[0];
      }
    }
    return tmpres;
  }

 private:
  // whether skip processing
  inline bool NeedProcess(void) const {
    if (max_rotate_angle_ > 0 || max_shear_ratio_ > 0.0f
        || rotate_ > 0 || rotate_list_.size() > 0) return true;
    if (min_crop_size_ > 0 && max_crop_size_ > 0) return true;
    return false;
  }
  // temp input space
  mshadow::TensorContainer<cpu, 3> tmpres;
  // temporal space
  cv::Mat temp0, temp, temp2;
  // rotation param
  cv::Mat rotateM;
  // parameters
  /*! \brief input shape */
  mshadow::Shape<4> shape_;
  /*! \brief whether we do random cropping */
  int rand_crop_;
  /*! \brief whether we do nonrandom croping */
  int crop_y_start_;
  /*! \brief whether we do nonrandom croping */
  int crop_x_start_;
  /*! \brief Indicate the max ratation angle for augmentation, we will random rotate */
  /*! \brief [-max_rotate_angle, max_rotate_angle] */
  int max_rotate_angle_;
  /*! \brief max aspect ratio */
  float max_aspect_ratio_;
  /*! \brief random shear the image [-max_shear_ratio, max_shear_ratio] */
  float max_shear_ratio_;
  /*! \brief max crop size */
  int max_crop_size_;
  /*! \brief min crop size */
  int min_crop_size_;
  /*! \brief max scale ratio */
  float max_random_scale_;
  /*! \brief min scale_ratio */
  float min_random_scale_;
  /*! \brief min image size */
  float min_img_size_;
  /*! \brief max image size */
  float max_img_size_;
  /*! \brief whether to mirror the image */
  int mirror_;
  /*! \brief rotate angle */
  int rotate_;
  /*! \brief filled color while padding */
  int fill_value_;
  /*! \brief list of possible rotate angle */
  std::vector<int> rotate_list_;
};
}  // namespace cxxnet
#endif
####$$$$ cxxnet-master\cxxnet-master\src\io/iter_attach_txt-inl.hpp
#ifndef CXXNET_ATTACH_TXT_ITER_INL_HPP_
#define CXXNET_ATTACH_TXT_ITER_INL_HPP_
/*!
 * \file iter_attach_txt-inl.hpp
 * \brief iterator that attach additional data store in txt file
 * \author Naiyan Wang
 */
#include <map>
#include <mshadow/tensor.h>
#include "./data.h"
#include "../utils/utils.h"
#include "../utils/io.h"

namespace cxxnet {
class AttachTxtIterator : public IIterator<DataBatch> {
 public:
  AttachTxtIterator(IIterator<DataBatch> *base)
      : base_(base) {
    file_ = NULL;
    batch_size_ = 0;
    round_batch_ = 0;
  }
  virtual void SetParam(const char *name, const char *val) {
    base_->SetParam(name, val);
    if (!strcmp(name, "filename")) filename_ = val;
    if (!strcmp(name, "batch_size"))  batch_size_ = (index_t)atoi(val);
    if (!strcmp(name, "round_batch")) round_batch_ = atoi(val);
  }
  virtual ~AttachTxtIterator(void) {
    delete base_;
    mshadow::FreeSpace(&extra_data_);
  }
  virtual void Init(void) {
    base_->Init();
    file_ = fopen(filename_.c_str(), "r");
    utils::Assert(file_ != NULL,
      "AttachTxt: Open file failed: %s", filename_.c_str());
    utils::Assert(fscanf(file_, "%d", &dim_) == 1,
      "AttachTxt: First line should indicate the data dim.");
    extra_data_ = mshadow::NewTensor<cpu>(
            mshadow::Shape4(batch_size_, 1, 1, dim_), 0.0f, false);
    int cnt = 0;
    int data_id = 0;
    while (fscanf(file_, "%d", &data_id) == 1) {
      id_map_[data_id] = cnt++;
      for (int i = 0; i < dim_; ++i) {
        float tmp;
        utils::Check(fscanf(file_, "%f", &tmp) == 1,
                     "AttachTxt: data do not match dimension specified");
        all_data_.push_back(tmp);
      }
    }
    fclose(file_);
  }
  virtual void BeforeFirst(void) {
    base_->BeforeFirst();
  }
  virtual bool Next(void) {
    if (base_->Next()) {
      out_ = base_->Value();
      out_.extra_data.clear();
      out_.extra_data.push_back(extra_data_);
      for (int top = 0; top < batch_size_; ++top) {
        if (id_map_.find(out_.inst_index[top]) != id_map_.end()) {
          int start = id_map_[out_.inst_index[top]] * dim_;
          for (int i = 0; i < dim_; ++i) {
            extra_data_[top][0][0][i] = all_data_[start++];  
          }
        }
      }
      return true;
    } else {
      return false;
    }
  }
  virtual const DataBatch &Value(void) const {
    return out_;
  }

 private:
  /*! \brief dim of the additional data */
  int dim_;
  /*! \brief batch size */
  int batch_size_;
  /*! \brief whether to use round robin to pad batch */
  int round_batch_;
  /*! \brief the output data batch */
  DataBatch out_;
  /*! \brief filename of the extra data */
  std::string filename_;
  /*! \brief file pointer of the file */
  FILE* file_;
  /*! \brief file pointer of the file */
  mshadow::Tensor<cpu, 4> extra_data_;
  /*! \brief base iterator */
  IIterator<DataBatch> *base_;
  std::map<int, int> id_map_;
  std::vector<float> all_data_;
};
}  // namespace cxxnet
#endif  // CXXNET_ATTACH_TXT_ITER_INL_HPP_
####$$$$ cxxnet-master\cxxnet-master\src\io/iter_augment_proc-inl.hpp
#ifndef CXXNET_ITER_AUGMENT_INL_HPP_
#define CXXNET_ITER_AUGMENT_INL_HPP_
/*!
 * \file iter_augment_proc-inl.hpp
 * \brief processing unit to do data augmention
 * \author Tianqi Chen, Bing Xu, Naiyan Wang
 */
#include <mshadow/tensor.h>
#include "data.h"
#include "../utils/utils.h"
#include "../utils/io.h"
#include "../utils/random.h"
#include "../utils/thread_buffer.h"

#if CXXNET_USE_OPENCV
#include "./image_augmenter-inl.hpp"
#endif

namespace cxxnet {
/*! \brief create a batch iterator from single instance iterator */
class AugmentIterator: public IIterator<DataInst> {
public:
  AugmentIterator(IIterator<DataInst> *base) 
      : base_(base) {
    rand_crop_ = 0;
    rand_mirror_ = 0;
    crop_y_start_ = -1;
    crop_x_start_ = -1;
    // scale data
    scale_ = 1.0f;
    // silent
    silent_ = 0;
    // by default, not mean image file
    name_meanimg_ = "";
    mean_r_ = 0.0f;
    mean_g_ = 0.0f;
    mean_b_ = 0.0f;
    mirror_ = 0;
    max_random_illumination_ = 0.0f;
    max_random_contrast_ = 0.0f;
    rnd.Seed(kRandMagic);
  }
  virtual ~AugmentIterator(void) {
    delete base_;
  }
  virtual void SetParam(const char *name, const char *val) {
    base_->SetParam(name, val);
    if (!strcmp(name, "input_shape")) {
      utils::Check(sscanf(val, "%u,%u,%u", &shape_[0], &shape_[1], &shape_[2]) == 3,
                   "input_shape must be three consecutive integers without space example: 1,1,200 ");
    }
    if (!strcmp(name, "seed_data")) rnd.Seed(kRandMagic + atoi(val));
    if (!strcmp(name, "rand_crop")) rand_crop_ = atoi(val);
    if (!strcmp(name, "silent")) silent_ = atoi(val);
    if (!strcmp(name, "divideby")) scale_ = static_cast<real_t>(1.0f / atof(val));
    if (!strcmp(name, "scale")) scale_ = static_cast<real_t>(atof(val));
    if (!strcmp(name, "image_mean")) name_meanimg_ = val;
    if (!strcmp(name, "crop_y_start")) crop_y_start_ = atoi(val);
    if (!strcmp(name, "crop_x_start")) crop_x_start_ = atoi(val);
    if (!strcmp(name, "rand_mirror")) rand_mirror_ = atoi(val);
    if (!strcmp(name, "mirror")) mirror_ = atoi(val);
    if (!strcmp(name, "max_random_contrast")) max_random_contrast_ = atof(val);
    if (!strcmp(name, "max_random_illumination")) max_random_illumination_ = atof(val);
    if (!strcmp(name, "mean_value")) {
      utils::Check(sscanf(val, "%f,%f,%f", &mean_b_, &mean_g_, &mean_r_) == 3,
                   "mean value must be three consecutive float without space example: 128,127.5,128.2 ");
    }
#if CXXNET_USE_OPENCV
    aug.SetParam(name, val);
#endif
  }
  virtual void Init(void) {
    base_->Init();
    meanfile_ready_ = false;
    if (name_meanimg_.length() != 0) {
      FILE *fi = fopen64(name_meanimg_.c_str(), "rb");
      if (fi == NULL) {
        this->CreateMeanImg();
      } else {
        if (silent_ == 0) {
          printf("loading mean image from %s\n", name_meanimg_.c_str());
        }
        utils::FileStream fs(fi) ;
        meanimg_.LoadBinary(fs);
        fclose(fi);
        meanfile_ready_ = true;
      }
    }
  }
  virtual void BeforeFirst(void) {
    base_->BeforeFirst();
  }
  virtual const DataInst &Value(void) const {
    return out_;
  }

private:
  inline void SetData(const DataInst &d) {
    using namespace mshadow::expr;
    out_.label = d.label;
    out_.index = d.index;
    mshadow::Tensor<cpu, 3> data = d.data;
#if CXXNET_USE_OPENCV
    data = aug.Process(data, &rnd);
#endif

    img_.Resize(mshadow::Shape3(data.shape_[0], shape_[1], shape_[2]));    
    if (shape_[1] == 1) {
      img_ = data * scale_;
    } else {
      utils::Assert(data.size(1) >= shape_[1] && data.size(2) >= shape_[2],
                    "Data size must be bigger than the input size to net.");
      mshadow::index_t yy = data.size(1) - shape_[1];
      mshadow::index_t xx = data.size(2) - shape_[2];
      if (rand_crop_ != 0 && (yy != 0 || xx != 0)) {
        yy = rnd.NextUInt32(yy + 1);
        xx = rnd.NextUInt32(xx + 1);
      } else {
        yy /= 2; xx /= 2;
      }
      if (data.size(1) != shape_[1] && crop_y_start_ != -1) {
        yy = crop_y_start_;
      }
      if (data.size(2) != shape_[2] && crop_x_start_ != -1) {
        xx = crop_x_start_;
      }
      float contrast = rnd.NextDouble() * max_random_contrast_ * 2 - max_random_contrast_ + 1;
      float illumination = rnd.NextDouble() * max_random_illumination_ * 2 - max_random_illumination_;
      if (mean_r_ > 0.0f || mean_g_ > 0.0f || mean_b_ > 0.0f) {
        // substract mean value
        d.data[0] -= mean_b_; d.data[1] -= mean_g_; d.data[2] -= mean_r_;
        if ((rand_mirror_ != 0 && rnd.NextDouble() < 0.5f) || mirror_ == 1) {
          img_ = mirror(crop(d.data * contrast + illumination, img_[0].shape_, yy, xx)) * scale_;
        } else {
          img_ = crop(d.data * contrast + illumination, img_[0].shape_, yy, xx) * scale_ ;
        }
      } else if (!meanfile_ready_ || name_meanimg_.length() == 0) {
        // do not substract anything
        if (rand_mirror_ != 0 && rnd.NextDouble() < 0.5f) {
          img_ = mirror(crop(d.data, img_[0].shape_, yy, xx)) * scale_;
        } else {
          img_ = crop(d.data, img_[0].shape_, yy, xx) * scale_ ;
        }
      } else {
        // substract mean image
        if ((rand_mirror_ != 0 && rnd.NextDouble() < 0.5f) || mirror_ == 1) {
          if (d.data.shape_ == meanimg_.shape_){
            img_ = mirror(crop((d.data - meanimg_) * contrast + illumination, img_[0].shape_, yy, xx)) * scale_;
          } else {
            img_ = (mirror(crop(d.data, img_[0].shape_, yy, xx) - meanimg_) * contrast + illumination) * scale_;
          }
        } else {
          if (d.data.shape_ == meanimg_.shape_){
            img_ = crop((d.data - meanimg_) * contrast + illumination, img_[0].shape_, yy, xx) * scale_ ;
          } else {
            img_ = ((crop(d.data, img_[0].shape_, yy, xx) - meanimg_) * contrast + illumination) * scale_;
          }
        }
      }
    }
    out_.data = img_;
  }
  inline bool Next(void) {
    if (!base_->Next()){
      return false;
    }
    const DataInst &d = base_->Value();
    this->SetData(d);
    return true;
  }
  inline void CreateMeanImg(void) {
    if (silent_ == 0) {
      printf("cannot find %s: create mean image, this will take some time...\n", name_meanimg_.c_str());
    }
    time_t start = time(NULL);
    unsigned long elapsed = 0;
    size_t imcnt = 1;

    utils::Assert(this->Next(), "input iterator failed.");
    meanimg_.Resize(mshadow::Shape3(shape_[0], shape_[1], shape_[2]));
    mshadow::Copy(meanimg_, img_);
    while (this->Next()) {
      meanimg_ += img_; imcnt += 1;
      elapsed = (long)(time(NULL) - start);
      if (imcnt % 1000 == 0 && silent_ == 0) {
        printf("\r                                                               \r");
        printf("[%8lu] images processed, %ld sec elapsed", imcnt, elapsed);
        fflush(stdout);
      }
    }
    meanimg_ *= (1.0f / imcnt);
    utils::StdFile fo(name_meanimg_.c_str(), "wb");
    meanimg_.SaveBinary(fo);
    if (silent_ == 0) {
      printf("save mean image to %s..\n", name_meanimg_.c_str());
    }
    this->BeforeFirst();
  }
private:
  /*! \brief base iterator */
  IIterator<DataInst> *base_;
  /*! \brief input shape */
  mshadow::Shape<4> shape_;
  /*! \brief output data */
  DataInst out_;
  /*! \brief silent */
  int silent_;
  /*! \brief scale of data */
  real_t scale_;
  /*! \brief whether we do random cropping */
  int rand_crop_;
  /*! \brief whether we do nonrandom croping */
  int crop_y_start_;
  /*! \brief whether we do nonrandom croping */
  int crop_x_start_;
  /*! \brief whether we do random mirroring */
  int rand_mirror_;
  /*! \brief mean image, if needed */
  mshadow::TensorContainer<cpu, 3> meanimg_;
  /*! \brief temp space */
  mshadow::TensorContainer<cpu, 3> img_;
  /*! \brief mean image file, if specified, will generate mean image file, and substract by mean */
  std::string name_meanimg_;
  /*! \brief mean value for r channel */
  float mean_r_;
  /*! \brief mean value for g channel */
  float mean_g_;
  /*! \brief mean value for b channel */
  float mean_b_;
  /*! \brief maximum ratio of contrast variation */
  float max_random_contrast_;
  /*! \brief maximum value of illumination variation */
  float max_random_illumination_;
  /*! \brief whether to mirror the image */
  int mirror_;
  /*! \brief whether mean file is ready */
  bool meanfile_ready_;
  // augmenter
#if CXXNET_USE_OPENCV
  ImageAugmenter aug;
#endif
  // random sampler
  utils::RandomSampler rnd;
  // random magic number of this iterator
  static const int kRandMagic = 0;
};  // class AugmentIterator
}  // namespace cxxnet
#endif
####$$$$ cxxnet-master\cxxnet-master\src\io/iter_batch_proc-inl.hpp
#ifndef CXXNET_ITER_BATCH_PROC_INL_HPP_
#define CXXNET_ITER_BATCH_PROC_INL_HPP_
/*!
 * \file iter_batch_proc-inl.hpp
 * \brief definition of preprocessing iterators that takes an iterator and do some preprocessing
 * \author Tianqi Chen
 */
#include <mshadow/tensor.h>
#include "./data.h"
#include "../utils/utils.h"
#include "../utils/io.h"
#include "../utils/thread_buffer.h"

namespace cxxnet {
/*! \brief create a batch iterator from single instance iterator */
class BatchAdaptIterator: public IIterator<DataBatch> {
public:
  BatchAdaptIterator(IIterator<DataInst> *base): base_(base) {
    // skip read, used for debug
    test_skipread_ = 0;
    // use round roubin to handle overflow batch
    round_batch_ = 0;
    // number of overflow instances that readed in round_batch mode
    num_overflow_ = 0;
    // silent
    silent_ = 0;
    // label width
    label_width_ = 1;
  }
  virtual ~BatchAdaptIterator(void) {
    delete base_;
    out_.FreeSpaceDense();
  }
  virtual void SetParam(const char *name, const char *val) {
    base_->SetParam(name, val);
    if (!strcmp(name, "batch_size"))  batch_size_ = (index_t)atoi(val);
    if (!strcmp(name, "input_shape")) {
      utils::Assert(sscanf(val, "%u,%u,%u", &shape_[1], &shape_[2], &shape_[3]) == 3,
                    "input_shape must be three consecutive integers without space example: 1,1,200 ");
    }
    if (!strcmp(name, "label_width")) {
      label_width_ = static_cast<index_t>(atoi(val));
    }
    if (!strcmp(name, "round_batch")) round_batch_ = atoi(val);
    if (!strcmp(name, "silent")) silent_ = atoi(val);
    if (!strcmp(name, "test_skipread")) test_skipread_ = atoi(val);
  }
  virtual void Init(void) {
    base_->Init();
    mshadow::Shape<4> tshape = shape_;
    if (tshape[2] == 1 && tshape[1] == 1) {
      // what is this for?
      tshape[0] = batch_size_; tshape[3] = 1;
    } else {
      tshape[0] = batch_size_;
    }
    out_.AllocSpaceDense(tshape, batch_size_, label_width_, false);
  }

  virtual void BeforeFirst(void) {
    if (round_batch_ == 0 || num_overflow_ == 0) {
      // otherise, we already called before first
      base_->BeforeFirst();
    } else {
      num_overflow_ = 0;
    }
    head_ = 1;
  }
  virtual bool Next(void) {
    out_.num_batch_padd = 0;

    // skip read if in head version
    if (test_skipread_ != 0 && head_ == 0) return true;
    else this->head_ = 0;

    // if overflow from previous round, directly return false, until before first is called
    if (num_overflow_ != 0) return false;
    index_t top = 0;

    while (base_->Next()) {
      const DataInst& d = base_->Value();
      mshadow::Copy(out_.label[top], d.label);
      out_.inst_index[top] = d.index;
      //out_.data[top] = d.data;
      Copy(out_.data[top], d.data);

      if (++ top >= batch_size_) return true;
    }
    if (top != 0) {
      if (round_batch_ != 0) {
        num_overflow_ = 0;
        base_->BeforeFirst();
        for (; top < batch_size_; ++top, ++num_overflow_) {
          utils::Assert(base_->Next(), "number of input must be bigger than batch size");
          const DataInst& d = base_->Value();
          mshadow::Copy(out_.label[top], d.label);
          out_.inst_index[top] = d.index;
          out_.data[top] = d.data;
        }
        out_.num_batch_padd = num_overflow_;
      } else {
        out_.num_batch_padd = batch_size_ - top;
      }
      return true;
    }
    return false;
  }
  virtual const DataBatch &Value(void) const {
    utils::Assert(head_ == 0, "must call Next to get value");
    return out_;
  }
private:
  /*! \brief base iterator */
  IIterator<DataInst> *base_;
  /*! \brief batch size */
  index_t batch_size_;
  /*! \brief input shape */
  mshadow::Shape<4> shape_;
  /*! \brief label width */
  index_t label_width_;
  /*! \brief output data */
  DataBatch out_;
  /*! \brief on first */
  int head_;
  /*! \brief skip read */
  int test_skipread_;
  /*! \brief silent */
  int silent_;
  /*! \brief use round roubin to handle overflow batch */
  int round_batch_;
  /*! \brief number of overflow instances that readed in round_batch mode */
  int num_overflow_;
}; // class BatchAdaptIterator

/*! \brief thread buffer iterator */
class ThreadBufferIterator: public IIterator< DataBatch > {
public :
  ThreadBufferIterator(IIterator<DataBatch> *base) {
    silent_ = 0;
    itr.get_factory().base_ = base;
    itr.SetParam("buffer_size", "2");
  }
  virtual ~ThreadBufferIterator() {
    itr.Destroy();
  }
  virtual void SetParam(const char *name, const char *val) {
    if (!strcmp(name, "silent")) silent_ = atoi(val);
    itr.SetParam(name, val);
  }
  virtual void Init(void) {
    utils::Assert(itr.Init(), "iterator init fail") ;
    printf("In batch init.\n");
    if (silent_ == 0) {
      printf("ThreadBufferIterator: buffer_size=%d\n", itr.buf_size);
    }
  }
  virtual void BeforeFirst() {
    itr.BeforeFirst();
  }
  virtual bool Next() {
    if (itr.Next(out_)) {
      return true;
    } else {
      return false;
    }
  }
  virtual const DataBatch &Value() const {
    return out_;
  }
private:
  struct Factory {
  public:
    IIterator<DataBatch> *base_;
  public:
    Factory(void) {
      base_ = NULL;
    }
    inline void SetParam(const char *name, const char *val) {
      base_->SetParam(name, val);
    }
    inline bool Init() {
      base_->Init();
      utils::Assert(base_->Next(), "ThreadBufferIterator: input can not be empty");
      oshape_ = base_->Value().data.shape_;
      batch_size_ = base_->Value().batch_size;
      label_width_ = base_->Value().label.size(1);
      for (size_t i = 0; i < base_->Value().extra_data.size(); ++i){
        extra_shape_.push_back(base_->Value().extra_data[i].shape_);
      }
      base_->BeforeFirst();
      return true;
    }
    inline bool LoadNext(DataBatch &val) {
      if (base_->Next()) {
        val.CopyFromDense(base_->Value());
        return true;
      } else {
        return false;
      }
    }
    inline DataBatch Create(void) {
      DataBatch a; a.AllocSpaceDense(oshape_, batch_size_, label_width_, extra_shape_);
      return a;
    }
    inline void FreeSpace(DataBatch &a) {
      a.FreeSpaceDense();
    }
    inline void Destroy() {
      if (base_ != NULL) delete base_;
    }
    inline void BeforeFirst() {
      base_->BeforeFirst();
    }
  private:
    mshadow::index_t batch_size_;
    mshadow::index_t label_width_;
    mshadow::Shape<4> oshape_;
    std::vector<mshadow::Shape<4> > extra_shape_;
  };
private:
  int silent_;
  DataBatch out_;
  utils::ThreadBuffer<DataBatch, Factory> itr;
}; // class ThreadBufferIterator
}  // namespace cxxnet
#endif  // CXXNET_ITER_BATCH_PROC_INL_HPP_
####$$$$ cxxnet-master\cxxnet-master\src\io/iter_img-inl.hpp
#ifndef CXXNET_ITER_IMG_INL_HPP
#define CXXNET_ITER_IMG_INL_HPP
#pragma once
/*!
 * \file iter_img-inl.hpp
 * \brief implementation of image iterator
 * \author Tianqi Chen, Naiyan Wang
 */
// use opencv for image loading
#include "data.h"
#include <mshadow/tensor.h>
#include <opencv2/opencv.hpp>

namespace cxxnet{
  /*! \brief simple image iterator that only loads data instance */
class ImageIterator : public IIterator< DataInst >{
public:
  ImageIterator(void) {
    img_.set_pad(false);
    fplst_ = NULL;
    silent_ = 0;
    path_imgdir_ = "";
    path_imglst_ = "img.lst";
    shuffle_ = 0;
    data_index_ = 0;
    label_width_ = 1;
  }
  virtual ~ImageIterator(void) {
    if(fplst_ != NULL) fclose(fplst_);
  }
  virtual void SetParam(const char *name, const char *val) {
    if(!strcmp(name, "image_list"))  path_imglst_ = val;
    if(!strcmp(name, "image_root"))   path_imgdir_ = val;
    if(!strcmp(name, "silent"  ))  silent_ = atoi(val);
    if(!strcmp(name, "shuffle"  ))  shuffle_ = atoi(val);
    if(!strcmp(name, "label_width"  ))  label_width_ = atoi(val);
  }
  virtual void Init(void) {
    fplst_  = utils::FopenCheck(path_imglst_.c_str(), "r");
    if(silent_ == 0) {
      printf("ImageIterator:image_list=%s\n", path_imglst_.c_str());
    }
    unsigned index;
    while (fscanf(fplst_, "%u", &index) == 1) {
      index_list_.push_back(index);
      for (int i = 0; i < label_width_; ++i) {
        float tmp;
        utils::Check(fscanf(fplst_, "%f", &tmp) == 1,
               "ImageList format:label_width=%d but only have %d labels per line",
               label_width_, i);
        labels_.push_back(tmp);
      }
      char name[256];
      utils::Assert(fscanf(fplst_, "%s\n", name) == 1, "ImageList: no file name");
      filenames_.push_back(name);
    }
    for (size_t i = 0; i < index_list_.size(); ++i) {
      order_.push_back(i);
    }
    this->BeforeFirst();
  }
  virtual void BeforeFirst(void) {
    data_index_ = 0;
    if (shuffle_) {
      std::random_shuffle(order_.begin(), order_.end());
    }
  }
  virtual bool Next(void) {
    if (data_index_ < static_cast<int>(order_.size())) {
      size_t index = order_[data_index_];
      if (path_imgdir_.length() == 0) {
        LoadImage(img_, out_, filenames_[index].c_str());
      } else {
        char sname[256];
        sprintf(sname, "%s%s", path_imgdir_.c_str(), filenames_[index].c_str());
        LoadImage(img_, out_, sname);
      }
      out_.index = index_list_[index];
      mshadow::Tensor<cpu, 1> label_(&(labels_[0]) + label_width_ * index,
        mshadow::Shape1(label_width_));
      out_.label = label_;
      ++data_index_;
      return true;
    }
    return false;
  }
  virtual const DataInst &Value(void) const{
    return out_;
  }
protected:
  inline static void LoadImage(mshadow::TensorContainer<cpu,3> &img, 
          DataInst &out,
          const char *fname) {
    cv::Mat res = cv::imread(fname);
    utils::Assert(res.data != NULL, "LoadImage: Reading image %s failed.\n", fname);
    img.Resize(mshadow::Shape3(3, res.rows, res.cols));
    for(index_t y = 0; y < img.size(1); ++y) {
      for(index_t x = 0; x < img.size(2); ++x) {
        cv::Vec3b bgr = res.at<cv::Vec3b>(y, x);
        // store in RGB order
        img[2][y][x] = bgr[0];
        img[1][y][x] = bgr[1];
        img[0][y][x] = bgr[2];
      }
    }
    out.data = img;
    // free memory
    res.release();
  }
protected:
  // output data
  DataInst out_;
  // silent
  int silent_;
  // file pointer to list file, information file
  FILE *fplst_;
  // prefix path of image folder, path to input lst, format: imageid label path
  std::string path_imgdir_, path_imglst_;
  // temp storage for image
  mshadow::TensorContainer<cpu, 3> img_;
  // whether the data will be shuffled in each epoch
  int shuffle_;
  // denotes the number of labels
  int label_width_;
  // denotes the current data index
  int data_index_;
  // stores the reading orders
  std::vector<int> order_;
  // stores the labels of data
  std::vector<float> labels_;
  // stores the file names of the images
  std::vector<std::string> filenames_;
  // stores the index list of images
  std::vector<int> index_list_;
  };
};
#endif
####$$$$ cxxnet-master\cxxnet-master\src\io/iter_mem_buffer-inl.hpp
#ifndef CXXNET_ITER_MEM_BUFFER_INL_HPP_
#define CXXNET_ITER_MEM_BUFFER_INL_HPP_
/*!
 * \file iter_mem_buffer-inl.hpp
 * \brief iterator that gets limited number of batch into memory,
 *        and only return these data
 * \author Tianqi Chen
 */
#include <mshadow/tensor.h>
#include "./data.h"
#include "../utils/utils.h"
#include "../utils/io.h"

namespace cxxnet {
/*! \brief iterator that gets limitted number of batch into memory */
class DenseBufferIterator : public IIterator<DataBatch> {
 public:
  DenseBufferIterator(IIterator<DataBatch> *base)
      : base_(base) {
    max_nbatch_ = 100;
    data_index_ = 0;
    silent_ = 0;
  }
  virtual void SetParam(const char *name, const char *val) {
    base_->SetParam(name, val);
    if (!strcmp(name, "max_nbatch")) {
      max_nbatch_ = static_cast<size_t>(atol(val));
    }
    if (!strcmp(name, "silent")) silent_ = atoi(val);
  }
  virtual void Init(void) {
    base_->Init();
    while (base_->Next()) {
      const DataBatch &batch = base_->Value();
      utils::Assert(batch.label.dptr_ != NULL, "need dense");
      DataBatch v;
      v.AllocSpaceDense(batch.data.shape_, batch.batch_size, batch.label.size(1));
      v.CopyFromDense(batch);
      buffer_.push_back(v);
      if (buffer_.size() >= max_nbatch_) break;
    }
    if (silent_ == 0) {
      printf("DenseBufferIterator: load %d batches\n",
             static_cast<int>(buffer_.size()));
    }
  }
  virtual void BeforeFirst(void) {
    data_index_ = 0;
  }
  virtual bool Next(void) {
    if (data_index_ < buffer_.size()) {
      data_index_ += 1;
      return true;
    } else {
      return false;
    }
  }
  virtual const DataBatch &Value(void) const {
    utils::Assert(data_index_ > 0,
                  "Iterator.Value: at beginning of iterator");
    return buffer_[data_index_ - 1];
  }

 private:
  /*! \brief silent */
  int silent_;
  /*! \brief maximum number of batch in buffer */
  size_t max_nbatch_;
  /*! \brief data index */
  size_t data_index_;
  /*! \brief base iterator */
  IIterator<DataBatch> *base_;
  /*! \brief data content */
  std::vector<DataBatch> buffer_;
};
}  // namespace cxxnet
#endif  // CXXNET_ITER_BATCH_PROC_INL_HPP_
####$$$$ cxxnet-master\cxxnet-master\src\io/iter_mnist-inl.hpp
#ifndef CXXNET_ITER_MNIST_INL_HPP_
#define CXXNET_ITER_MNIST_INL_HPP_
/*!
 * \file iter_mnist-inl.hpp
 * \brief iterator that takes mnist dataset
 * \author Tianqi Chen
 */
#include <mshadow/tensor.h>
#include "data.h"
#include "../utils/io.h"
#include "../utils/random.h"

namespace cxxnet {
class MNISTIterator: public IIterator<DataBatch> {
 public:
  MNISTIterator(void) {
    img_.dptr_ = NULL;
    mode_ = 1;
    inst_offset_ = 0;
    silent_ = 0;
    shuffle_ = 0;
    rnd.Seed(kRandMagic);
  }  
  virtual ~MNISTIterator(void) {
    if (img_.dptr_ != NULL) delete []img_.dptr_;
  }
  virtual void SetParam(const char *name, const char *val) {
    if (!strcmp(name, "silent")) silent_ = atoi(val);
    if (!strcmp(name, "batch_size")) batch_size_ = (index_t)atoi(val);
    if (!strcmp(name, "input_flat")) mode_ = atoi(val);
    if (!strcmp(name, "shuffle")) shuffle_ = atoi(val);
    if (!strcmp(name, "index_offset")) inst_offset_ = atoi(val);
    if (!strcmp(name, "path_img")) path_img = val;
    if (!strcmp(name, "path_label")) path_label = val;
    if (!strcmp(name, "path_img")) path_img = val;
    if (!strcmp(name, "seed_data")) rnd.Seed(kRandMagic + atoi(val));
  }
  // intialize iterator loads data in
  virtual void Init(void) {
    this->LoadImage();
    this->LoadLabel();
    if (mode_ == 1) {
      out_.data.shape_ = mshadow::Shape4(batch_size_, 1, 1, img_.size(1) * img_.size(2));
    } else {
      out_.data.shape_ = mshadow::Shape4(batch_size_, 1, img_.size(1), img_.size(2));
    }
    out_.inst_index = NULL;
    out_.label.shape_ = mshadow::Shape2(batch_size_, 1);
    out_.label.stride_ = 1;
    out_.data.stride_ = out_.data.size(3);
    out_.batch_size = batch_size_;
    if (shuffle_) this->Shuffle();
    if (silent_ == 0) {
      mshadow::Shape<4> s = out_.data.shape_;
      printf("MNISTIterator: load %u images, shuffle=%d, shape=%u,%u,%u,%u\n",
             (unsigned)img_.size(0), shuffle_, s[0], s[1], s[2], s[3]);
    }
  }
  virtual void BeforeFirst(void) {
    this->loc_ = 0;
  }
  virtual bool Next(void) {
    if (loc_ + batch_size_ <= img_.size(0)) {
      out_.data.dptr_ = img_[loc_].dptr_;
      out_.label.dptr_ = &labels_[loc_];
      out_.inst_index = &inst_[loc_];
      loc_ += batch_size_;
      return true;
    } else{
      return false;
    }
  }
  virtual const DataBatch &Value(void) const {
    return out_;
  }
 private:
  inline void LoadImage(void) {
    utils::GzFile gzimg(path_img.c_str(), "rb");
    ReadInt(gzimg);
    int image_count = ReadInt(gzimg);
    int image_rows  = ReadInt(gzimg);
    int image_cols  = ReadInt(gzimg);

    img_.shape_ = mshadow::Shape3(image_count, image_rows, image_cols);
    img_.stride_ = img_.size(2);

    // allocate continuous memory
    img_.dptr_ = new float[img_.MSize()];
    for (int i = 0; i < image_count; ++i) {
      for (int j = 0; j < image_rows; ++j) {
        for (int k = 0; k < image_cols; ++k) {
          img_[i][j][k] = gzimg.ReadType<unsigned char>();
        }
      }
    }
    // normalize to 0-1
    img_ *= 1.0f / 256.0f;
  }
  inline void LoadLabel(void) {
    utils::GzFile gzlabel(path_label.c_str(), "rb");
    ReadInt(gzlabel);
    int labels_count =ReadInt(gzlabel);

    labels_.resize(labels_count);
    for (int i = 0; i < labels_count; ++i) {
      labels_[i] = gzlabel.ReadType<unsigned char>();
      inst_.push_back((unsigned)i + inst_offset_);
    }
  }
  inline void Shuffle(void) {
    rnd.Shuffle(inst_);
    std::vector<float> tmplabel(labels_.size());
    mshadow::TensorContainer<cpu,3> tmpimg(img_.shape_);
    for (size_t i = 0; i < inst_.size(); ++ i) {
      unsigned ridx = inst_[i] - inst_offset_;
      mshadow::Copy(tmpimg[i], img_[ridx]);
      tmplabel[i] = labels_[ridx];
    }
    // copy back
    mshadow::Copy(img_, tmpimg);
    labels_ = tmplabel;
  }
 private:
  inline static int ReadInt(utils::IStream &fi) {
    unsigned char buf[4];
    utils::Assert(fi.Read(buf, sizeof(buf)) == sizeof(buf), "Failed to read an int\n");
    return int(buf[0] << 24 | buf[1] << 16 | buf[2] << 8 | buf[3]);
  }
 private:
  /*! \brief silent */
  int silent_;
  /*! \brief path */
  std::string path_img, path_label;
  /*! \brief output */
  DataBatch out_;
  /*! \brief whether do shuffle */
  int shuffle_;
  /*! \brief data mode */
  int mode_;
  /*! \brief current location */
  index_t loc_;
  /*! \brief batch size */
  index_t batch_size_;
  /*! \brief image content */
  mshadow::Tensor<cpu, 3> img_;
  /*! \brief label content */
  std::vector<float> labels_;
  /*! \brief instance index offset */
  unsigned inst_offset_;
  /*! \brief instance index */
  std::vector<unsigned> inst_;
  // random sampler
  utils::RandomSampler rnd;
  // magic number to setup randomness
  static const int kRandMagic = 0;
}; //class MNISTIterator
}  // namespace cxxnet
#endif  // CXXNET_ITER_MNIST_INL_HPP_
####$$$$ cxxnet-master\cxxnet-master\src\io/iter_thread_imbin-inl.hpp
#ifndef ITER_THREAD_IMBIN_INL_HPP_
#define ITER_THREAD_IMBIN_INL_HPP_
/*!
 * \file cxxnet_iter_thread_imbin-inl.hpp
 * \brief threaded version of page iterator
 * \author Tianqi Chen
 */
#include "data.h"
#include <cstdlib>
#include <opencv2/opencv.hpp>
#include "../utils/thread_buffer.h"
#include "../utils/utils.h"

namespace cxxnet {
/*! \brief thread buffer iterator */
class ThreadImagePageIterator: public IIterator<DataInst> {
public:
  ThreadImagePageIterator(void) {
    idx_ = 0;
    img_.set_pad(false);
    label_.set_pad(false);
    fplst_ = NULL;
    silent_ = 0;
    itr.SetParam("buffer_size", "4");
    page_.page = NULL;
    img_conf_prefix_ = "";
    flag_ = true;
    label_width_ = 1;
    dist_num_worker_ = 0;
    dist_worker_rank_ = 0;
  }
  virtual ~ThreadImagePageIterator(void) {
    if (fplst_ != NULL) fclose(fplst_);
  }
  virtual void SetParam(const char *name, const char *val) {
    if (!strcmp(name, "image_list")) {
      raw_imglst_ += val;
      raw_imglst_ += ",";
      path_imglst_.push_back(std::string(val));
    }
    if (!strcmp(name, "image_bin")) {
      raw_imgbin_ += val;
      raw_imgbin_ += ",";
      path_imgbin_.push_back(std::string(val));
    }
    if (!strcmp(name, "image_conf_prefix")) {
      img_conf_prefix_ = val;
    }
    if (!strcmp(name, "image_conf_ids")) {
      img_conf_ids_ = val;
    }
    if (!strcmp(name, "dist_num_worker")) {
      dist_num_worker_ = atoi(val);
    }
    if (!strcmp(name, "dist_worker_rank")) {
      dist_worker_rank_ = atoi(val);
    }
    if (!strcmp(name, "silent")) silent_ = atoi(val);
    if (!strcmp(name, "label_width")) {
      label_width_ = atoi(val);
    }
  }
  virtual void Init(void) {
    this->ParseImageConf();
    fplst_  = utils::FopenCheck(path_imglst_[0].c_str(), "r");
    if (silent_ == 0) {
      if (img_conf_prefix_.length() == 0) {
        printf("ThreadImagePageIterator:image_list=%s, bin=%s\n",
               raw_imglst_.c_str(), raw_imgbin_.c_str());
      } else {
        printf("ThreadImagePageIterator:image_conf=%s, image_ids=%s\n",
               img_conf_prefix_.c_str(), img_conf_ids_.c_str());
      }
    }
    utils::Check(path_imgbin_.size() == path_imglst_.size(),
                 "List/Bin number not consist");
    label_.Resize(mshadow::Shape1(label_width_));
    out_.label = label_;
    itr.get_factory().path_imgbin = path_imgbin_;
    itr.get_factory().Ready();
    itr.Init();
    this->BeforeFirst();
  }
  virtual void BeforeFirst(void) {
    if (path_imglst_.size() == 1) {
      fseek(fplst_ , 0, SEEK_SET);
    } else {
      if (fplst_) fclose(fplst_);
      idx_ = 0;
      fplst_  = utils::FopenCheck(path_imglst_[0].c_str(), "r");
    }
    itr.BeforeFirst();
    this->LoadNextPage();
    flag_ = true;
  }
  virtual bool Next(void) {
    if (!flag_) return flag_;
    while (fscanf(fplst_, "%u", &out_.index) == 1) {
      for (int i = 0; i < label_width_; ++i) {
        float tmp;
        utils::Check(fscanf(fplst_, "%f", &tmp) == 1,
                     "ImageList format:label_width=%d but only have %d labels per line",
                     label_width_, i);
        label_[i] = tmp;
      }
      utils::Assert(fscanf(fplst_, "%*[^\n]\n") == 0, "ignore");
      this->NextBuffer(buf_);
      this->LoadImage(img_, out_, buf_);
      return true;
    }
    idx_ += 1;
    idx_ %= path_imglst_.size();
    if (idx_ == 0 || path_imglst_.size() == 1) {
      flag_ = false;
      return flag_;
    } else {
      if (fplst_) fclose(fplst_);
      fplst_  = utils::FopenCheck(path_imglst_[idx_].c_str(), "r");
      return Next();
    }
  }
  virtual const DataInst &Value(void) const {
    return out_;
  }
protected:
  inline static void LoadImage(mshadow::TensorContainer<cpu, 3> &img,
                               DataInst &out,
                               std::vector<unsigned char> &buf) {
    cv::Mat res = cv::imdecode(buf, 1);
    utils::Assert(res.data != NULL, "decoding fail");

    img.Resize(mshadow::Shape3(3, res.rows, res.cols));
    for (index_t y = 0; y < img.size(1); ++y) {
      for (index_t x = 0; x < img.size(2); ++x) {
        cv::Vec3b bgr = res.at<cv::Vec3b>(y, x);
        // store in RGB order
        img[2][y][x] = bgr[0];
        img[1][y][x] = bgr[1];
        img[0][y][x] = bgr[2];
      }
    }
    out.data = img;
    // free memory
    res.release();
  }
  inline void NextBuffer(std::vector<unsigned char> &buf) {
    while (ptop_ >= page_.page->Size()) {
      this->LoadNextPage();
    }
    utils::BinaryPage::Obj obj = (*page_.page)[ ptop_ ];
    buf.resize(obj.sz);
    memcpy(&buf[0], obj.dptr, obj.sz);
    ++ ptop_;
  }
  inline void LoadNextPage(void) {
    utils::Assert(itr.Next(page_), "can not get first page");
    ptop_ = 0;
  }

protected:
  /*! \brief internal flag */
  bool flag_;
  /*! \brief internal index */
  int idx_;
  /*! \brief number of distributed worker */
  int dist_num_worker_, dist_worker_rank_;
  /*! \brief output data */
  DataInst out_;
  /*! \brief label-width */
  int label_width_;
  /*! \brief silent */
  int silent_;
  /*! \brief file pointer to list file, information file */
  FILE *fplst_;
  /*! \brief prefix path of image binary, path to input lst */
  // format: imageid label path
  std::vector<std::string> path_imgbin_, path_imglst_;
  /*! \brief configuration bing */
  std::string img_conf_prefix_, img_conf_ids_;
  /*! \brief raw image list */
  std::string raw_imglst_, raw_imgbin_;
  /*! \brief temp storage for label */
  mshadow::TensorContainer<cpu, 1> label_;
  /*! \brief temp storage for image */
  mshadow::TensorContainer<cpu, 3> img_;
  /*! \brief temp memory buffer */
  std::vector<unsigned char> buf_;
  /*! \brief parse configure file */
  inline void ParseImageConf(void) {
    // handling for hadoop
    const char *ps_rank = getenv("PS_RANK");
    if (ps_rank != NULL) {
      this->SetParam("dist_worker_rank", ps_rank);
    }
    if (img_conf_prefix_.length() == 0) return;
    utils::Check(path_imglst_.size() == 0 &&
                 path_imgbin_.size() == 0,
                 "you can either set image_conf_prefix or image_bin/image_list");
    int lb, ub;
    utils::Check(sscanf(img_conf_ids_.c_str(), "%d-%d", &lb, &ub) == 2,
                 "image_conf_ids only support range, like 1-100");
    int n = ub + 1 - lb;
    if (dist_num_worker_ > 1) {
      int step = (n + dist_num_worker_ - 1) / dist_num_worker_;
      int begin = std::min(dist_worker_rank_ * step, n) + lb;
      int end = std::min((dist_worker_rank_ + 1) * step, n) + lb;
      lb = begin; ub = end - 1;
      utils::Check(lb <= ub,
                   "ThreadImagePageIterator: too many workers"\
                   "such that idlist cannot be divided between them");
    }
    for (int i = lb; i <= ub; ++i) {
      std::string tmp;
      tmp.resize(img_conf_prefix_.length() + 30);
      sprintf(&tmp[0], img_conf_prefix_.c_str(), i);
      tmp.resize(strlen(tmp.c_str()));
      path_imglst_.push_back(tmp + ".lst");
      path_imgbin_.push_back(tmp + ".bin");
    }
  }
private:
  struct PagePtr {
    utils::BinaryPage *page;
  };
  struct Factory {
  public:
    utils::StdFile fi;
    std::vector<std::string> path_imgbin;
    int idx;
    bool flag;
  public:
    Factory() : idx(0), flag(true) {}
    inline bool Init() {
      return true;
    }
    inline void SetParam(const char *name, const char *val) {}
    inline void Ready() {
      fi.Open(path_imgbin[idx].c_str(), "rb");
    }
    inline bool LoadNext(PagePtr &val) {
      if (!flag) return flag;
      bool res = val.page->Load(fi);
      if (res) {
        return res;
      } else {
        idx += 1;
        idx %= path_imgbin.size();
        if (idx == 0) {
          flag = false;
          return flag;
        } else {
          fi.Close();
          fi.Open(path_imgbin[idx].c_str(), "rb");
          return val.page->Load(fi);
        }
      }
    }
    inline PagePtr Create(void) {
      PagePtr a; a.page = new utils::BinaryPage();
      return a;
    }
    inline void FreeSpace(PagePtr &a) {
      delete a.page;
    }
    inline void Destroy() {
      fi.Close();
    }
    inline void BeforeFirst() {
      if (path_imgbin.size() == 1) {
        fi.Seek(0);
      } else {
        idx = 0;
        fi.Close();
        fi.Open(path_imgbin[idx].c_str(), "rb");
      }
      flag = true;
    }
  };
protected:
  PagePtr page_;
  int     ptop_;
  utils::ThreadBuffer<PagePtr, Factory> itr;
}; // class ThreadImagePageIterator
}; // namespace cxxnet
#endif
####$$$$ cxxnet-master\cxxnet-master\src\io/iter_thread_imbin_x-inl.hpp
#ifndef ITER_THREAD_IMBIN_X_INL_HPP_
#define ITER_THREAD_IMBIN_X_INL_HPP_
/*!
 * \file cxxnet_iter_thread_imbin-inl.hpp
 * \brief threaded version of page iterator
 * \author Tianqi Chen
 */
#include "data.h"
#include <cstdlib>
#include <omp.h>
#include "../utils/thread_buffer.h"
#include "../utils/utils.h"
#include "../utils/decoder.h"
#include "../utils/random.h"

namespace cxxnet {
/*! \brief thread buffer iterator */
class ThreadImagePageIteratorX: public IIterator<DataInst> {
public:
  ThreadImagePageIteratorX(void) {
    silent_ = 0;
    itrpage.SetParam("buffer_size", "2");
    itrimg.SetParam("buffer_size", "256");
    img_conf_prefix_ = "";
    dist_num_worker_ = 0;
    dist_worker_rank_ = 0;
  }
  virtual ~ThreadImagePageIteratorX(void) {
  }
  virtual void SetParam(const char *name, const char *val) {
    if (!strcmp(name, "image_list")) {
      raw_imglst_ += val;
      raw_imglst_ += ",";
      path_imglst_.push_back(std::string(val));
    }
    if (!strcmp(name, "image_bin")) {
      raw_imgbin_ += val;
      raw_imgbin_ += ",";
      path_imgbin_.push_back(std::string(val));
    }
    if (!strcmp(name, "image_conf_prefix")) {
      img_conf_prefix_ = val;
    }
    if (!strcmp(name, "image_conf_ids")) {
      img_conf_ids_ = val;
    }
    if (!strcmp(name, "dist_num_worker")) {
      dist_num_worker_ = atoi(val);
    }
    if (!strcmp(name, "dist_worker_rank")) {
      dist_worker_rank_ = atoi(val);
    }
    if (!strcmp(name, "silent")) silent_ = atoi(val);
    itrpage.get_factory().SetParam(name, val);
    itrimg.get_factory().SetParam(name, val);
  }
  virtual void Init(void) {
    this->ParseImageConf();
    if (silent_ == 0) {
      if (img_conf_prefix_.length() == 0) {
        printf("ThreadImagePageIterator:image_list=%s, bin=%s\n",
               raw_imglst_.c_str(), raw_imgbin_.c_str());
      } else {
        printf("ThreadImagePageIterator:image_conf=%s, image_ids=%s\n",
               img_conf_prefix_.c_str(), img_conf_ids_.c_str());
      }
    }
    utils::Check(path_imgbin_.size() == path_imglst_.size(),
                 "List/Bin number not consist");
    itrpage.get_factory().path_imgbin = path_imgbin_;
    itrpage.get_factory().path_imglst = path_imglst_;
    itrpage.Init();
    itrimg.get_factory().itrpage = &itrpage;
    itrimg.Init();
    this->BeforeFirst();
  }
  virtual void BeforeFirst(void) {
    itrimg.BeforeFirst();
  }
  virtual bool Next(void) {
    if (itrimg.Next(outimg_)) {
      out_.index = outimg_->inst_index;
      out_.label = outimg_->label;
      out_.data = outimg_->img;
      return true;
    } else {
      return false;
    }
  }
  virtual const DataInst &Value(void) const {
    return out_;
  }

 protected:
  /*! \brief number of distributed worker */
  int dist_num_worker_, dist_worker_rank_;
  /*! \brief output data */
  DataInst out_;
  /*! \brief silent */
  int silent_;
  /*! \brief prefix path of image binary, path to input lst */
  // format: imageid label path
  std::vector<std::string> path_imgbin_, path_imglst_;
  /*! \brief configuration bing */
  std::string img_conf_prefix_, img_conf_ids_;
  /*! \brief raw image list */
  std::string raw_imglst_, raw_imgbin_;
  /*! \brief parse configure file */
  inline void ParseImageConf(void) {
    // handling for hadoop
    const char *ps_rank = getenv("PS_RANK");
    if (ps_rank != NULL) {
      this->SetParam("dist_worker_rank", ps_rank);
    }
    if (img_conf_prefix_.length() == 0) return;
    utils::Check(path_imglst_.size() == 0 &&
                 path_imgbin_.size() == 0,
                 "you can either set image_conf_prefix or image_bin/image_list");
    int lb, ub;
    utils::Check(sscanf(img_conf_ids_.c_str(), "%d-%d", &lb, &ub) == 2,
                 "image_conf_ids only support range, like 1-100");
    int n = ub + 1 - lb;
    if (dist_num_worker_ > 1) {
      int step = (n + dist_num_worker_ - 1) / dist_num_worker_;
      int begin = std::min(dist_worker_rank_ * step, n) + lb;
      int end = std::min((dist_worker_rank_ + 1) * step, n) + lb;
      lb = begin; ub = end - 1;
      utils::Check(lb <= ub,
                   "ThreadImagePageIterator: too many workers"\
                   "such that idlist cannot be divided between them");
    }
    for (int i = lb; i <= ub; ++i) {
      std::string tmp;
      tmp.resize(img_conf_prefix_.length() + 30);
      sprintf(&tmp[0], img_conf_prefix_.c_str(), i);
      tmp.resize(strlen(tmp.c_str()));
      path_imglst_.push_back(tmp + ".lst");
      path_imgbin_.push_back(tmp + ".bin");
    }
  }

private:
  // load page data structure
  struct PageEntry {
    utils::BinaryPage page;
    std::vector<float> labels;
    std::vector<unsigned> inst_index;
  };
  // factory to load page
  struct PageFactory {
   public:
    // list of bin path
    std::vector<std::string> path_imgbin;
    // list of img list path
    std::vector<std::string> path_imglst;
    // constructor
    PageFactory(void) {
      label_width = 1;
      list_ptr = 0;
      fplist = NULL;
      shuffle = 0;
      rnd.Seed(kRandMagic);
    }
    inline void SetParam(const char *name, const char *val) {
      if (!strcmp(name, "label_width")) {
        label_width = atoi(val);
      }
      if (!strcmp(name, "shuffle")) {
        shuffle = atoi(val);
      }
      if (!strcmp(name, "seed_data")) {
        rnd.Seed(atoi(val) + kRandMagic);
      }
    }
    inline bool Init(void) {
      list_order.resize(path_imgbin.size());
      for (size_t i = 0; i < path_imgbin.size(); ++i) {
        list_order[i] = i;
      }
      if (shuffle != 0) {
        rnd.Shuffle(list_order);
      }
      // load in data
      list_ptr = 0;
      fi.Open(path_imgbin[list_order[0]].c_str(), "rb");
      fplist = utils::FopenCheck(path_imglst[list_order[0]].c_str(), "r");
      return true;
    }
    inline void BeforeFirst(void) {
      list_ptr = 0;
      if (path_imgbin.size() == 1) {
        fi.Seek(0);
        fseek(fplist, 0, SEEK_SET);
      } else {
        if (shuffle != 0) {
          rnd.Shuffle(list_order);
        }
        fi.Close();
        fi.Open(path_imgbin[list_order[0]].c_str(), "rb");
        if (fplist != NULL) fclose(fplist);
        fplist = utils::FopenCheck(path_imglst[list_order[0]].c_str(), "r");
      }
    }
    inline PageEntry *Create(void) {
      return new PageEntry();
    }
    inline bool LoadNext(PageEntry *&a) {
      while (true) {
        if (a->page.Load(fi)) {
          a->labels.resize(a->page.Size() * label_width);
          a->inst_index.resize(a->page.Size());
          for (int i = 0; i < a->page.Size(); ++i) {
            utils::Check(fscanf(fplist, "%u", &(a->inst_index[i])) == 1,
                         "invalid list format");
            for (int j = 0; j < label_width; ++j) {
              utils::Check(fscanf(fplist, "%f", &(a->labels[i * label_width + j])) == 1,
                           "ImageList format:label_width=%u but only have %d labels per line",
                           label_width, j);

            }
            utils::Assert(fscanf(fplist, "%*[^\n]\n") == 0, "ignore");
          }
          return true;
        } else {
          list_ptr += 1;
          if (list_ptr >= list_order.size()) return false;
          fi.Close();
          fi.Open(path_imgbin[list_order[list_ptr]].c_str(), "rb");
          if (fplist != NULL) fclose(fplist);
          fplist = utils::FopenCheck(path_imglst[list_order[list_ptr]].c_str(), "r");
        }
      }
    }
    inline void FreeSpace(PageEntry *&a) {
      delete a;
    }
    inline void Destroy() {
      fi.Close();
      if (fplist != NULL) fclose(fplist);
    }

   private:
    // file stream for binary page
    utils::StdFile fi;
    // seq of list index
    std::vector<size_t> list_order;
    /*! \brief label-width */
    int label_width;
    // pointer for each list
    size_t list_ptr;
    // file ptr for list
    FILE *fplist;
    // shuffle
    int shuffle;
    // random sampler
    utils::RandomSampler rnd;
    // magic seed number for random sampler
    static const int kRandMagic = 121;
  };
  // put everything in inst entry
  struct ImageEntry {
    // insance index
    unsigned inst_index;
    // label of each instance
    mshadow::TensorContainer<cpu, 1> label;
    // image data
    mshadow::TensorContainer<cpu, 3> img;
    ImageEntry() : label(false), img(false) {}
  };
  struct ImageFactory {
  public:
    // page iterator
    utils::ThreadBuffer<PageEntry*, PageFactory> *itrpage;
    // constructor
    ImageFactory(void) {
      label_width = 1;
      data_ptr = 0;
      shuffle = 0;
      end_of_data = false;
      page = NULL;
      rnd.Seed(kRandMagic);
      img.set_pad(false);
    }
    inline void SetParam(const char *name, const char *val) {
      if (!strcmp(name, "label_width")) {
        label_width = atoi(val);
      }
      if (!strcmp(name, "shuffle")) {
        shuffle = atoi(val);
      }
      if (!strcmp(name, "seed_data")) {
        rnd.Seed(atoi(val) + kRandMagic);
      }
    }
    inline bool Init(void) {
      return true;
    }
    inline ImageEntry *Create(void) {
      return new ImageEntry();
    }
    inline void FreeSpace(ImageEntry *&a) {
      delete a;
    }
    inline bool LoadNext(ImageEntry *&val) {
      if (end_of_data) return false;
      while (true) {
        if (page == NULL || data_ptr >= page->page.Size()) {
          if (!itrpage->Next(page)) {
            end_of_data = true; return false;
          }
          data_ptr = 0;
          inst_order.resize(page->page.Size());
          for (int i = 0; i < page->page.Size(); ++i) {
            inst_order[i] = i;
          }
          if (shuffle != 0) {
            rnd.Shuffle(inst_order);
          }
        } else {
          const int idx = inst_order[data_ptr];
          utils::BinaryPage::Obj obj = page->page[idx];
          decoder.Decode(static_cast<unsigned char*>(obj.dptr),
                         obj.sz, &img);
          val->img.Resize(mshadow::Shape3(3, img.size(0), img.size(1)));
          // assign image
          if (img.size(2) == 3) {
            mshadow::Tensor<cpu, 3> dst = val->img;
            for (index_t i = 0; i < img.size(0); ++i) {
              for (index_t j = 0; j < img.size(1); ++j) {
                for (index_t k = 0; k < 3; ++k) {
                  dst[k][i][j] = static_cast<real_t>(img[i][j][k]);
                }
              }
            }
          } else {
            mshadow::Tensor<cpu, 3> dst = val->img;
            for (index_t i = 0; i < img.size(0); ++i) {
              for (index_t j = 0; j < img.size(1); ++j) {
                real_t s = static_cast<real_t>(img[i][j][0]);
                dst[0][i][j] = s;
                dst[1][i][j] = s;
                dst[2][i][j] = s;
              }
            }
          }
          val->label.Resize(mshadow::Shape1(label_width));
          for (int j = 0; j < label_width; ++j) {
            val->label[j] = page->labels[idx * label_width + j];
          }
          val->inst_index = page->inst_index[idx];
          data_ptr += 1;
          return true;
        }
      }
    }
    inline void Destroy() {}
    inline void BeforeFirst() {
      itrpage->BeforeFirst();
      end_of_data = false;
      page = NULL;
      data_ptr = 0;
    }
   private:
    // mark end of data
    bool end_of_data;
    // current page
    PageEntry *page;
    // seq of inst index
    std::vector<int> inst_order;
    // jpeg decoder
    #if CXXNET_USE_OPENCV_DECODER == 1
    utils::OpenCVDecoder decoder;
    #else
    utils::JpegDecoder decoder;
    #endif
    // id for data
    int data_ptr;
    // shuffle
    int shuffle;
    // label_width
    int label_width;
    // image
    mshadow::TensorContainer<cpu, 3, unsigned char> img;
    // random number generator
    utils::RandomSampler rnd;
    // magic number
    static const int kRandMagic = 111;
  };

protected:
  /*! \brief output data */
  ImageEntry *outimg_;
  utils::ThreadBuffer<PageEntry*, PageFactory> itrpage;
  utils::ThreadBuffer<ImageEntry*, ImageFactory> itrimg;
}; // class ThreadImagePageIterator
}; // namespace cxxnet
#endif
####$$$$ cxxnet-master\cxxnet-master\src\layer/activation_layer-inl.hpp
#ifndef CXXNET_LAYER_ACTIVATION_LAYER_INL_HPP_
#define CXXNET_LAYER_ACTIVATION_LAYER_INL_HPP_

#include <mshadow/tensor.h>
#include "./layer.h"
#include "./op.h"

namespace cxxnet {
namespace layer {

template<typename xpu,typename ForwardOp, typename BackOp>
class ActivationLayer : public ILayer<xpu>{
 public:
  virtual ~ActivationLayer(void) {}
  virtual void InitConnection(const std::vector<Node<xpu>*> &nodes_in,
                              const std::vector<Node<xpu>*> &nodes_out,
                              ConnectState<xpu> *p_cstate) {
    utils::Check(nodes_in.size() == 1 && nodes_out.size() == 1,
                 "ActivationLayer Layer only support 1-1 connection");
    nodes_out[0]->data.shape_ = nodes_in[0]->data.shape_;
  }
  virtual void Forward(bool is_train,
                       const std::vector<Node<xpu>*> &nodes_in,
                       const std::vector<Node<xpu>*> &nodes_out,
                       ConnectState<xpu> *p_cstate) {
    using namespace mshadow::expr;
    // InitConnection is already called, no need to check size again
    nodes_in[0]->data = F<ForwardOp>(nodes_in[0]->data);
    mshadow::Copy(nodes_out[0]->data, nodes_in[0]->data, nodes_out[0]->data.stream_);
  }
  virtual void Backprop(bool prop_grad,
                        const std::vector<Node<xpu>*> &nodes_in,
                        const std::vector<Node<xpu>*> &nodes_out,
                        ConnectState<xpu> *p_cstate) {
    using namespace mshadow::expr;
    if (prop_grad) {
      nodes_in[0]->data = F<BackOp>(nodes_in[0]->data) * nodes_out[0]->data;
    }
  }
};
}  // namespace layer
}  // namespace cxxnet
#endif  // LAYER_ACTIVATION_LAYER_INL_HPP_

####$$$$ cxxnet-master\cxxnet-master\src\layer/batch_norm_layer-inl.hpp
#ifndef BATCH_NORM_LAYER_INL_HPP_
#define BATCH_NORM_LAYER_INL_HPP_
#pragma once

#include <mshadow/tensor.h>
#include "./layer.h"
#include "./op.h"


namespace cxxnet {
namespace layer {

template<typename xpu>
class BatchNormLayer : public ILayer<xpu> {
 public:
  BatchNormLayer(mshadow::Random<xpu> *p_rnd) : prnd_(p_rnd) {
    init_slope_ = 1.0f;
    init_bias_ = 0.0f;
    eps_ = 1e-10f;
  }
  virtual void SetParam(const char *name, const char* val) {
    if (!strcmp(name, "init_slope")) init_slope_ = atof(val);
    if (!strcmp(name, "init_bias")) init_bias_ = atof(val);
    if (!strcmp(name, "eps")) eps_ = atof(val);
  }
  virtual void ApplyVisitor(typename ILayer<xpu>::IVisitor *pvisitor) {
    pvisitor->Visit("wmat", slope_, gslope_);
    pvisitor->Visit("bias", bias_, gbias_);
  }
  virtual void InitConnection(const std::vector<Node<xpu>*> &nodes_in,
                              const std::vector<Node<xpu>*> &nodes_out,
                              ConnectState<xpu> *p_cstate) {
    utils::Check(nodes_in.size() == 1 && nodes_out.size() == 1,
                 "BNLayer: only support 1-1 connection");
    in_shape_ = nodes_in[0]->data.shape_;
    if (nodes_in[0]->data.size(1) == 1){
      // This is a fc layer
      channel_ = nodes_in[0]->data.size(3);
    } else {
      // This is a conv layer
      channel_ = nodes_in[0]->data.size(1);
    }
    nodes_out[0]->data.shape_ = nodes_in[0]->data.shape_;
    p_cstate->states.resize(1);
    p_cstate->states[0].Resize(nodes_in[0]->data.shape_);
  }
  virtual void InitModel(void) {
    slope_.Resize(mshadow::Shape1(channel_));
    gslope_.Resize(mshadow::Shape1(channel_));
    exp_.Resize(mshadow::Shape1(channel_));
    var_.Resize(mshadow::Shape1(channel_));
    gexp_.Resize(slope_.shape_);
    gvar_.Resize(slope_.shape_);
    wtf_.Resize(slope_.shape_);
    bias_.Resize(slope_.shape_);
    gbias_.Resize(slope_.shape_);
    gslope_ = 0.0f;
    gexp_ = 0.0f;
    gvar_ = 0.0f;
    slope_ = init_slope_;
    bias_ = init_bias_;
  }
  virtual void SaveModel(utils::IStream &fo) const{
    slope_.SaveBinary(fo);
    bias_.SaveBinary(fo);
  }
  virtual void LoadModel(utils::IStream &fi){
    slope_.LoadBinary(fi);
    bias_.LoadBinary(fi);
    gslope_.Resize(slope_.shape_);
    exp_.Resize(slope_.shape_);
    gexp_.Resize(slope_.shape_);
    var_.Resize(slope_.shape_);
    gvar_.Resize(slope_.shape_);
    wtf_.Resize(slope_.shape_);
    gbias_.Resize(slope_.shape_);
    gslope_ = 0.0f;
    gbias_ = 0.0f;
    gexp_ = 0.0f;
    gvar_ = 0.0f;
  }
  virtual void SetStream(mshadow::Stream<xpu> *stream) {
    slope_.set_stream(stream);
    gslope_.set_stream(stream);
    exp_.set_stream(stream);
    gexp_.set_stream(stream);
    var_.set_stream(stream);
    gvar_.set_stream(stream);
    wtf_.set_stream(stream);
    bias_.set_stream(stream);
    gbias_.set_stream(stream);
  }
  virtual void OnBatchSizeChanged(const std::vector<Node<xpu>*> &nodes_in,
                                  const std::vector<Node<xpu>*> &nodes_out,
                                  ConnectState<xpu> *p_cstate) {
    p_cstate->states[0].Resize(nodes_in[0]->data.shape_);
  }
  virtual void Forward(bool is_train,
                       const std::vector<Node<xpu>*> &nodes_in,
                       const std::vector<Node<xpu>*> &nodes_out,
                       ConnectState<xpu> *p_cstate) {
    using namespace mshadow::expr;
    mshadow::Tensor<xpu, 4> &in = nodes_in[0]->data;
    mshadow::Tensor<xpu, 4> &out = nodes_out[0]->data;
    float scale = 1.0f / in.shape_.Size() * channel_;
    mshadow::TensorContainer<xpu,4> &temp_ = p_cstate->states[0];
    if (is_train) {
      mshadow::Copy(temp_, in, temp_.stream_);
      if (in.size(1) != 1) {
        exp_ = scale * sumall_except_dim<1>(in);
        var_ = scale * sumall_except_dim<1>(F<op::square>(in - broadcast<1>(exp_, in.shape_)));
        in = (in - broadcast<1>(exp_, in.shape_)) /
          F<op::square_root>(broadcast<1>(var_ + eps_, in_shape_));
        out = in * broadcast<1>(slope_, in.shape_) + broadcast<1>(bias_, in.shape_);
      } else {
        exp_ = scale * sumall_except_dim<3>(in);
        var_ = scale * sumall_except_dim<3>(F<op::square>(in - broadcast<3>(exp_, in.shape_)));
        in = (in - broadcast<3>(exp_, in.shape_)) /
          F<op::square_root>(broadcast<3>(var_ + eps_, in_shape_));
        out = in * broadcast<3>(slope_, in.shape_) + broadcast<3>(bias_, in.shape_);
      }
    } else {
      if (in.size(1) != 1) {
        exp_ = scale * sumall_except_dim<1>(in);
        var_ = scale * sumall_except_dim<1>(F<op::square>(in - broadcast<1>(exp_, in.shape_)));
        out = broadcast<1>(slope_ / F<op::square_root>(var_ + eps_), in.shape_) *
          in + broadcast<1>(bias_ - (slope_ * exp_) /
                            F<op::square_root>(var_ + eps_), in.shape_);
      } else {
        exp_ = scale * sumall_except_dim<3>(in);
        var_ = scale * sumall_except_dim<3>(F<op::square>(in - broadcast<3>(exp_, in.shape_)));
        out = broadcast<3>(slope_ / F<op::square_root>(var_  + eps_), in.shape_) *
          in + broadcast<3>(bias_ - (slope_ * exp_) /
                            F<op::square_root>(var_ + eps_), in.shape_);
      }
    }
  }
  virtual void Backprop(bool prop_grad,
                        const std::vector<Node<xpu>*> &nodes_in,
                        const std::vector<Node<xpu>*> &nodes_out,
                        ConnectState<xpu> *p_cstate) {
    using namespace mshadow::expr;
    mshadow::Tensor<xpu, 4> &in = nodes_in[0]->data;
    mshadow::Tensor<xpu, 4> &out = nodes_out[0]->data;
    float scale = 1.0f / in.shape_.Size() * channel_;
    mshadow::TensorContainer<xpu,4> &temp_ = p_cstate->states[0];
    if (in.size(1) != 1){
      gvar_ = sumall_except_dim<1>((out * broadcast<1>(slope_, in.shape_)) *
                        (temp_ - broadcast<1>(exp_, in.shape_)) *
                        -0.5f * F<op::power>(broadcast<1>(var_ + eps_, in.shape_), -1.5f));
      gexp_ = sumall_except_dim<1>(out * broadcast<1>(slope_, in.shape_));
      gexp_ *= -1.0f / F<op::square_root>(var_ + eps_);
      wtf_ = scale * sumall_except_dim<1>(-2.0f * (temp_ - broadcast<1>(exp_, in.shape_)));
      wtf_ *= gvar_;
      gexp_ += wtf_;
      gslope_ += sumall_except_dim<1>(out * in);
      gbias_ += sumall_except_dim<1>(out);
      in = (out * broadcast<1>(slope_, in.shape_)) *
           broadcast<1>(1.0f / F<op::square_root>(var_ + eps_), in.shape_) +
           broadcast<1>(gvar_, in.shape_) * scale * 2.0f * (temp_ - broadcast<1>(exp_, in.shape_)) +
           broadcast<1>(gexp_, in.shape_) * scale;
    } else {
      gvar_ = sumall_except_dim<3>((out * broadcast<3>(slope_, in.shape_)) *
                        (temp_ - broadcast<3>(exp_, in.shape_)) *
                        -0.5f * F<op::power>(broadcast<3>(var_ + eps_, in.shape_), -1.5f));
      gexp_ = sumall_except_dim<3>(out * broadcast<3>(slope_, in.shape_));
      gexp_ *= -1.0f / F<op::square_root>(var_ + eps_);
      wtf_ = scale * sumall_except_dim<3>(-2.0f * (temp_ - broadcast<3>(exp_, in.shape_)));
      wtf_ *= gvar_;
      gexp_ += wtf_;
      gslope_ += sumall_except_dim<3>(out * in);
      gbias_ += sumall_except_dim<3>(out);
      in = (out * broadcast<3>(slope_, in.shape_)) *
           broadcast<3>(1.0f / F<op::square_root>(var_ + eps_), in.shape_) +
           broadcast<3>(gvar_, in.shape_) * scale * 2.0f * (temp_ - broadcast<3>(exp_, in.shape_)) +
           broadcast<3>(gexp_, in.shape_) * scale;

    }
  }

 private:
  mshadow::Random<xpu> *prnd_;
  int channel_;
  mshadow::Shape<4> in_shape_;
  mshadow::TensorContainer<xpu, 1> wtf_;
  mshadow::TensorContainer<xpu, 1> slope_;
  mshadow::TensorContainer<xpu, 1> gslope_;
  mshadow::TensorContainer<xpu, 1> bias_;
  mshadow::TensorContainer<xpu, 1> gbias_;
  mshadow::TensorContainer<xpu, 1> exp_;
  mshadow::TensorContainer<xpu, 1> gexp_;
  mshadow::TensorContainer<xpu, 1> var_;
  mshadow::TensorContainer<xpu, 1> gvar_;
  float init_slope_;
  float init_bias_;
  float eps_;
};  // class BatchNormLayer

} // namespace layer
} // namespace cxxnet
#endif
####$$$$ cxxnet-master\cxxnet-master\src\layer/bias_layer-inl.hpp
#ifndef CXXNET_LAYER_BIAS_LAYER_INL_HPP_
#define CXXNET_LAYER_BIAS_LAYER_INL_HPP_

#include <mshadow/tensor.h>
#include "./layer.h"
#include "./param.h"
#include "../utils/utils.h"

namespace cxxnet {
namespace layer {

/*! \brief a simple layer that adds bias to every node in batch, this is a self-loop layer */
template<typename xpu>
class BiasLayer : public ILayer<xpu> {
 public:
  virtual ~BiasLayer( void ){}
  virtual void ApplyVisitor(typename ILayer<xpu>::IVisitor *pvisitor) {
    pvisitor->Visit("bias", bias_, gbias_);
  }
  virtual void SetParam(const char *name, const char* val){
    param_.SetParam(name, val);
  }
  virtual void InitModel(void) {
    bias_.Resize(mshadow::Shape1(param_.num_input_node));
    bias_ = param_.init_bias;
    gbias_.Resize(bias_.shape_);
    gbias_ = 0.0f;
  }
  virtual void SaveModel(utils::IStream &fo) const{
    fo.Write(&param_, sizeof(LayerParam));
    bias_.SaveBinary(fo);
  }
  virtual void LoadModel(utils::IStream &fi){
    utils::Check(fi.Read(&param_, sizeof(LayerParam)) != 0,
                 "BiasLayer: LoadModel invalid model file");
    bias_.LoadBinary(fi);
    gbias_.Resize(bias_.shape_);
    gbias_ = 0.0f;
  }
  virtual void SetStream(mshadow::Stream<xpu> *stream) {
    bias_.set_stream(stream);
    gbias_.set_stream(stream);
  }
  virtual void InitConnection(const std::vector<Node<xpu>*> &nodes_in,
                              const std::vector<Node<xpu>*> &nodes_out,
                              ConnectState<xpu> *p_cstate) {
    utils::Check(nodes_in.size() == 1 && nodes_out.size() == 1,
                 "BiasLayer Layer only support 1-1 connection");
    utils::Check(nodes_in[0] == nodes_out[0], "BiasLayer is an self-loop Layer");
    utils::Check(nodes_in[0]->is_mat(), "BiasLayer only works for flatten node so far");
    if (param_.num_input_node == 0) {
      param_.num_input_node = static_cast<int>(nodes_in[0]->data.size(3));
    } else {
      utils::Check(param_.num_input_node == static_cast<int>(nodes_in[0]->data.size(3)),
                   "BiasLayer: input hidden nodes is not consistent");
    }
  }
  virtual void Forward(bool is_train,
                       const std::vector<Node<xpu>*> &nodes_in,
                       const std::vector<Node<xpu>*> &nodes_out,
                       ConnectState<xpu> *p_cstate) {
    using namespace mshadow::expr;
    // InitConnection is already called, no need to check shape again
    mshadow::index_t nbatch = nodes_in[0]->data.size(0);
    nodes_in[0]->mat() += repmat(bias_, nbatch);
  }
  virtual void Backprop(bool prop_grad,
                        const std::vector<Node<xpu>*> &nodes_in,
                        const std::vector<Node<xpu>*> &nodes_out,
                        ConnectState<xpu> *p_cstate) {
    using namespace mshadow::expr;
    gbias_ += sum_rows(nodes_in[0]->mat());
  }

 private:
  /*! \brief parameters that potentially be useful */
  LayerParam param_; 
  /*! \brief bias */
  mshadow::TensorContainer<xpu,1> bias_;
  /*! \brief accumulates the gradient of bias */
  mshadow::TensorContainer<xpu,1> gbias_;
};
}  // namespace layer
}  // namespace cxxnet
#endif // LAYER_BIAS_LAYER_INL_HPP_

####$$$$ cxxnet-master\cxxnet-master\src\layer/concat_layer-inl.hpp
#ifndef CXXNET_LAYER_CONCAT_LAYER_INL_HPP_
#define CXXNET_LAYER_CONCAT_LAYER_INL_HPP_

#include "./layer.h"
#include "./op.h"


namespace cxxnet {
namespace layer {

template<typename xpu, int dim>
class ConcatLayer : public ILayer<xpu> {
 public:
  virtual void InitConnection(const std::vector<Node<xpu>*> &nodes_in,
                              const std::vector<Node<xpu>*> &nodes_out,
                              ConnectState<xpu> *p_cstate) {
    utils::Check(nodes_in.size() > 1 && nodes_out.size() == 1,
                 "Concat layer only support n-1 connection");
    utils::Check(nodes_in.size() <= 4, "More than 4 input node is unspported");
    mshadow::Shape<4> oshape = nodes_in[0]->data.shape_;
    mshadow::index_t out_ch = 0;
    for (mshadow::index_t i = 0; i < nodes_in.size(); ++i) {
      out_ch += nodes_in[i]->data.shape_[dim];
      for (mshadow::index_t j = 0; j < 4; ++j) {
        if (j == dim) continue;
        utils::Check(nodes_in[i]->data.shape_[j] == oshape[j],
                     "Concat shape doesn't match");
      }
    }
    oshape[dim] = out_ch;
    nodes_out[0]->data.shape_ = oshape;
  }
  virtual void Forward(bool is_train,
                       const std::vector<Node<xpu>*> &nodes_in,
                       const std::vector<Node<xpu>*> &nodes_out,
                       ConnectState<xpu> *p_cstate) {
    using namespace mshadow::expr;
    switch(nodes_in.size()) {
    case 2:
      nodes_out[0]->data = concat<dim>(nodes_in[0]->data, nodes_in[1]->data);
      break;
    case 3:
      nodes_out[0]->data = concat<dim>(nodes_in[0]->data,
                                     concat<dim>(nodes_in[1]->data, nodes_in[2]->data));
      break;
    case 4:
      nodes_out[0]->data = concat<dim>(concat<dim>(nodes_in[0]->data, nodes_in[1]->data),
                                     concat<dim>(nodes_in[2]->data, nodes_in[3]->data));
      break;
    default:
      utils::Error("Too many node to concat");
      break;
    };
  }
  virtual void Backprop(bool prop_grad,
                        const std::vector<Node<xpu>*> &nodes_in,
                        const std::vector<Node<xpu>*> &nodes_out,
                        ConnectState<xpu> *p_cstate) {
    using namespace mshadow::expr;
    if (prop_grad) {
      switch(nodes_in.size()) {
      case 2:
        concat<dim>(nodes_in[0]->data, nodes_in[1]->data) = nodes_out[0]->data;
        break;
      case 3:
        concat<dim>(nodes_in[0]->data,
                  concat<dim>(nodes_in[1]->data, nodes_in[2]->data)) = nodes_out[0]->data;
        break;
      case 4:
        concat<dim>(concat<dim>(nodes_in[0]->data, nodes_in[1]->data),
                  concat<dim>(nodes_in[2]->data, nodes_in[3]->data)) = nodes_out[0]->data;
        break;
      default:
        utils::Error("Too many nodes to concat");
        break;
      };
    }
  }
}; //class ConcatLayer
} // namespace layer
} // namespace cxxnet
#endif
####$$$$ cxxnet-master\cxxnet-master\src\layer/convolution_layer-inl.hpp
#ifndef CXXNET_LAYER_CONVOLUTION_LAYER_INL_HPP_
#define CXXNET_LAYER_CONVOLUTION_LAYER_INL_HPP_

#include <mshadow/tensor.h>
#include "./layer.h"
#include "./param.h"
#include "../utils/utils.h"

namespace cxxnet {
namespace layer {

template<typename xpu>
class ConvolutionLayer : public ILayer<xpu> {
 public:
  ConvolutionLayer(mshadow::Random<xpu> *p_rnd)
      : prnd_(p_rnd), wmat_(false), bias_(false), gwmat_(false), gbias_(false) {}
  virtual ~ConvolutionLayer(void) {}
  virtual void SetParam(const char *name, const char* val) {
    param_.SetParam(name, val);
  }
  virtual void ApplyVisitor(typename ILayer<xpu>::IVisitor *pvisitor) {
    pvisitor->Visit("wmat", wmat_, gwmat_);
    if (param_.no_bias == 0) {
      pvisitor->Visit("bias", bias_, gbias_);
    }
  }
  virtual void InitModel(void) {
    // resize to correct shape, use 2d to store the weight, since we use dot
    wmat_.Resize(mshadow::Shape3(param_.num_group, param_.num_channel / param_.num_group,
                                 param_.num_input_channel / param_.num_group * param_.kernel_height * param_.kernel_width));
    bias_.Resize(mshadow::Shape1(param_.num_channel));
    param_.RandInitWeight(this->prnd_, wmat_, wmat_.size(2), wmat_.size(1));
    bias_ = param_.init_bias;
    // setup gradient
    gwmat_.Resize(wmat_.shape_);
    gbias_.Resize(bias_.shape_);
    gwmat_ = 0.0f; gbias_ = 0.0f;
  }
  virtual void SaveModel(utils::IStream &fo) const {
    fo.Write(&param_, sizeof(LayerParam));
    wmat_.SaveBinary(fo);
    bias_.SaveBinary(fo);
  }
  virtual void LoadModel(utils::IStream &fi) {
    utils::Check(fi.Read(&param_, sizeof(LayerParam)) != 0,
                  "ConvolutionLayer: LoadModel invalid model file");
    wmat_.LoadBinary(fi);
    bias_.LoadBinary(fi);
    // setup gradient
    gwmat_.Resize(wmat_.shape_);
    gbias_.Resize(bias_.shape_);
    gwmat_ = 0.0f; gbias_ = 0.0f;
  }
  virtual void SetStream(mshadow::Stream<xpu> *stream) {
    // stream of wmat and bias may be reset, but it is ok
    wmat_.set_stream(stream);
    bias_.set_stream(stream);
    gwmat_.set_stream(stream);
    gbias_.set_stream(stream);
    temp_dst_.set_stream(stream);
    temp_col_.set_stream(stream);
  }
  virtual void InitConnection(const std::vector<Node<xpu>*> &nodes_in,
                              const std::vector<Node<xpu>*> &nodes_out,
                              ConnectState<xpu> *p_cstate) {
    this->InitNode(nodes_in, nodes_out);
    this->InitTemp(nodes_in[0]->data.shape_,
                   nodes_out[0]->data.shape_);
  }
  virtual void Forward(bool is_train,
                       const std::vector<Node<xpu>*> &nodes_in,
                       const std::vector<Node<xpu>*> &nodes_out,
                       ConnectState<xpu> *p_cstate) {
    using namespace mshadow::expr;
    mshadow::Tensor<xpu, 4> &in = nodes_in[0]->data;
    mshadow::Tensor<xpu, 4> &out = nodes_out[0]->data;
    this->InitTemp(in.shape_, out.shape_);
    const index_t nbatch = in.size(0);
    for (index_t i = 0; i < nbatch; i += nstep_) {
      // resize, incase last batch is smaller
      const index_t step = std::min(nstep_, nbatch - i);
      temp_col_.Resize(mshadow::Shape2(shape_colunit_[0], shape_colunit_[1] * step));
      temp_dst_.Resize(mshadow::Shape3(shape_dstunit_[0], shape_dstunit_[1], shape_dstunit_[2] * step));

      if (param_.pad_x == 0 && param_.pad_y == 0) {
        temp_col_ = unpack_patch2col(in.Slice(i, i+step), param_.kernel_height, param_.kernel_width, param_.stride);
      }else{
        temp_col_ = unpack_patch2col(pad(in.Slice(i,i+step), param_.pad_y, param_.pad_x),
                                     param_.kernel_height, param_.kernel_width, param_.stride);
      }

      const index_t gstride = temp_col_.size(0) / param_.num_group;
      for (int gid = 0; gid < param_.num_group; ++ gid) {
        mshadow::Tensor<xpu,2> tmpc = temp_col_.Slice(gstride * gid, gstride * (gid + 1));
        temp_dst_[gid] = dot(wmat_[gid], tmpc);
      }
      out.Slice(i, i + step) =
          swapaxis<1,0>(reshape(temp_dst_,
                                mshadow::Shape4(param_.num_channel, step, out.size(2), out.size(3))));
      
    }
    if (param_.no_bias == 0) {
      // add bias, broadcast bias to dim 1: channel
      out += broadcast<1>(bias_, out.shape_);
    }
  }
  virtual void Backprop(bool prop_grad,
                        const std::vector<Node<xpu>*> &nodes_in,
                        const std::vector<Node<xpu>*> &nodes_out,
                        ConnectState<xpu> *p_cstate) {
    using namespace mshadow::expr;
    mshadow::Tensor<xpu, 4> &in = nodes_in[0]->data;
    mshadow::Tensor<xpu, 4> &out = nodes_out[0]->data;
    this->InitTemp(in.shape_, out.shape_);
    const index_t nbatch = in.size(0);

    if (param_.no_bias == 0) {
      gbias_ += sumall_except_dim<1>(out);
    }

    for (index_t i = 0; i < nbatch; i += nstep_) {
      const index_t step = std::min(nstep_, nbatch-i);
      temp_col_.Resize(mshadow::Shape2(shape_colunit_[0], shape_colunit_[1] * step));
      temp_dst_.Resize(mshadow::Shape3(shape_dstunit_[0], shape_dstunit_[1], shape_dstunit_[2] * step));

      temp_dst_ = reshape(swapaxis<1,0>(out.Slice(i, i + step)), temp_dst_.shape_);

      if (param_.pad_x == 0 && param_.pad_y == 0) {
        temp_col_ = unpack_patch2col(in.Slice(i, i + step), param_.kernel_height, param_.kernel_width, param_.stride);
      } else {
        temp_col_ = unpack_patch2col(pad(in.Slice(i,i + step),param_.pad_y, param_.pad_x), param_.kernel_height, param_.kernel_width, param_.stride);
      }

      const index_t gstride = temp_col_.size(0) / param_.num_group;
      for (int gid = 0; gid < param_.num_group; ++ gid) {
        mshadow::Tensor<xpu,2> tmpc = temp_col_.Slice(gstride * gid, gstride * (gid+1));
        gwmat_[gid] += dot(temp_dst_[gid], tmpc.T());
      }

      if (prop_grad) {
        for (int gid = 0; gid < param_.num_group; ++ gid) {
          mshadow::Tensor<xpu,2> tmpc = temp_col_.Slice(gstride * gid, gstride * (gid+1));
          tmpc = dot(wmat_[gid].T(), temp_dst_[gid]);
        }

        if (param_.pad_x == 0 && param_.pad_y == 0) {
          in.Slice(i,i+step) = pack_col2patch(temp_col_, in.Slice(i, i + step).shape_, param_.kernel_height, param_.kernel_width, param_.stride);
        }else{
          mshadow::Shape<4> pshape = in.Slice(i, i + step).shape_;
          pshape[2] += 2 * param_.pad_y; pshape[3] += 2 * param_.pad_x;
          in.Slice(i, i + step) = crop(pack_col2patch(temp_col_, pshape, param_.kernel_height, param_.kernel_width, param_.stride), in[i][0].shape_);
        }
      }
    }
  }

 protected:
    inline void InitNode(const std::vector<Node<xpu>*> &nodes_in,
                              const std::vector<Node<xpu>*> &nodes_out) {
      utils::Check(nodes_in.size() == 1 && nodes_out.size() == 1,
                 "ConvolutionLayer Layer only support 1-1 connection");
      const index_t ksize_y = static_cast<index_t>(param_.kernel_height);
      const index_t ksize_x = static_cast<index_t>(param_.kernel_width);
      const index_t kstride = static_cast<index_t>(param_.stride);
      mshadow::Shape<4> ishape = nodes_in[0]->data.shape_;
      utils::Check(ishape[1] % param_.num_group == 0,
                   "input channels must divide group size");
      utils::Check(param_.num_channel % param_.num_group == 0,
                   "output channels must divide group size");
      utils::Check(param_.num_channel > 0, "must set nchannel correctly");
      utils::Check(param_.kernel_height > 0 && param_.kernel_width > 0,
                   "must set kernel_size correctly");
      utils::Check(ksize_x <= ishape[3] && ksize_y <= ishape[2], "kernel size exceed input");
      mshadow::Shape<4> oshape = mshadow::
          Shape4(ishape[0], param_.num_channel,
                (ishape[2] + 2 * param_.pad_y - ksize_y) / kstride + 1,
                (ishape[3] + 2 * param_.pad_x - ksize_x) / kstride + 1);
      nodes_out[0]->data.shape_ = oshape;

      if (param_.num_input_channel == 0) {
        param_.num_input_channel = static_cast<int>(ishape[1]);
      } else {
        utils::Check(param_.num_input_channel == static_cast<int>(ishape[1]),
                   "ConvolutionLayer: number of input channels is not consistent");
      }
    }


  inline void InitTemp(mshadow::Shape<4> ishape, mshadow::Shape<4> oshape) {
    const index_t ksize_y = static_cast<index_t>(param_.kernel_height);
    const index_t ksize_x = static_cast<index_t>(param_.kernel_width);

    // this is the unit size of eacj temp structure
    shape_colunit_ = mshadow::Shape2(ishape[1] * ksize_y * ksize_x, oshape[2] * oshape[3]);
    shape_dstunit_ = mshadow::Shape3(param_.num_group, param_.num_channel/param_.num_group, oshape[2] * oshape[3]);
    nstep_ = std::max(std::min((index_t)(param_.temp_col_max / shape_colunit_.Size()), ishape[0]), 1U);
    // make nstep more balanced,  nstep will use exactly same number of operations to finish,
    index_t nop = (ishape[0]+nstep_-1) / nstep_;
    nstep_ = (ishape[0] + nop - 1)/ nop;
    utils::Assert(nstep_ > 0, "InitLayer_: nstep check");
    // helper structure
    temp_col_.Resize(mshadow::Shape2(shape_colunit_[0], shape_colunit_[1] * nstep_));
    temp_dst_.Resize(mshadow::Shape3(shape_dstunit_[0], shape_dstunit_[1], shape_dstunit_[2] * nstep_));
  }

  /*! \brief random number generator */
  mshadow::Random<xpu> *prnd_;
  /*! \brief parameters that potentially be useful */
  LayerParam param_;
  /*! \brief weight matrix */
  mshadow::TensorContainer<xpu,3> wmat_;
  /*! \brief bias */
  mshadow::TensorContainer<xpu,1> bias_;
  /*! \brief accumulates the gradient of weight matrix */
  mshadow::TensorContainer<xpu,3> gwmat_;
  /*! \brief accumulates the gradient of bias */
  mshadow::TensorContainer<xpu,1> gbias_;
  /*! \brief temporary data structure to store patches */
  mshadow::TensorContainer<xpu,2> temp_col_;
  /*! \brief temporary data structure to store results */
  mshadow::TensorContainer<xpu,3> temp_dst_;
  /*! \brief shape of column unit */
  mshadow::Shape<2> shape_colunit_;
  /*! \brief shape of dst unit */
  mshadow::Shape<3> shape_dstunit_;
  /*! \brief how many number of batches to be unpacked together */
  mshadow::index_t nstep_;
};
}  // namespace layer
}  // namespace cxxnet
#endif  // LAYER_CONVOLUTION_LAYER_INL_HPP_
####$$$$ cxxnet-master\cxxnet-master\src\layer/cudnn_convolution_layer-inl.hpp
#ifndef CXXNET_LAYER_CUDNN_CONVOLUTION_LAYER_INL_HPP_
#define CXXNET_LAYER_CUDNN_CONVOLUTION_LAYER_INL_HPP_
#pragma once

#include <mshadow/tensor.h>
#include "./layer.h"
#include "./param.h"
#include "../utils/utils.h"

namespace cxxnet {
namespace layer {

template<typename xpu>
class CuDNNConvolutionLayer : public ConvolutionLayer<xpu> {
 public:
  CuDNNConvolutionLayer(mshadow::Random<xpu> *p_rnd) : ConvolutionLayer<xpu>(p_rnd) {
    use_fast_algo_ = false;
  };
#ifdef __CUDACC__
#if CXXNET_USE_CUDNN == 1
  virtual ~CuDNNConvolutionLayer() {
    CUDA_CHECK(cudnnDestroyTensorDescriptor(in_desc_));
    CUDA_CHECK(cudnnDestroyTensorDescriptor(out_desc_));
    CUDA_CHECK(cudnnDestroyTensorDescriptor(bias_desc_));
    CUDA_CHECK(cudnnDestroyFilterDescriptor(filter_desc_));
    CUDA_CHECK(cudnnDestroyConvolutionDescriptor(conv_desc_));
    CUDA_CHECK(cudnnDestroy(handle_));
  };
  virtual void InitConnection(const std::vector<Node<xpu>*> &nodes_in,
                              const std::vector<Node<xpu>*> &nodes_out,
                              ConnectState<xpu> *p_cstate) {
    ConvolutionLayer<xpu>::InitNode(nodes_in, nodes_out);
    nodes_in[0]->must_contiguous = true;
    nodes_out[0]->must_contiguous = true;
    this->InitCuDNN();
  }
  virtual void SetParam(const char *name, const char* val) {
    Parent::SetParam(name, val);
    if (!strcmp(name, "algo")) {
      if (!strcmp(val, "fast")) use_fast_algo_ = true;
      else if(!strcmp(val, "balance")) use_fast_algo_ = false;
      else utils::Error("Unkown convolution algo mode");
    }
  }
  virtual void Forward(bool is_train,
                       const std::vector<Node<xpu>*> &nodes_in,
                       const std::vector<Node<xpu>*> &nodes_out,
                       ConnectState<xpu> *p_cstate) {
    float alpha = 1.0f;
    float beta = 0.0f;
    if (!init_cudnn_) {
      init_cudnn_ = true;
      if (use_fast_algo_) {
        algo_ = CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM;
      } else {
        algo_ = CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM;
      }
      temp_.set_stream(nodes_out[0]->data.stream_);
      CUDA_CHECK(cudnnSetStream(handle_, nodes_out[0]->data.stream_->stream_));
      CUDA_CHECK(cudnnSetFilter4dDescriptor(filter_desc_, dtype_,
                                            Parent::param_.num_channel,
                                            Parent::param_.num_input_channel,
                                            Parent::param_.kernel_height,
                                            Parent::param_.kernel_width));
      CUDA_CHECK(cudnnSetConvolution2dDescriptor(conv_desc_,
                                                 Parent::param_.pad_y,
                                                 Parent::param_.pad_x,
                                                 Parent::param_.stride,
                                                 Parent::param_.stride, 1, 1,
                                                 CUDNN_CROSS_CORRELATION));
      mshadow::Tensor<gpu, 4, float> &in = nodes_in[0]->data;
      mshadow::Tensor<gpu, 4, float> &out = nodes_out[0]->data;
      CUDA_CHECK(cudnnSetTensor4dDescriptor(in_desc_, CUDNN_TENSOR_NCHW, dtype_,
                                            in.shape_[0], in.shape_[1],
                                            in.shape_[2], in.shape_[3]));
      CUDA_CHECK(cudnnSetTensor4dDescriptor(out_desc_, CUDNN_TENSOR_NCHW, dtype_,
                                            out.shape_[0], out.shape_[1],
                                            out.shape_[2], out.shape_[3]));
      CUDA_CHECK(cudnnSetTensor4dDescriptor(bias_desc_, CUDNN_TENSOR_NCHW, dtype_,
                                            1, Parent::bias_.shape_[0], 1, 1));
      CUDA_CHECK(cudnnGetConvolutionForwardWorkspaceSize(handle_, in_desc_,
                                                         filter_desc_, conv_desc_,
                                                         out_desc_, algo_,
                                                         &workspace_size_));
      temp_.Resize(mshadow::Shape1(workspace_size_ / sizeof(float) + 1), 0.0f);
    }
    utils::Assert(nodes_in[0]->data.CheckContiguous(), "contiguous in conv");
    utils::Assert(nodes_out[0]->data.CheckContiguous(), "contiguous in conv");
    CUDA_CHECK(cudnnConvolutionForward(handle_, &alpha,
                                       in_desc_, nodes_in[0]->data.dptr_,
                                       filter_desc_, Parent::wmat_.dptr_,
                                       conv_desc_, algo_, temp_.dptr_, workspace_size_, &beta,
                                       out_desc_, nodes_out[0]->data.dptr_));
    if (Parent::param_.no_bias == 0) {
      beta = 1.0f;
      CUDA_CHECK(cudnnAddTensor(handle_, CUDNN_ADD_SAME_C, &alpha,
                                bias_desc_, Parent::bias_.dptr_, &beta,
                                out_desc_, nodes_out[0]->data.dptr_));
    }


  }
  virtual void Backward(bool prop_grad,
                        const std::vector<Node<xpu>*> &nodes_in,
                        const std::vector<Node<xpu>*> &nodes_out,
                        ConnectState<xpu> *p_cstate) {
    float alpha = 1.0f;
    float beta = 0.0f;
    if (Parent::param_.no_bias == 0) {
      CUDA_CHECK(cudnnConvolutionBackwardBias(handle_, &alpha,
                                              out_desc_, nodes_out[0]->data.dptr_,
                                              &beta,
                                              bias_desc_, Parent::gbias_.dptr_));
    }
    CUDA_CHECK(cudnnConvolutionBackwardFilter(handle_, &alpha,
                                              in_desc_, nodes_in[0]->data.dptr_,
                                              out_desc_, nodes_out[0]->data.dptr_,
                                              conv_desc_, &beta,
                                              filter_desc_, Parent::gwmat_.dptr_));
    CUDA_CHECK(cudnnConvolutionBackwardData(handle_, &alpha,
                                            filter_desc_, Parent::wmat_.dptr_,
                                            out_desc_, nodes_out[0]->data.dptr_,
                                            conv_desc_, &beta,
                                            in_desc_, nodes_in[0]->data.dptr_));
  }
 private:
  inline void InitCuDNN() {
    init_cudnn_ = false;
    dtype_ = CUDNN_DATA_FLOAT;
    algo_ = CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM;
    CUDA_CHECK(cudnnCreate(&handle_));
    CUDA_CHECK(cudnnCreateTensorDescriptor(&in_desc_));
    CUDA_CHECK(cudnnCreateTensorDescriptor(&out_desc_));
    CUDA_CHECK(cudnnCreateTensorDescriptor(&bias_desc_));
    CUDA_CHECK(cudnnCreateFilterDescriptor(&filter_desc_));
    CUDA_CHECK(cudnnCreateConvolutionDescriptor(&conv_desc_));
  }
  /*! \brief cuDNN init status */
  bool init_cudnn_;
  /*! \brief cuDNN handle */
  cudnnHandle_t handle_;
  /*! \brief cuDNN data type */
  cudnnDataType_t dtype_;
  /*! \brief cuDNN input tensor descriptor */
  cudnnTensorDescriptor_t in_desc_;
  /*! \brief cuDNN output tensor descriptor */
  cudnnTensorDescriptor_t out_desc_;
  /*! \brief cuDNN bias tensor descriptor */
  cudnnTensorDescriptor_t bias_desc_;
  /*! \brief cuDNN filter descriptor */
  cudnnFilterDescriptor_t filter_desc_;
  /*! \brief cuDNN conv descriptor */
  cudnnConvolutionDescriptor_t conv_desc_;
  /*! \brief cuDNN conv algorithm */
  cudnnConvolutionFwdAlgo_t algo_;
  /*! \brief cuDNN workspace size */
  size_t workspace_size_;
  /*! \brief cuDNN workspace */
  mshadow::TensorContainer<xpu, 1> temp_;
  /*! \brief parent */
  typedef ConvolutionLayer<xpu> Parent;
  /*! \brief whether use fast algorithm */
#endif // CXXNET_USE_CUDNN
#endif // __CUDACC__
  bool use_fast_algo_;
}; // class CuDNNConvolutionLayer
} // namespace layer
} // namespace cxxnet


#endif // CXXNET_LAYER_CUDNN_CONVOLUTION_LAYER_INL_HPP
####$$$$ cxxnet-master\cxxnet-master\src\layer/cudnn_pooling_layer-inl.hpp
#ifndef CXXNET_LAYER_CUDNN_POOLING_LAYER_INL_HPP_
#define CXXNET_LAYER_CUDNN_POOLING_LAYER_INL_HPP_

#include <mshadow/tensor.h>
#include "./layer.h"
#include "./param.h"

namespace cxxnet {
namespace layer {

template<typename Reducer, int mode, typename xpu>
class CuDNNPoolingLayer : public PoolingLayer<Reducer, mode, xpu> {
  private:
    typedef PoolingLayer<Reducer, mode, xpu> Parent;
  public:
    CuDNNPoolingLayer(){}
#ifdef __CUDACC__
#if CXXNET_USE_CUDNN == 1
  public:
    virtual void InitConnection(const std::vector<Node<xpu>*> &nodes_in,
                                const std::vector<Node<xpu>*> &nodes_out,
                                ConnectState<xpu> *p_cstate) {
      Parent::InitNode(nodes_in, nodes_out, p_cstate);
      this->InitCuDNN();
      nodes_in[0]->must_contiguous = true;
      nodes_out[0]->must_contiguous = true;
    }
    virtual ~CuDNNPoolingLayer(void) {
      CUDA_CHECK(cudnnDestroyTensorDescriptor(in_desc_));
      CUDA_CHECK(cudnnDestroyTensorDescriptor(out_desc_));
      CUDA_CHECK(cudnnDestroyPoolingDescriptor(pooling_desc_));
      CUDA_CHECK(cudnnDestroy(handle_));
    }

    virtual void Forward(bool is_train,
                         const std::vector<Node<xpu>*> &nodes_in,
                         const std::vector<Node<xpu>*> &nodes_out,
                         ConnectState<xpu> *p_cstate) {
      mshadow::Tensor<xpu,4> &tmp = p_cstate->states[0];
      if (!init_cudnn_) {
        init_cudnn_ = true;
        CUDA_CHECK(cudnnSetStream(handle_, nodes_out[0]->data.stream_->stream_));
        mshadow::Tensor<gpu, 4, float> &in = nodes_in[0]->data;
        mshadow::Tensor<gpu, 4, float> &out = nodes_out[0]->data;
        CUDA_CHECK(cudnnSetTensor4dDescriptor(in_desc_, CUDNN_TENSOR_NCHW, dtype_,
                                              in.shape_[0], in.shape_[1],
                                              in.shape_[2], in.shape_[3]));
        CUDA_CHECK(cudnnSetTensor4dDescriptor(out_desc_, CUDNN_TENSOR_NCHW, dtype_,
                                              out.shape_[0], out.shape_[1],
                                              out.shape_[2], out.shape_[3]));
      }
      float alpha = 1.0f;
      float beta = 0.0f;
      utils::Assert(nodes_in[0]->data.CheckContiguous(), "contiguous in conv");
      utils::Assert(nodes_out[0]->data.CheckContiguous(), "contiguous in conv");
      utils::Assert(tmp.CheckContiguous(), "contiguous in conv");
      CUDA_CHECK(cudnnPoolingForward(handle_, pooling_desc_, &alpha,
                                     in_desc_, nodes_in[0]->data.dptr_, &beta,
                                     out_desc_, tmp.dptr_));
      mshadow::Copy(nodes_out[0]->data, tmp, nodes_out[0]->data.stream_);
    }

    virtual void Backprop(bool prop_grad,
                          const std::vector<Node<xpu>*> &nodes_in,
                          const std::vector<Node<xpu>*> &nodes_out,
                          ConnectState<xpu> *p_cstate) {
      mshadow::Tensor<xpu,4> &tmp = p_cstate->states[0];
      float alpha = 1.0f;
      float beta = 0.0f;
      if (prop_grad) {
        CUDA_CHECK(cudnnPoolingBackward(handle_, pooling_desc_, &alpha,
                                        out_desc_, tmp.dptr_,
                                        out_desc_, nodes_out[0]->data.dptr_,
                                        in_desc_, nodes_in[0]->data.dptr_, &beta,
                                        in_desc_, nodes_in[0]->data.dptr_));
      }
    }
  protected:
    inline void InitCuDNN() {
      init_cudnn_ = false;
      dtype_ = CUDNN_DATA_FLOAT;
      switch(mode) {
       case kMaxPooling: mode_ = CUDNN_POOLING_MAX; break;
       case kAvgPooling: mode_ = CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING; break;
       default: utils::Error("This should not happen -,-"); break;
      }
      CUDA_CHECK(cudnnCreate(&handle_));
      CUDA_CHECK(cudnnCreateTensorDescriptor(&in_desc_));
      CUDA_CHECK(cudnnCreateTensorDescriptor(&out_desc_));
      CUDA_CHECK(cudnnCreatePoolingDescriptor(&pooling_desc_));
      CUDA_CHECK(cudnnSetPooling2dDescriptor(pooling_desc_, mode_,
                                             Parent::param_.kernel_height,
                                             Parent::param_.kernel_width,
                                             0, 0,
                                             Parent::param_.stride,
                                             Parent::param_.stride));

    }
    /*! \brief cudnn init state flag*/
    bool init_cudnn_;
    /*! \brief cuDNN data type */
    cudnnDataType_t dtype_;
    /*! \brief cudnn handle */
    cudnnHandle_t handle_;
    /*! \brief cudnn pooling mode */
    cudnnPoolingMode_t mode_;
    /*! \brief input descriptor */
    cudnnTensorDescriptor_t in_desc_;
    /*! \brief output descriptor */
    cudnnTensorDescriptor_t out_desc_;
    /*! \brief pooling descriptor */
    cudnnPoolingDescriptor_t pooling_desc_;
#endif // CXXNET_USE_CUDNN
#endif // __CUDACC__
}; // class CuDNNPoolingLayer

}  // namespace layer
}  // namespace cxxnet
#endif  // LAYER_CUDNN_POOLING_LAYER_INL_HPP_

####$$$$ cxxnet-master\cxxnet-master\src\layer/dropout_layer-inl.hpp
#ifndef CXXNET_LAYER_DROPOUT_LAYER_INL_HPP_
#define CXXNET_LAYER_DROPOUT_LAYER_INL_HPP_

#include <mshadow/tensor.h>
#include "./layer.h"
#include "./op.h"

namespace cxxnet {
namespace layer {

template<typename xpu>
class DropoutLayer : public ILayer<xpu> {
 public:
  DropoutLayer(mshadow::Random<xpu> *p_rnd) : prnd_(p_rnd) {
    // setup default value
    dropout_threshold = 0.0f;
  }
  virtual void SetParam(const char *name, const char* val) {
    if (!strcmp("threshold", name)) dropout_threshold = static_cast<real_t>(atof(val));
  }
  virtual void InitConnection(const std::vector<Node<xpu>*> &nodes_in,
                              const std::vector<Node<xpu>*> &nodes_out,
                              ConnectState<xpu> *p_cstate) {
    utils::Check(nodes_in.size() == 1 && nodes_out.size() == 1,
                 "DropoutLayer: only support 1-1 connection");
    utils::Check(nodes_in[0] == nodes_out[0], "DropoutLayer is an self-loop Layer");
    utils::Check(dropout_threshold >= 0.0f && dropout_threshold < 1.0f,
                 "DropoutLayer: invalid dropout_threshold\n");
    // use 1 temp state for mask
    p_cstate->states.resize(1);
    p_cstate->states[0].Resize(nodes_in[0]->data.shape_);
  }
  virtual void OnBatchSizeChanged(const std::vector<Node<xpu>*> &nodes_in,
                                  const std::vector<Node<xpu>*> &nodes_out,
                                  ConnectState<xpu> *p_cstate) {
    p_cstate->states[0].Resize(nodes_in[0]->data.shape_);  
  }
  virtual void Forward(bool is_train,
                       const std::vector<Node<xpu>*> &nodes_in,
                       const std::vector<Node<xpu>*> &nodes_out,
                       ConnectState<xpu> *p_cstate) {
    using namespace mshadow::expr;
    mshadow::TensorContainer<xpu,4> &mask = p_cstate->states[0];
    const real_t pkeep = 1.0f - dropout_threshold;
    if (is_train) {
      mask = F<op::threshold>(prnd_->uniform(mask.shape_), pkeep)  * (1.0f/pkeep);
      nodes_out[0]->data = nodes_out[0]->data * mask;
    }
  }
  virtual void Backprop(bool prop_grad,
                        const std::vector<Node<xpu>*> &nodes_in,
                        const std::vector<Node<xpu>*> &nodes_out,
                        ConnectState<xpu> *p_cstate) {
    using namespace mshadow::expr;
    mshadow::TensorContainer<xpu,4> &mask = p_cstate->states[0];
    if (prop_grad) {
      nodes_out[0]->data *= mask;
    }    
  }

 private:
  /*! \brief random number generator */
  mshadow::Random<xpu> *prnd_;
  /*! \brief dropout  */
  real_t dropout_threshold;
};  // class DropoutLayer
}  // namespace layer
}  // namespace cxxnet
#endif  // LAYER_DROPOUT_LAYER_INL_HPP_

####$$$$ cxxnet-master\cxxnet-master\src\layer/fixconn_layer-inl.hpp
#ifndef CXXNET_LAYER_FIXCONN_LAYER_INL_HPP_
#define CXXNET_LAYER_FIXCONN_LAYER_INL_HPP_

#include <mshadow/tensor.h>
#include "./layer.h"
#include "./param.h"
#include "./op.h"
#include "../utils/utils.h"

namespace cxxnet {
namespace layer {
// layer that fix the con weight
template<typename xpu>
class FixConnectLayer : public ILayer<xpu> {
 public:
  FixConnectLayer(void) {
    fname_weight_ = "NULL";
    init = false;
  }
  virtual void SetParam(const char *name, const char* val) {
    param_.SetParam(name, val);
    if (!strcmp(name, "fixconn_weight")) fname_weight_ = val;
  }
  virtual void SetStream(mshadow::Stream<xpu> *stream) {
    wmat_.set_stream(stream);
  }
  virtual void InitConnection(const std::vector<Node<xpu>*> &nodes_in,
                              const std::vector<Node<xpu>*> &nodes_out,
                              ConnectState<xpu> *p_cstate) {
    utils::Check(nodes_in.size() == 1 && nodes_out.size() == 1,
                 "FixConnLayer: Layer only support 1-1 connection");
    utils::Check(nodes_in[0]->is_mat(), "FixConnLayer: input need to be a matrix");
    utils::Check(param_.num_hidden > 0, "FixConncLayer: must set nhidden correctly");
    // we change matrix convention
    nodes_out[0]->data.shape_ =
        mshadow::Shape4(nodes_in[0]->data.size(0), 1, 1, param_.num_hidden);
    wmat_.Resize(mshadow::Shape2(param_.num_hidden, nodes_in[0]->mat().size(1)));
    utils::Check(fname_weight_ != "NULL", "FixConnLayer: must specify fixconn_weight");
    // mshadow::TensorContainer<cpu, 2> tmp(false);
    tmp.set_pad(false);
    tmp.Resize(wmat_.shape_); tmp = 0.0f;
    FILE *fi = utils::FopenCheck(fname_weight_.c_str(), "r");
    unsigned nrow, ncol, nonzero;
    utils::Check(fscanf(fi, "%u%u%u", &nrow, &ncol, &nonzero) == 3,
                 "FixConnLayer: fixconn_weight invalid sparse matrix format");
    utils::Check(nrow == tmp.size(0) && ncol == tmp.size(1),
                 "FixConnLayer: fixconn_weight shape do not match architecture");
    while (nonzero--) {
      float value;
      unsigned x, y;
      utils::Check(fscanf(fi, "%u%u%f", &x, &y, &value) == 3,
                   "FixConnLayer: fixconn_weight invalid sparse matrix format");
      utils::Check(x < tmp.size(0) && y < tmp.size(1),
                   "FixConnLayer: fixconn_weight index exceed matrix shape");
      tmp[x][y] = value;
    }
    fclose(fi);
    //mshadow::Stream<xpu> *stream = wmat_.stream_;
    // mshadow::Copy(wmat_, tmp, stream);
    // must wait till copy end, otherwise tmp will be de-allocated
    // if (stream != NULL) stream->Wait();
  }
  virtual void Forward(bool is_train,
                       const std::vector<Node<xpu>*> &nodes_in,
                       const std::vector<Node<xpu>*> &nodes_out,
                       ConnectState<xpu> *p_cstate) {
    using namespace mshadow::expr;
    if (!init) {
      mshadow::Copy(wmat_, tmp, wmat_.stream_);
      init = true;
    }
    nodes_out[0]->mat() = dot(nodes_in[0]->mat(), wmat_.T());
  }
  virtual void Backprop(bool prop_grad,
                        const std::vector<Node<xpu>*> &nodes_in,
                        const std::vector<Node<xpu>*> &nodes_out,
                        ConnectState<xpu> *p_cstate) {
    using namespace mshadow::expr;
    if (prop_grad) {
      nodes_in[0]->mat() = dot(nodes_out[0]->mat(), wmat_);
    }
  }
 private:
  /*! \brief name to the weight */
  std::string fname_weight_;
  /*! \brief parameters that potentially be useful */
  LayerParam param_;
  /*! \brief weight matrix */
  mshadow::TensorContainer<xpu, 2> wmat_;
  /*! \brief temp weight */
  mshadow::TensorContainer<cpu, 2> tmp;
  bool init;
};
}  // namespace layer
}  // namespace cxxnet
#endif  // CXXNET_LAYER_FIXCONN_LAYER_INL_HPP_
####$$$$ cxxnet-master\cxxnet-master\src\layer/flatten_layer-inl.hpp
#ifndef CXXNET_LAYER_FLATTEN_LAYER_INL_HPP_
#define CXXNET_LAYER_FLATTEN_LAYER_INL_HPP_

#include "./layer.h"
#include "./op.h"

namespace cxxnet {
namespace layer {

template<typename xpu>
class FlattenLayer : public ILayer<xpu> {
 public:
  virtual ~FlattenLayer(void) {}
  virtual void InitConnection(const std::vector<Node<xpu>*> &nodes_in,
                              const std::vector<Node<xpu>*> &nodes_out,
                              ConnectState<xpu> *p_cstate) {
    utils::Check(nodes_in.size() == 1 && nodes_out.size() == 1,
                 "FlattenLayer: only support 1-1 connection");
    mshadow::Shape<4> ishape = nodes_in[0]->data.shape_;
    nodes_out[0]->data.shape_ = 
        mshadow::Shape4(ishape[0], 1, 1, ishape[1] * ishape[2] * ishape[3]);
  }
  virtual void Forward(bool is_train,
                       const std::vector<Node<xpu>*> &nodes_in,
                       const std::vector<Node<xpu>*> &nodes_out,
                       ConnectState<xpu> *p_cstate) {
    using namespace mshadow::expr;    
    nodes_out[0]->data = reshape(nodes_in[0]->data, nodes_out[0]->data.shape_);
  }
  virtual void Backprop(bool prop_grad,
                        const std::vector<Node<xpu>*> &nodes_in,
                        const std::vector<Node<xpu>*> &nodes_out,
                        ConnectState<xpu> *p_cstate) {
    using namespace mshadow::expr;
    if (prop_grad) {
      nodes_in[0]->data = reshape(nodes_out[0]->data, nodes_in[0]->data.shape_);
    }    
  }
};
}  // namespace layer
}  // namespace cxxnet
#endif  // LAYER_FLATTEN_LAYER_INL_HPP_

####$$$$ cxxnet-master\cxxnet-master\src\layer/fullc_layer-inl.hpp
#ifndef CXXNET_LAYER_FULLC_LAYER_INL_HPP_
#define CXXNET_LAYER_FULLC_LAYER_INL_HPP_

#include <mshadow/tensor.h>
#include "./layer.h"
#include "./param.h"
#include "./op.h"
#include "../utils/utils.h"

namespace cxxnet {
namespace layer {

template<typename xpu>
class FullConnectLayer : public ILayer<xpu> {
 public:
  FullConnectLayer(mshadow::Random<xpu> *p_rnd) : prnd_(p_rnd) {
    fullc_gather = 0;
  }
  virtual ~FullConnectLayer(void) {}
  virtual void SetParam(const char *name, const char* val) {
    param_.SetParam(name, val);
    if (!strcmp(name, "fullc_gather")) fullc_gather = atoi(val);
    // support force contiguous option
    if (!strcmp(name, "force_contiguous") && atoi(val) != 0) {
      wmat_.set_pad(false); gwmat_.set_pad(false);
      bias_.set_pad(false); gbias_.set_pad(false);
    }
  }
  virtual void ApplyVisitor(typename ILayer<xpu>::IVisitor *pvisitor) {
    pvisitor->Visit("wmat", wmat_, gwmat_);
    if (param_.no_bias == 0) {
      pvisitor->Visit("bias", bias_, gbias_);
    }
  }
  virtual void InitModel(void) {
    // rexsize to correct shape
    wmat_.Resize(mshadow::Shape2(param_.num_hidden, param_.num_input_node));
    bias_.Resize(mshadow::Shape1(param_.num_hidden));
    param_.RandInitWeight(this->prnd_, wmat_, wmat_.size(1), wmat_.size(0));
    bias_ = param_.init_bias;
    // setup gradient weight
    gwmat_.Resize(wmat_.shape_);
    gbias_.Resize(bias_.shape_);
    gwmat_ = 0.0f; gbias_ = 0.0f;
  }
  virtual void SaveModel(utils::IStream &fo) const {
    fo.Write(&param_, sizeof(LayerParam));
    wmat_.SaveBinary(fo);
    bias_.SaveBinary(fo);
  }
  virtual void LoadModel(utils::IStream &fi) {
    utils::Check(fi.Read(&param_, sizeof(LayerParam)) != 0,
                  "FullConnectLayer:LoadModel invalid model file");    
    wmat_.LoadBinary(fi);
    bias_.LoadBinary(fi);
    // setup gradient weight
    gwmat_.Resize(wmat_.shape_);
    gbias_.Resize(bias_.shape_);
    gwmat_ = 0.0f; gbias_ = 0.0f;
  }
  virtual void SetStream(mshadow::Stream<xpu> *stream) {
    // stream of wmat and bias may be reset, but it is ok
    wmat_.set_stream(stream);
    bias_.set_stream(stream);
    gwmat_.set_stream(stream);
    gbias_.set_stream(stream);
  }
  virtual void InitConnection(const std::vector<Node<xpu>*> &nodes_in,
                              const std::vector<Node<xpu>*> &nodes_out,
                              ConnectState<xpu> *p_cstate) {
    utils::Check(nodes_in.size() == 1 && nodes_out.size() == 1,
                 "FullcLayer: Layer only support 1-1 connection");
    utils::Check(nodes_in[0]->is_mat(), "FullcLayer: input need to be a matrix");
    utils::Check(param_.num_hidden > 0, "FullcLayer: must set nhidden correctly");
    // we change matrix convention
    nodes_out[0]->data.shape_ =
        mshadow::Shape4(nodes_in[0]->data.size(0), 1, 1, param_.num_hidden);
    if (param_.num_input_node == 0) {
      param_.num_input_node = static_cast<int>(nodes_in[0]->data.size(3));
    } else {
      utils::Check(param_.num_input_node == static_cast<int>(nodes_in[0]->data.size(3)),
                   "FullcLayer: input hidden nodes is not consistent");
    }
  }
  virtual void Forward(bool is_train,
                       const std::vector<Node<xpu>*> &nodes_in,
                       const std::vector<Node<xpu>*> &nodes_out,
                       ConnectState<xpu> *p_cstate) {
    this->Forward_(is_train, wmat_, nodes_in[0], nodes_out[0]);
  }
  virtual void Backprop(bool prop_grad,
                        const std::vector<Node<xpu>*> &nodes_in,
                        const std::vector<Node<xpu>*> &nodes_out,
                        ConnectState<xpu> *p_cstate) {
    using namespace mshadow::expr;
    this->Backprop_(prop_grad, wmat_, nodes_in[0], nodes_out[0]);
  }

 protected:
  // internal implementation
  inline void Forward_(bool is_train,
                       mshadow::Tensor<xpu,2> wmat,
                       Node<xpu> *pnode_in,
                       Node<xpu> *pnode_out) {
    mshadow::Tensor<xpu, 2> m_in = pnode_in->mat();
    mshadow::Tensor<xpu, 2> m_out = pnode_out->mat();
    index_t nbatch = m_in.size(0);
    m_out = dot(m_in, wmat.T());
    if (param_.no_bias == 0) {
      m_out += repmat(bias_, nbatch);
    }
  }
  inline void Backprop_(bool prop_grad,
                        mshadow::Tensor<xpu,2> wmat,
                        Node<xpu> *pnode_in,
                        Node<xpu> *pnode_out) {
    mshadow::Tensor<xpu, 2> m_in = pnode_in->mat();
    mshadow::Tensor<xpu, 2> m_out = pnode_out->mat();
    // accumulate gradient
    if (fullc_gather == 0) {
      gwmat_ += dot(m_out.T(), m_in);
    }
    if (param_.no_bias == 0) {
      gbias_ += sum_rows(m_out);
    }
    // backprop
    if (prop_grad) {
      m_in = dot(m_out, wmat);
    }
  }

  /*! \brief random number generator */
  mshadow::Random<xpu> *prnd_;
  /*! \brief parameters that potentially be useful */
  LayerParam param_;
  /*! \brief weight matrix */
  mshadow::TensorContainer<xpu,2> wmat_;
  /*! \brief bias */
  mshadow::TensorContainer<xpu,1> bias_;
  /*! \brief accumulates the gradient of weight matrix */
  mshadow::TensorContainer<xpu,2> gwmat_;
  /*! \brief accumulates the gradient of bias */  
  mshadow::TensorContainer<xpu,1> gbias_;
  /*! \brief use gather to do fullc */
  int fullc_gather;
};
}  // namespace layer
}  // namespace cxxnet
#endif  // LAYER_FULLC_LAYER_INL_HPP_
####$$$$ cxxnet-master\cxxnet-master\src\layer/insanity_layer-inl.hpp
#ifndef INSANITY_LAYER_INL_HPP
#define INSANITY_LAYER_INL_HPP
#pragma once

#include <mshadow/tensor.h>
#include "./layer.h"
#include "./op.h"


namespace cxxnet {
namespace layer {

template<typename xpu>
class InsanityLayer : public ILayer<xpu> {
 public:
  InsanityLayer(mshadow::Random<xpu> *p_rnd) : prnd_(p_rnd) {
    // setup default value
    lb_ = 5.0f;
    ub_ = 10.0f;
    step_ = 0;
    saturation_start_ = 0;
    saturation_end_ = 0;
    delta_ = 0.0f;
    init_ = false;
  }
  virtual void SetParam(const char *name, const char* val) {
    if (!strcmp("lb", name)) lb_ = atof(val);
    if (!strcmp("ub", name)) ub_ = atof(val);
    if (!strcmp("calm_start", name)) saturation_start_ = atol(val);
    if (!strcmp("calm_end",  name)) saturation_end_ = atol(val);
  }
  virtual void InitConnection(const std::vector<Node<xpu>*> &nodes_in,
                              const std::vector<Node<xpu>*> &nodes_out,
                              ConnectState<xpu> *p_cstate) {
    utils::Check(nodes_in.size() == 1 && nodes_out.size() == 1,
                 "InsanityLayer: only support 1-1 connection");
    // use 1 temp state for mask
    p_cstate->states.resize(1);
    p_cstate->states[0].Resize(nodes_in[0]->data.shape_);
    nodes_out[0]->data.shape_ = nodes_in[0]->data.shape_;
  }
  virtual void OnBatchSizeChanged(const std::vector<Node<xpu>*> &nodes_in,
                                  const std::vector<Node<xpu>*> &nodes_out,
                                  ConnectState<xpu> *p_cstate) {
    p_cstate->states[0].Resize(nodes_in[0]->data.shape_);
  }
  virtual void Forward(bool is_train,
                       const std::vector<Node<xpu>*> &nodes_in,
                       const std::vector<Node<xpu>*> &nodes_out,
                       ConnectState<xpu> *p_cstate) {
    if (!init_) {
      init_ = true;
      delta_ = (ub_ + lb_) / 2.0f;
      delta_ = ub_ - delta_;
      delta_ /= (saturation_end_ - saturation_start_);
    }
    using namespace mshadow::expr;
    if (step_ < saturation_end_ && step_ > saturation_start_) {
      ub_ -= delta_ * step_;
      lb_ += delta_ * step_;
      step_ ++;
    }
    mshadow::TensorContainer<xpu,4> &mask = p_cstate->states[0];
    if (is_train) {
      mask = prnd_->uniform(mask.shape_);
      mask = mask * (ub_ - lb_) + lb_;
      nodes_in[0]->data = F<op::xelu>(nodes_in[0]->data, mask);
      mshadow::Copy(nodes_out[0]->data, nodes_in[0]->data, nodes_out[0]->data.stream_);
    } else {
      nodes_in[0]->data = F<op::xelu>(nodes_in[0]->data, (lb_ + ub_) / 2.0f);
      mshadow::Copy(nodes_out[0]->data, nodes_in[0]->data, nodes_out[0]->data.stream_);
    }
  }
  virtual void Backprop(bool prop_grad,
                        const std::vector<Node<xpu>*> &nodes_in,
                        const std::vector<Node<xpu>*> &nodes_out,
                        ConnectState<xpu> *p_cstate) {
    using namespace mshadow::expr;
    mshadow::TensorContainer<xpu,4> &mask = p_cstate->states[0];
    if (prop_grad) {
      nodes_in[0]->data = F<op::xelu_grad>(nodes_in[0]->data, mask) * nodes_out[0]->data;
    }
  }

 private:
  /*! \brief random number generator */
  mshadow::Random<xpu> *prnd_;
  /*! \brief whether initialized */
  bool init_;
  /*! \brief lower bound */
  float lb_;
  /*! \brief upper bound */
  float ub_;
  /*! \brief step counter */
  long step_;
  /*! \brief epoch to start saturation process */
  long saturation_start_;
  /*! \brief epoch to finish saturation */
  long saturation_end_;
  /*! \brief change in each epoch */
  double delta_;
};  // class InsanityLayer

} // namespace layer
} // namespace cxxnet
#endif // INSANITY_LAYER_INL_HPP
####$$$$ cxxnet-master\cxxnet-master\src\layer/insanity_pooling_layer-inl.hpp
#ifndef INSANITY_POOLING_LAYER_INL_HPP
#define INSANITY_POOLING_LAYER_INL_HPP
#pragma once

#include <mshadow/tensor.h>
#include "./layer.h"
#include "./param.h"

namespace mshadow {
namespace expr {

template<typename Reducer, typename SrcExp, typename MaskExp, typename DType, int srcdim>
struct InsanityPoolingExp :
  public MakeTensorExp<InsanityPoolingExp<Reducer, SrcExp, MaskExp, DType, srcdim>,
                       SrcExp, srcdim, DType> {
  const SrcExp &src_;
  const MaskExp &mask_;
  index_t ksize_y_;
  index_t ksize_x_;
  index_t kstride_;
  index_t src_height_;
  index_t src_width_;
  DType p_keep_;
  InsanityPoolingExp(const SrcExp &src, const MaskExp &mask,
                     index_t ksize_y, index_t ksize_x, index_t kstride, DType p_keep)
    : src_(src), mask_(mask),
      ksize_y_(ksize_y), ksize_x_(ksize_x), kstride_(kstride), p_keep_(p_keep) {
      Shape<srcdim> sshape = ShapeCheck<srcdim, SrcExp>::Check(src_);
      Shape<srcdim> smshape = ShapeCheck<srcdim, MaskExp>::Check(mask_);
      utils::Check(sshape == smshape, "Incorrect shape");
      utils::Check(sshape[srcdim - 1] >= ksize_x && sshape[srcdim - 2] >= ksize_y,
                   "InsanityPoolingExp: kernel must be smaller than image");
      this->src_height_ = sshape[srcdim - 2];
      this->src_width_  = sshape[srcdim - 1];
      this->shape_ = sshape;
      this->shape_[srcdim - 2] = std::min(src_height_ - ksize_y + kstride - 1, src_height_ - 1) / kstride + 1,
      this->shape_[srcdim - 1] = std::min(src_width_ - ksize_x + kstride-1, src_width_ - 1) / kstride + 1;
  }
}; // struct InsanityPoolingExp

template<typename Reducer, typename SrcExp, typename MaskExp, typename DType, int etype>
inline InsanityPoolingExp<Reducer, SrcExp, MaskExp, DType, ExpInfo<SrcExp>::kDim>
insanity_pool(const Exp<SrcExp, DType, etype> &src,
              const Exp<MaskExp, DType, etype> &mask,
              index_t ksize_y, index_t ksize_x, index_t kstride, DType p_keep) {
  TypeCheckPass<ExpInfo<SrcExp>::kDim >= 2>::Error_Expression_Does_Not_Meet_Dimension_Req();
  return InsanityPoolingExp<Reducer, SrcExp, MaskExp, DType, ExpInfo<SrcExp>::kDim>
    (src.self(), mask.self(), ksize_y, ksize_x, kstride, p_keep);
}


template<typename Reducer, typename SrcExp, typename MaskExp, typename DType, int srcdim>
struct Plan<InsanityPoolingExp<Reducer, SrcExp, MaskExp, DType, srcdim>, DType> {
  public:
    explicit Plan(const InsanityPoolingExp<Reducer, SrcExp, MaskExp, DType, srcdim> &e)
      : src_(MakePlan(e.src_)),
        mask_(MakePlan(e.mask_)),
        ksize_y_(e.ksize_y_), ksize_x_(e.ksize_x_), kstride_(e.kstride_),
        src_height_(e.src_height_), src_width_(e.src_width_),
        new_height_(e.shape_[srcdim - 2]),
        p_keep_(e.p_keep_), delta_((1.0f - e.p_keep_) / 4.0f) {}

    MSHADOW_XINLINE DType Eval(index_t i, index_t j) const {
      using namespace std;
      const index_t py = i % new_height_;
      const index_t y_start = py * kstride_;
      const index_t y_end = min(y_start + ksize_y_, src_height_);
      const index_t px = j;
      const index_t x_start = px * kstride_;
      const index_t x_end = min(x_start + ksize_x_, src_width_);
      const index_t c = i / new_height_;

      DType res; Reducer::SetInitValue(res);
      for (index_t y = y_start; y < y_end; ++y) {
        for (index_t x = x_start; x < x_end; ++x) {
          index_t loc_y = y;
          index_t loc_x = x;
          DType flag = mask_.Eval(c * src_height_ + y, x);
          if (flag < p_keep_) {
            ;
          } else if (flag < p_keep_ + delta_) {
            loc_y = loc_y > 0 ? loc_y - 1 : loc_y;
          } else if (flag < p_keep_ + delta_ * 2.0f) {
            loc_y = loc_y + 1 < src_height_ ? loc_y + 1 : src_height_ - 1;
          } else if (flag < p_keep_ + delta_ * 3.0f) {
            loc_x = loc_x > 0 ? loc_x - 1 : loc_x;
          } else {
            loc_x = loc_x + 1 < src_width_ ? loc_x + 1 : src_width_ - 1;
          }
          Reducer::Reduce(res, src_.Eval(c * src_height_ + loc_y, loc_x));
        }
      }
      return res;
    }
  private:
    Plan<SrcExp, DType> src_;
    Plan<MaskExp, DType> mask_;
    const index_t ksize_y_, ksize_x_, kstride_;
    const index_t src_height_, src_width_;
    const index_t new_height_;
    const DType p_keep_;
    const DType delta_;
}; // struct Plan

} // namespace expr
} // namespace mshadow


namespace mshadow {
namespace expr {

template<typename Reducer, typename SrcExp, typename MaskExp, typename DType, int srcdim>
struct InsanityUnPoolingExp:
  public MakeTensorExp<InsanityUnPoolingExp<Reducer, SrcExp, MaskExp, DType, srcdim>,
          SrcExp, srcdim, DType> {
  const SrcExp &data_src_;
  const SrcExp &data_pooled_;
  const SrcExp &grad_pooled_;
  const MaskExp &mask_;
  index_t pshape_y_;
  index_t pshape_x_;
  index_t ksize_y_;
  index_t ksize_x_;
  index_t kstride_;
  DType p_keep_;
  InsanityUnPoolingExp(const SrcExp &data_src,
                       const SrcExp &data_pooled,
                       const SrcExp &grad_pooled,
                       const MaskExp &mask,
                       index_t ksize_y, index_t ksize_x, index_t kstride, DType p_keep)
    : data_src_(data_src), data_pooled_(data_pooled), grad_pooled_(grad_pooled),
      mask_(mask), ksize_y_(ksize_y), ksize_x_(ksize_x), kstride_(kstride), p_keep_(p_keep) {
    Shape<srcdim> pshape = ShapeCheck<srcdim, SrcExp>::Check(grad_pooled);
    utils::Check(pshape == ShapeCheck<srcdim, SrcExp>::Check(data_pooled),
                 "UnPoolingExp: pooled shape mismatch");
    Shape<srcdim> sshape = ShapeCheck<srcdim, SrcExp>::Check(data_src);
    Shape<srcdim> smshape = ShapeCheck<srcdim, MaskExp>::Check(mask_);
    utils::Check(sshape == smshape, "Incorrect shape");
    for (int k = 0;  k < srcdim - 2; ++k) {
      utils::Check(pshape[k] == sshape[k],
                  "UnPoolingExp: pool and src shape mismatch");
    }
    pshape_x_ = pshape[srcdim - 1];
    pshape_y_ = pshape[srcdim - 2];
    this->shape_ = sshape;
  }
}; // struct InsanityUnPoolingExp

template<typename Reducer, typename SrcExp, typename MaskExp, typename DType, int etype>
inline InsanityUnPoolingExp<Reducer, SrcExp, MaskExp, DType, ExpInfo<SrcExp>::kDim>
insanity_unpool(const Exp<SrcExp, DType, etype> &data_src,
                const Exp<SrcExp, DType, etype> &data_pooled,
                const Exp<SrcExp, DType, etype> &grad_pooled,
                const Exp<MaskExp, DType, etype> &mask,
                index_t ksize_y, index_t ksize_x, index_t kstride, DType p_keep) {
  return InsanityUnPoolingExp<Reducer, SrcExp, MaskExp, DType, ExpInfo<SrcExp>::kDim>
    (data_src.self(), data_pooled.self(), grad_pooled.self(), mask.self(),
     ksize_y, ksize_x, kstride, p_keep);
}

template<typename Reducer, typename SrcExp, typename MaskExp, typename DType, int srcdim>
struct Plan<InsanityUnPoolingExp<Reducer, SrcExp, MaskExp, DType, srcdim>, DType> {
  public:
    explicit Plan(const InsanityUnPoolingExp<Reducer, SrcExp, MaskExp, DType, srcdim> &e)
      : data_src_(e.data_src_), data_pooled_(e.data_pooled_), grad_pooled_(e.grad_pooled_),
        mask_(e.mask_), sshape_y_(e.shape_[srcdim - 2]), sshape_x_(e.shape_[srcdim - 3]),
        pshape_y_(e.pshape_y_),  pshape_x_(e.pshape_x_),
        ksize_y_(e.ksize_y_), ksize_x_(e.ksize_x_), kstride_(e.kstride_),
        p_keep_(e.p_keep_), delta_((1.0f - p_keep_) / 4.0f) {}
    MSHADOW_XINLINE DType Eval(index_t i, index_t j) const {
      using namespace std;
      const index_t x = j;
      const index_t y = i % sshape_y_;
      const index_t c = i / sshape_y_;
      const DType flag = mask_.Eval(i, j);
      index_t loc_x = x;
      index_t loc_y = y;
      if (flag < p_keep_) {
        ;
      } else if (flag < p_keep_ + delta_) {
        loc_y = loc_y > 0 ? loc_y - 1 : loc_y;
      } else if (flag < p_keep_ + delta_ * 2.0f) {
        loc_y = loc_y + 1 < sshape_y_ ? loc_y + 1 : sshape_y_ - 1;
      } else if (flag < p_keep_ + delta_ * 3.0f) {
        loc_x = loc_x > 0 ? loc_x - 1 : loc_x;
      } else {
        loc_x = loc_x + 1 < sshape_x_ ? loc_x + 1 : sshape_x_ - 1;
      }
      const DType vsrc = data_src_.Eval(c * sshape_y_ + loc_y ,loc_x);
      const index_t py_min =
        y < ksize_y_ ? 0 : (y - ksize_y_ + kstride_) / kstride_;
      const index_t px_min =
        x < ksize_x_ ? 0 : (x - ksize_x_ + kstride_) / kstride_;
      const index_t py_max = min((y + kstride_) / kstride_, pshape_y_);
      const index_t px_max = min((x + kstride_) / kstride_, pshape_x_);
      DType val = static_cast<DType>(0);
      for (index_t py = py_min; py < py_max; ++py) {
        for (index_t px = px_min; px < px_max; ++px) {
          val += Reducer::PartialGrad(vsrc,
                                      data_pooled_.Eval(c * pshape_y_ + py, px)) *
                                      grad_pooled_.Eval(c * pshape_y_ + py, px);
        }
      }
      return val;
    }
  private:
    Plan<SrcExp, DType> data_src_, data_pooled_, grad_pooled_;
    Plan<MaskExp, DType> mask_;
    const index_t sshape_y_, sshape_x_, pshape_y_, pshape_x_;
    const index_t ksize_y_, ksize_x_;
    const index_t kstride_;
    const DType p_keep_;
    const DType delta_;
}; // struct Plan

} // namespace expr
} // namespace mshadow

namespace cxxnet {
namespace layer {

template<typename Reducer, int mode, typename xpu>
class InsanityPoolingLayer : public PoolingLayer<Reducer, mode, xpu> {
  private:
    typedef PoolingLayer<Reducer, mode, xpu> Parent;
  public:
    InsanityPoolingLayer(mshadow::Random<xpu> *p_rnd) : prnd(p_rnd) {p_keep = 1.0f;};
    virtual ~InsanityPoolingLayer() {}
    virtual void SetParam(const char *name, const char *val) {
      Parent::SetParam(name, val);
      if (!strcmp(name, "keep")) p_keep = atof(val);
    }
    virtual void InitConnection(const std::vector<Node<xpu>*> &nodes_in,
                                const std::vector<Node<xpu>*> &nodes_out,
                                ConnectState<xpu> *p_cstate) {
      this->InitNode(nodes_in, nodes_out, p_cstate);
    }
    virtual void Forward(bool is_train,
                         const std::vector<Node<xpu>*> &nodes_in,
                         const std::vector<Node<xpu>*> &nodes_out,
                         ConnectState<xpu> *p_cstate) {
      mshadow::TensorContainer<xpu,4> &tmp = p_cstate->states[0];
      mshadow::TensorContainer<xpu,4> &mask = p_cstate->states[1];
      mshadow::Shape<2> pshape = nodes_out[0]->data[0][0].shape_;
      using namespace mshadow::expr;
      if (is_train) {
        mask = prnd->uniform(mask.shape_);
        tmp = insanity_pool<Reducer>(nodes_in[0]->data, mask,
                                     Parent::param_.kernel_height,
                                     Parent::param_.kernel_width,
                                     Parent::param_.stride,
                                     p_keep);
      } else {
        tmp = pool<Reducer>(nodes_in[0]->data, pshape,
                            Parent::param_.kernel_height,
                            Parent::param_.kernel_width,
                            Parent::param_.stride);
      }
      mshadow::Copy(nodes_out[0]->data, tmp, nodes_out[0]->data.stream_);
    }
    virtual void Backprop(bool prop_grad,
                          const std::vector<Node<xpu>*> &nodes_in,
                          const std::vector<Node<xpu>*> &nodes_out,
                          ConnectState<xpu> *p_cstate) {
      mshadow::TensorContainer<xpu,4> &tmp = p_cstate->states[0];
      mshadow::TensorContainer<xpu,4> &mask = p_cstate->states[1];
      using namespace mshadow::expr;
      nodes_in[0]->data = insanity_unpool<Reducer>(nodes_in[0]->data, tmp,
                                                   nodes_out[0]->data, mask,
                                                   Parent::param_.kernel_height,
                                                   Parent::param_.kernel_width,
                                                   Parent::param_.stride,
                                                   p_keep);
    }
  protected:
    inline void InitNode(const std::vector<Node<xpu>*> &nodes_in,
                         const std::vector<Node<xpu>*> &nodes_out,
                         ConnectState<xpu> *p_cstate) {
      Parent::InitNode(nodes_in, nodes_out, p_cstate);
      p_cstate->states.push_back(mshadow::TensorContainer<xpu,4>(false));
      p_cstate->states[1].Resize(nodes_in[0]->data.shape_);
    }
  private:
    mshadow::Random<xpu> *prnd;
    float p_keep;
}; // class InsanityPoolingLayer

} // namespace layer
} // namespace cxxnet
#endif // INSANITY_POOLING_LAYER_INL_HPP
####$$$$ cxxnet-master\cxxnet-master\src\layer/layer.h
#ifndef CXXNET_LAYER_LAYER_H_
#define CXXNET_LAYER_LAYER_H_
/*!
 * \file layer.h
 * \brief abstract interface of layer
 * \author Tianqi Chen, Bing Xu
 */
#include <vector>
#include <map>
#include <string>
#include <mshadow/tensor.h>
#include "../global.h"
#include "../utils/utils.h"
#include "../utils/io.h"
#if CXXNET_USE_CUDNN == 1
 #ifdef __CUDACC__
  #include <cudnn.h>
 #endif
#endif

/*! \brief namespace of cxxnet */
namespace cxxnet {
/*! \brief namespace of layer defintiion */
namespace layer {
/*!
 * \brief node structure, this is used to store forward activation,
 *    and backproped gradient in the network
 * \tparam xpu the device name it lies in, can be cpu/gpu
 */
template<typename xpu>
struct Node {
  /*!
   * \brief content of the node
   *  layout:
   *     images (batch_size, nchannel, height, width)
   *     matrix (batch_size, 1, 1, length-of-vector)
   */
  mshadow::Tensor<xpu, 4> data;
  /*! \brief whether the underlying data must be contiguous */
  bool must_contiguous;
  bool inited;
  // constructor
  Node(void) : must_contiguous(false) {
    data.shape_ = mshadow::Shape4(0,0,0,0);
    inited = false;
  }
  /*! \brief matrix view of the node */
  inline mshadow::Tensor<xpu, 2> mat(void) {
    return data.FlatTo2D();
  }
  /*! \brief check whether it holds a matrix data */
  inline bool is_mat(void) const {
    return data.size(1) == 1 && data.size(2) == 1;
  }
  /*! \brief helper rountine to free space */
  inline void FreeSpace(void) {
    if (inited){
      mshadow::FreeSpace(&data);
    }
  }
  /*! \brief helper rountine to allocate space */
  inline void AllocSpace(void) {
    if (must_contiguous) {
      mshadow::AllocSpace(&data, false);
      utils::Assert(data.CheckContiguous(), "contiguous");
    } else {
      mshadow::AllocSpace(&data);
    }
    inited = true;
  }
}; // struct Node

/*!
 * \brief a single label record that can be taken by a loss function
 *    use struct for future extensibility
 */
struct LabelRecord {
  /*! \brief label field */
  mshadow::Tensor<cpu, 2> label;
  /*!
   * \brief slice the label information to take [begin, end)
   * \param begin beginning of index
   * \param end end of index
   */
  inline LabelRecord Slice(index_t begin, index_t end) const {
    LabelRecord r;
    r.label = label.Slice(begin, end);
    return r;
  }
};
/*!
 * \brief data structure to hold additional information about label of instances
 * this information is used by layers that computes the gradient over objectibe functions,
 * this data structure  will be evolving, to meet needs of different kinds of supervision signals
 */
struct LabelInfo {
  /*! \brief fields of each label */
  std::vector<LabelRecord> fields;
  /*!
   * \brief name map that maps field name
   *  to the index of fields
   */
  const std::map<std::string, size_t> *name2findex;
  // constructor
  LabelInfo(void) : name2findex(NULL) {
  }
  /*!
   * \brief slice the label information to take [begin, end)
   * \param begin beginning of index
   * \param end end of index
   */
  inline LabelInfo Slice(index_t begin, index_t end) const {
    LabelInfo ret;
    ret.fields.resize(fields.size());
    for (size_t i = 0; i < fields.size(); ++i) {
      ret.fields[i] = fields[i].Slice(begin, end);
    }
    ret.name2findex = name2findex;
    return ret;
  }
};
/*!
 * \brief connection states
 *   temporal state space that can be used to share information between forward and backprop
 *   not every layer needs this, this is used
 */
template<typename xpu>
struct ConnectState {
  /*! \brief the contents of states */
  std::vector< mshadow::TensorContainer<xpu, 4> > states;
};

/*!
 * \brief Interface of layer
 *    this is a pure interface and there is not data memember
 *    in ILayer. However, there can be common pattern of memembers in a layer,
 *    see the following notes
 *
 *  Connection and Layer:
 *     In the current design of cxxnet, there is concept of Connection, and Layer
 *     A Layer is defines set of of functions Forward and Backprop, given input/output nodes
 *     A Layer is not attached to specific pair of nodes, while Connection is.
 *     Connection is the connection between nodes, whose function is backed by Layers.
 *     Different connection can share a same Layer
 *
 *     This means Layer can not contain any node specific state(for example, dropout mask) in the class.
 *     The Connection specific states are maintained by Connection, and passed to Layer during Forward/Backprop
 *
 *  Weights and gradient:
 *     Some layers can have connection weight parameters, and gradient of weights.
 *     These weights are hold in the specific implementation.
 *     They can be accesed by using a vistor, see also IVisitor
 *     SaveModel and LoadModel are used to serialize and deserialize them
 *
 *  Parameters:
 *     Parameters related to each layer (e.g. number of hidden nodes), can be set by calling SetParam
 *
 * \tparam xpu the device name it lies in, can be cpu/gpu
 * \sa CommonLayerBase, ILayer::IVisitor, ConnectState
 */
template<typename xpu>
class ILayer {
 public:
  /*!
   * \brief visitor to the layer
   *    visits the weight and gradient in the layer
   */
  struct IVisitor {
    /*!
     * \brief visit content of the layer, this is called by Layer
     *    when ApplyVisitor is called
     *
     *    Visitor can use to get weight content, copy/set weights, etc.
     *
     * \param field_name the name of field on the layer
     * \param weight the connect weight used in the layer
     * \param grad the gradient of the weight,
     *        it is ensured to be have same shape as weight
     */
    virtual void Visit(const char *field_name,
                       mshadow::Tensor<xpu, 1> weight,
                       mshadow::Tensor<xpu, 1> grad) = 0;
    virtual void Visit(const char *field_name,
                       mshadow::Tensor<xpu, 2> weight,
                       mshadow::Tensor<xpu, 2> grad) = 0;
    virtual void Visit(const char *field_name,
                       mshadow::Tensor<xpu, 3> weight,
                       mshadow::Tensor<xpu, 3> grad) = 0;
    virtual void Visit(const char *field_name,
                       mshadow::Tensor<xpu, 4> weight,
                       mshadow::Tensor<xpu, 4> grad) = 0;
  };
 public:
  /*! \brief virtual destructor */
  virtual ~ILayer(void) {}

  /*!
   * \brief initialize the connection, this function takes charge of two shings
   *   (1) setup the shape of output nodes in nodes_out, given the
   *   (2) allocate necessary temporal state space in p_cstate
   * \param nodes_in vector of input nodes
   * \param nodes_out vector of output nodes
   * \param p_cstate
   */
  virtual void InitConnection(const std::vector<Node<xpu>*> &nodes_in,
                              const std::vector<Node<xpu>*> &nodes_out,
                              ConnectState<xpu> *p_cstate) = 0;
  /*!
   * \brief update the p_cstate when batch size(shape[3] of input output nodes) changed
   *        This function is called whenever the batch_size changed, and the Layer can make use
   *        of this to update the p_cstate
   * \param nodes_in vector of input nodes
   * \param nodes_out vector of output nodes
   * \param p_cstate temporal state space that can be used to share information between forward and backprop
   */
  virtual void OnBatchSizeChanged(const std::vector<Node<xpu>*> &nodes_in,
                                  const std::vector<Node<xpu>*> &nodes_out,
                                  ConnectState<xpu> *p_cstate) {}
  /*!
   * \brief Forward propagation from input nodes to output nodes
   * \param is_train the propagation is during training phase
   * \param nodes_in vector of input nodes
   * \param nodes_out vector of output nodes
   * \param p_cstate temporal state space that can be used to share information between forward and backprop
   */
  virtual void Forward(bool is_train,
                       const std::vector<Node<xpu>*> &nodes_in,
                       const std::vector<Node<xpu>*> &nodes_out,
                       ConnectState<xpu> *p_cstate) = 0;
  /*!
   * \brief Back propagation from output nodes to input nodes
   *    in the beginning of function call, the output nodes is ensured to contain the gradient value
   * \param prop_grad if true, then the layer will propagate gradient back to its input node
   * \param nodes_in vector of input nodes
   * \param nodes_out vector of output nodes
   * \param p_cstate temporal state space that can be used to share information between forward and backprop
   */
  virtual void Backprop(bool prop_grad,
                        const std::vector<Node<xpu>*> &nodes_in,
                        const std::vector<Node<xpu>*> &nodes_out,
                        ConnectState<xpu> *p_cstate) = 0;
  /*!
   * \brief return whether this layer can be shared across multiple
   * connections, for most layers this should be true
   */
  virtual bool AllowSharing(void) const {
    return true;
  }
  /*!
   * \brief set the stream of internal computation to be stream
   * \param stream the stream to be used
   */
  virtual void SetStream(mshadow::Stream<xpu> *stream) {}
  /*!
   * \brief apply visitor to the layer,
   *   this is used to visit tha content of the layer
   */
  virtual void ApplyVisitor(IVisitor *pvisitor) {}
  /*!
   * \brief Set param for the layer from string
   * \param name parameter name
   * \param val string for configuration
   */
  virtual void SetParam(const char *name, const char* val) {}
  /*!
   * \brief intialized model parameters, only called when model parameters are not initialzied
   */
  virtual void InitModel(void) {}
  /*!
   * \brief Save model into binary file
   * \param fo output stream
   */
  virtual void SaveModel(utils::IStream &fo) const {}
  /*!
   * \brief Load model from binary file
   * \param fi input stream
   */
  virtual void LoadModel(utils::IStream &fi) {}
};

/*! \brief these are enumeration */
// shared layer is a special type indicating that this connection
// is sharing Layer with an existing connection
const int kSharedLayer = 0;
const int kFullConnect = 1;
const int kSoftmax = 2;
const int kRectifiedLinear = 3;
const int kSigmoid = 4;
const int kTanh = 5;
const int kSoftplus = 6;
const int kFlatten = 7;
const int kDropout = 8;
const int kConv = 10;
const int kMaxPooling = 11;
const int kSumPooling = 12;
const int kAvgPooling = 13;
const int kLRN = 15;
const int kBias = 17;
const int kConcat = 18;
const int kXelu = 19;
const int kCaffe = 20;
// first apply relu then maxpooling
const int kReluMaxPooling = 21;
const int kMaxout = 22;
const int kSplit = 23;
const int kInsanity = 24;
const int kInsanityPooling = 25;
const int kL2Loss = 26;
const int kMultiLogistic = 27;
const int kChConcat = 28;
const int kPRelu = 29;
const int kBatchNorm = 30;
const int kFixConnect = 31;
/*! \brief gap used to encode pairtest layer */
const int kPairTestGap = 1024;
/*! \brief use integer to encode layer types */
typedef int LayerType;
/*!
 * \brief get the layer type from string
 * \param type indicate the type of a layer
 */
inline LayerType GetLayerType(const char *type) {
  if (!strncmp(type, "share", 5)) return kSharedLayer;
  if (!strcmp(type, "fullc")) return kFullConnect;
  if (!strcmp(type, "fixconn")) return kFixConnect;
  if (!strcmp(type, "bias")) return kBias;
  if (!strcmp(type, "softmax")) return kSoftmax;
  if (!strcmp(type, "relu")) return kRectifiedLinear;
  if (!strcmp(type, "sigmoid")) return kSigmoid;
  if (!strcmp(type, "tanh")) return kTanh;
  if (!strcmp(type, "softplus")) return kSoftplus;
  if (!strcmp(type, "flatten")) return kFlatten;
  if (!strcmp(type, "dropout")) return kDropout;
  if (!strcmp(type, "conv")) return kConv;
  if (!strcmp(type, "relu_max_pooling")) return kReluMaxPooling;
  if (!strcmp(type, "max_pooling")) return kMaxPooling;
  if (!strcmp(type, "sum_pooling")) return kSumPooling;
  if (!strcmp(type, "avg_pooling")) return kAvgPooling;
  if (!strcmp(type, "lrn")) return kLRN;
  if (!strcmp(type, "concat")) return kConcat;
  if (!strcmp(type, "xelu")) return kXelu;
  if (!strcmp(type, "maxout")) return kMaxout;
  if (!strcmp(type, "split")) return kSplit;
  if (!strcmp(type, "insanity")) return kInsanity;
  if (!strcmp(type, "insanity_max_pooling")) return kInsanityPooling;
  if (!strcmp(type, "l2_loss")) return kL2Loss;
  if (!strcmp(type, "multi_logistic")) return kMultiLogistic;
  if (!strcmp(type, "ch_concat")) return kChConcat;
  if (!strcmp(type, "prelu")) return kPRelu;
  if (!strcmp(type, "batch_norm")) return kBatchNorm;
  #if CXXNET_USE_CAFFE_ADAPTOR
  if (!strcmp(type, "caffe")) return kCaffe;
  #endif
  if (!strncmp(type, "pairtest-", 9)) {
    char tmaster[256], tslave[256];
    sscanf(type + 9, "%[^-]-%[^:]", tmaster, tslave);
    return kPairTestGap * GetLayerType(tmaster) + GetLayerType(tslave);
  }
  utils::Error("unknown layer type: \"%s\"", type);
  return kConv;
}
/*!
 * \brief factory: create an upadater algorithm of given type
 * \param type indicate the type of a layer
 * \param p_rnd random number generator
 * \param label_info pointer to the label information field, that will be contain,
 *                   this is similar to node, but contains label information that can be used
 *                   to compute gradient over objetives
 */
template<typename xpu>
ILayer<xpu>* CreateLayer(LayerType type,
                         mshadow::Random<xpu> *p_rnd,
                         const LabelInfo *label_info);
/*!
 * \brief this data structure specifies a connection
 * this is a node specific data structure, that defines connection between nodes
 * \tparam xpu the device the connection lies in
 */
template<typename xpu>
struct Connection {
  /*! \brief the backend layer of the connection */
  ILayer<xpu> *layer;
  /*! \brief the type of the backend layer */
  LayerType type;
  /*! \brief shared states of the connection */
  ConnectState<xpu> state;
  /*! \brief list of input nodes */
  std::vector<Node<xpu>*> nodes_in;
  /*! \brief list of output nodes */
  std::vector<Node<xpu>*> nodes_out;
  /*!
   * \brief set the internal computation stream
   * \param stream the stream that was used for computation
   */
  inline void SetStream(mshadow::Stream<xpu> *stream) {
    layer->SetStream(stream);
    for (size_t i = 0; i < state.states.size(); ++i) {
      state.states[i].set_stream(stream);
    }
    for (size_t i = 0; i < nodes_in.size(); ++i) {
      nodes_in[i]->data.set_stream(stream);
    }
    for (size_t i = 0; i < nodes_out.size(); ++i) {
      nodes_out[i]->data.set_stream(stream);
    }
  }
};
}  // namespace layer
}  // namespace cxxnet
#endif  // CXXNET_LAYER_LAYER_H
####$$$$ cxxnet-master\cxxnet-master\src\layer/layer_impl-inl.hpp
#ifndef CXXNET_LAYER_IMPL_INL_HPP_
#define CXXNET_LAYER_IMPL_INL_HPP_
/*!
 * \file layer-inl.hpp
 * \brief this file compiles all implementation of layers together
 * \author Bing Xu, Tianqi Chen
 */
#include "./layer.h"
#include "./activation_layer-inl.hpp"
#include "./convolution_layer-inl.hpp"
#include "./bias_layer-inl.hpp"
#include "./dropout_layer-inl.hpp"
#include "./fullc_layer-inl.hpp"
#include "./fixconn_layer-inl.hpp"
#include "./lrn_layer-inl.hpp"
#include "./flatten_layer-inl.hpp"
#include "./pooling_layer-inl.hpp"
#include "./pairtest_layer-inl.hpp"
#include "./concat_layer-inl.hpp"
#include "./cudnn_convolution_layer-inl.hpp"
#include "./split_layer-inl.hpp"
#include "./cudnn_pooling_layer-inl.hpp"
#include "./xelu_layer-inl.hpp"
#include "./insanity_layer-inl.hpp"
#include "./insanity_pooling_layer-inl.hpp"
#include "./prelu_layer-inl.hpp"
#include "./batch_norm_layer-inl.hpp"
#include "./loss/softmax_layer-inl.hpp"
#include "./loss/l2_loss_layer-inl.hpp"
#include "./loss/multi_logistic_layer-inl.hpp"
#if CXXNET_USE_CAFFE_ADAPTOR
#include "../plugin/caffe_adapter-inl.hpp"
#endif
namespace cxxnet {
namespace layer {
template<typename xpu>
ILayer<xpu>* CreateLayer_(LayerType type,
                          mshadow::Random<xpu> *p_rnd,
                          const LabelInfo *label_info) {
  if (type >= kPairTestGap) {
    return new PairTestLayer<xpu>(CreateLayer_(type / kPairTestGap, p_rnd, label_info),
                                  CreateLayer_(type % kPairTestGap, p_rnd, label_info));
  }
  switch(type) {
    case kSigmoid: return new ActivationLayer<xpu, op::sigmoid, op::sigmoid_grad>();
    case kTanh: return new ActivationLayer<xpu, op::tanh, op::tanh_grad>();
    case kRectifiedLinear: return new ActivationLayer<xpu, op::relu, op::relu_grad>();
    case kConv: return new CuDNNConvolutionLayer<xpu>(p_rnd);
    case kBias: return new BiasLayer<xpu>();
    case kDropout: return new DropoutLayer<xpu>(p_rnd);
    case kFullConnect: return new FullConnectLayer<xpu>(p_rnd);
    case kFixConnect: return new FixConnectLayer<xpu>();
    case kLRN: return new LRNLayer<xpu>();
    case kFlatten: return new FlattenLayer<xpu>();
    case kReluMaxPooling: return
        new PoolingLayer<mshadow::red::maximum, false, xpu, false, op::relu, op::relu_grad>();
    case kMaxPooling: return new CuDNNPoolingLayer<mshadow::red::maximum, kMaxPooling, xpu>();
    case kSumPooling: return new PoolingLayer<mshadow::red::sum, kSumPooling, xpu>();
    case kAvgPooling: return new CuDNNPoolingLayer<mshadow::red::sum, kAvgPooling, xpu>();
    case kSoftmax: return new SoftmaxLayer<xpu>(label_info);
    case kConcat: return new ConcatLayer<xpu, 3>();
    case kChConcat: return new ConcatLayer<xpu, 1>();
    case kSplit: return new SplitLayer<xpu>();
    case kXelu: return new XeluLayer<xpu>();
    case kInsanity: return new InsanityLayer<xpu>(p_rnd);
    case kInsanityPooling: return new InsanityPoolingLayer<mshadow::red::maximum, kMaxPooling, xpu>(p_rnd);
    case kPRelu: return new PReluLayer<xpu>(p_rnd);
    case kBatchNorm: return new BatchNormLayer<xpu>(p_rnd);
    case kL2Loss: return new L2LossLayer<xpu>(label_info);
    case kMultiLogistic: return new MultiLogisticLayer<xpu>(label_info);
    #if CXXNET_USE_CAFFE_ADAPTOR
    case kCaffe: return new CaffeLayer<xpu>();
    #endif
    default: utils::Error("unknown layer type id : \"%d\"", type); return NULL;
  }
}

}  // namespace layer
}  // namespace cxxnet
#endif
####$$$$ cxxnet-master\cxxnet-master\src\layer/layer_impl.cpp
#define _CRT_SECURE_NO_WARNINGS
#define _CRT_SECURE_NO_DEPRECATE
// include the layer, this is where the actual implementations are

#include "layer_impl-inl.hpp"
// specialize the cpu implementation here
namespace cxxnet {
namespace layer {
template<>
ILayer<cpu>* CreateLayer<cpu>(LayerType type,
                              mshadow::Random<cpu> *p_rnd,
                              const LabelInfo *label_info) {
  return CreateLayer_<cpu>(type, p_rnd, label_info); 
}
}  // namespace layer
}  // namespace cxxnet
####$$$$ cxxnet-master\cxxnet-master\src\layer/layer_impl.cu
#define _CRT_SECURE_NO_WARNINGS
#define _CRT_SECURE_NO_DEPRECATE
// include the layer, this is where the actual implementations are
#include "layer_impl-inl.hpp"
// specialize the gpu implementation here
namespace cxxnet {
namespace layer {
template<>
ILayer<gpu>* CreateLayer<gpu>(LayerType type,
                              mshadow::Random<gpu> *p_rnd,
                              const LabelInfo *label_info) {
  return CreateLayer_<gpu>(type, p_rnd, label_info); 
}
}  // namespace layer
}  // namespace cxxnet

####$$$$ cxxnet-master\cxxnet-master\src\layer/lrn_layer-inl.hpp
#ifndef CXXNET_LAYER_LRN_LAYER_INL_HPP_
#define CXXNET_LAYER_LRN_LAYER_INL_HPP_

#include <mshadow/tensor.h>
#include "./layer.h"
#include "./op.h"

namespace cxxnet {
namespace layer {

template<typename xpu>
class LRNLayer : public ILayer<xpu> {
 public:
  LRNLayer(void) {
    // default values
    this->knorm_ = 1.0f;
    this->nsize_ = 3;
  }
  virtual ~LRNLayer(void){}
  virtual void SetParam(const char *name, const char *val) {
    if (!strcmp(name, "local_size")) nsize_ = static_cast<index_t>(atoi(val));
    if (!strcmp(name, "alpha")) alpha_ = static_cast<real_t>(atof(val));
    if (!strcmp(name, "beta")) beta_ = static_cast<real_t>(atof(val));
    if (!strcmp(name, "knorm")) knorm_ = static_cast<real_t>(atof(val));
  }
  virtual void SetStream(mshadow::Stream<xpu> *stream) {
    tmp_in.set_stream(stream);
  }
  virtual void InitConnection(const std::vector<Node<xpu>*> &nodes_in,
                              const std::vector<Node<xpu>*> &nodes_out,
                              ConnectState<xpu> *p_cstate) {
    utils::Check(nodes_in.size() == 1 && nodes_out.size() == 1,
                 "LRNLayer: only support 1-1 connection");
    nodes_out[0]->data.shape_ = nodes_in[0]->data.shape_;
    // use 1 temp state for mask
    p_cstate->states.resize(1);
    p_cstate->states[0].Resize(nodes_in[0]->data.shape_);
    // temp in is kepted in layer, since it does not go across forward/backprop
    tmp_in.Resize(nodes_in[0]->data.shape_);
  }
  virtual void OnBatchSizeChanged(const std::vector<Node<xpu>*> &nodes_in,
                                  const std::vector<Node<xpu>*> &nodes_out,
                                  ConnectState<xpu> *p_cstate) {
    p_cstate->states[0].Resize(nodes_in[0]->data.shape_);
  }
  virtual void Forward(bool is_train,
                       const std::vector<Node<xpu>*> &nodes_in,
                       const std::vector<Node<xpu>*> &nodes_out,
                       ConnectState<xpu> *p_cstate) {
    using namespace mshadow;
    using namespace mshadow::expr;
    mshadow::Tensor<xpu,4> &tmp_norm = p_cstate->states[0];
    const real_t salpha = alpha_ / nsize_;
    // stores normalizer without power
    tmp_norm = chpool<red::sum>(F<op::square>(nodes_in[0]->data) , nsize_) * salpha + knorm_;
    nodes_out[0]->data = nodes_in[0]->data * F<op::power>(tmp_norm, -beta_);
  }
  virtual void Backprop(bool prop_grad,
                        const std::vector<Node<xpu>*> &nodes_in,
                        const std::vector<Node<xpu>*> &nodes_out,
                        ConnectState<xpu> *p_cstate) {
    using namespace mshadow;
    using namespace mshadow::expr;
    tmp_in.Resize(nodes_in[0]->data.shape_);
    mshadow::Tensor<xpu,4> &tmp_norm = p_cstate->states[0];
    const real_t salpha = alpha_ / nsize_;
    if (prop_grad) {
      // backup input data
      mshadow::Copy(tmp_in, nodes_in[0]->data, tmp_in.stream_);
      // first gradient to a[i], will be 1 / normalizer
      nodes_in[0]->data = nodes_out[0]->data * F<op::power>(tmp_norm, -beta_);
      // gradient to normalizer
      nodes_in[0]->data += (- 2.0f * beta_ * salpha) * 
          chpool<red::sum>(nodes_out[0]->data * tmp_in * F<op::power>(tmp_norm, -beta_-1.0f), nsize_)  * tmp_in;      
    }
  }
  
 private:
  /*! \brief input temp data */
  mshadow::TensorContainer<xpu,4> tmp_in;
  /*! \brief alpha */
  real_t alpha_;
  /*! \brief beta */
  real_t beta_;
  /*! \brief knorm */
  real_t knorm_;
  /*! \brief neighbor size */
  index_t nsize_;
}; // class lrn layer
}  // namespace layer
}  // namespace cxxnet
#endif  // LAYER_LRN_LAYER_INL_HPP_

####$$$$ cxxnet-master\cxxnet-master\src\layer/op.h
#ifndef CXXNET_LAYER_OP_H_
#define CXXNET_LAYER_OP_H_
#pragma once
/*!
 * \file op.h
 * \brief extra mshadow operation for cxxnet
 * \author Bing Xu
 */
#include <mshadow/tensor.h>

namespace cxxnet {
/*! \brief operations for ActivationLayer */
namespace op {
/*! \brief Rectified Linear Operation */
struct identity {
  MSHADOW_XINLINE static real_t Map(real_t a) {
    return a;
  }
};
struct identity_grad {
  MSHADOW_XINLINE static real_t Map(real_t a) {
    return 1.0f;
  }
};

/*! \brief sigmoid unit */
struct sigmoid {
  MSHADOW_XINLINE static real_t Map(real_t a) {
    return 1.0f / (1.0f + expf(-a));
  }
};
struct sigmoid_grad {
  MSHADOW_XINLINE static real_t Map(real_t a) {
    return a * (1.0f - a);
  }
};
/*! \brief Rectified Linear Operation */
struct relu {
  MSHADOW_XINLINE static real_t Map(real_t a) {
    using namespace std;
    return max(a, 0.0f);
  }
};
struct relu_grad {
  MSHADOW_XINLINE static real_t Map(real_t a) {
    return a > 0.0f ? 1.0f : 0.0f;
  }
};

/*! \brief Leaky ReLU Operation */
struct xelu {
  MSHADOW_XINLINE static real_t Map(real_t a, real_t b) {
    return a > 0 ? a : a / b;
  }
};

struct xelu_grad {
  MSHADOW_XINLINE static real_t Map(real_t a, real_t b) {
    return a > 0 ? 1 : 1.0f / b;
  }
};

struct tanh {
  MSHADOW_XINLINE static real_t Map(real_t a) {
    return tanhf( a );
  }
};

struct tanh_grad {
  MSHADOW_XINLINE static real_t Map(real_t a) {
    return 1.0f - a * a;
  }
};


struct square {
  MSHADOW_XINLINE static real_t Map(real_t a) {
    return a * a;
  }
};

/*! \brief used for generate Bernoulli mask */
struct threshold {
  MSHADOW_XINLINE static real_t Map(real_t a, real_t b) {
    return a < b ? 1.0f : 0.0f;
  }
};

/*! \brief used for generate element of power */
struct power {
  MSHADOW_XINLINE static real_t Map(real_t a, real_t b) {
    return powf( a, b );
  }
};

/*!\ \brief used for generate element sqrt */
struct square_root {
  MSHADOW_XINLINE static real_t Map(real_t a) {
    return sqrt(a);
  }
};

}  // namespace op
}  // namespace cxxnet
#endif // CXXNET_LAYER_OP_H
####$$$$ cxxnet-master\cxxnet-master\src\layer/pairtest_layer-inl.hpp
#ifndef CXXNET_LAYER_PAIRTEST_LAYER_INL_HPP_
#define CXXNET_LAYER_PAIRTEST_LAYER_INL_HPP_
/*!
 * \file pairtest-inl.hpp
 * \brief module to do pairtest, used to compare layer implementations
 * \author Tianqi Chen, Bing Xu
 */
#include <mshadow/tensor.h>
#include "./layer.h"
#include "./visitor.h"

namespace cxxnet {
namespace layer {
template<typename xpu>
class PairTestLayer : public ILayer<xpu> {
 public:
  PairTestLayer(ILayer<xpu> *master, ILayer<xpu> *slave) 
      : master_(master) {
    slave_.layer = slave;
  }
  // virtual destructor
  virtual ~PairTestLayer(void) {
    delete master_;
    delete slave_.layer;
    for (size_t i = 0; i < snodes_in_.size(); ++i) {
      snodes_in_[i].FreeSpace();
    }
    for (size_t i = 0; i < snodes_out_.size(); ++i) {
      snodes_out_[i].FreeSpace();
    }
  }
  virtual bool AllowSharing(void) const {
    return false;
  }
  virtual void InitConnection(const std::vector<Node<xpu>*> &nodes_in,
                              const std::vector<Node<xpu>*> &nodes_out,
                              ConnectState<xpu> *p_cstate) {
    master_->InitConnection(nodes_in, nodes_out, p_cstate);
    snodes_in_.resize(nodes_in.size());
    snodes_out_.resize(nodes_out.size());
    slave_.nodes_in.resize(nodes_in.size());
    slave_.nodes_out.resize(nodes_out.size());
    for (size_t i = 0; i < nodes_in.size(); ++i) {
      snodes_in_[i].data.shape_ = nodes_in[i]->data.shape_;
      mshadow::AllocSpace(&snodes_in_[i].data);
      slave_.nodes_in[i] = &snodes_in_[i];
    }
    for (size_t i = 0; i < nodes_out.size(); ++i) {
      slave_.nodes_out[i] = &snodes_out_[i];
    }
    slave_.layer->InitConnection(slave_.nodes_in,
                                 slave_.nodes_out,
                                 &slave_.state);   
    for (size_t i = 0; i < nodes_out.size(); ++i) { 
      utils::Check(snodes_out_[i].data.shape_ == nodes_out[i]->data.shape_,
                   "PairTestLayer.InitConnection: shape inconsistent");          
      mshadow::AllocSpace(&snodes_out_[i].data);
    }
  }
  
  virtual void OnBatchSizeChanged(const std::vector<Node<xpu>*> &nodes_in,
                                  const std::vector<Node<xpu>*> &nodes_out,
                                  ConnectState<xpu> *p_cstate) {
    master_->OnBatchSizeChanged(nodes_in, nodes_out, p_cstate);
    for (size_t i = 0; i < nodes_in.size(); ++i) {
      snodes_in_[i].data.shape_ = nodes_in[i]->data.shape_;
    }
    for (size_t i = 0; i < nodes_out.size(); ++i) {
      snodes_out_[i].data.shape_ = nodes_out[i]->data.shape_;
    }
    slave_.layer->OnBatchSizeChanged(slave_.nodes_in,
                                     slave_.nodes_out,
                                     &slave_.state);
  }
  virtual void Forward(bool is_train,
                       const std::vector<Node<xpu>*> &nodes_in,
                       const std::vector<Node<xpu>*> &nodes_out,
                       ConnectState<xpu> *p_cstate) {
    this->Cmp("Before-Forward:weight", "weight");
    master_->Forward(is_train, nodes_in, nodes_out, p_cstate);
    for (size_t i = 0; i < nodes_in.size(); ++i) {
      mshadow::Copy(snodes_in_[i].data, nodes_in[i]->data,
                    nodes_in[i]->data.stream_);
    }
    slave_.layer->Forward(is_train,
                          slave_.nodes_in,
                          slave_.nodes_out,
                          &slave_.state);
    utils::Check(nodes_out.size() == snodes_out_.size(),
                 "Forward: size mismatch");
    for (size_t i = 0; i < nodes_out.size(); ++i) {
      CmpResult(nodes_out[i]->data.FlatTo2D(),
                snodes_out_[i].data.FlatTo2D(), "Forward");
    }
  }
  virtual void Backprop(bool prop_grad,
                        const std::vector<Node<xpu>*> &nodes_in,
                        const std::vector<Node<xpu>*> &nodes_out,
                        ConnectState<xpu> *p_cstate) {
    master_->Backprop(prop_grad, nodes_in, nodes_out, p_cstate);
    for (size_t i = 0; i < nodes_in.size(); ++i) {
      mshadow::Copy(snodes_out_[i].data, nodes_out[i]->data,
                    nodes_out[i]->data.stream_);
    }
    slave_.layer->Backprop(prop_grad,
                           slave_.nodes_in,
                           slave_.nodes_out,
                           &slave_.state);
    this->Cmp("After-Backprop:grad", "grad");
    if (prop_grad) {
      utils::Check(nodes_in.size() == snodes_in_.size(),
                   "Backprop: size mismatch");
      for (size_t i = 0; i < nodes_in.size(); ++i) {
        CmpResult(nodes_in[i]->data.FlatTo2D(),
                  snodes_in_[i].data.FlatTo2D(), "Forward");
      }
    }
  }
  virtual void SetStream(mshadow::Stream<xpu> *stream) {
    master_->SetStream(stream);
    slave_.SetStream(stream);
  }
  virtual void ApplyVisitor(typename ILayer<xpu>::IVisitor *pvisitor) {
    master_->ApplyVisitor(pvisitor);
    slave_.layer->ApplyVisitor(pvisitor);
  }
  virtual void SetParam(const char *name, const char* val) {
    master_->SetParam(name, val);
    slave_.layer->SetParam(name, val);
    if (!strncmp( name, "master:", 7)) {
      master_->SetParam(name + 7, val);
    }
    if (!strncmp( name, "slave:", 6)) {
      slave_.layer->SetParam(name + 6, val);
    }
  }
  virtual void InitModel(void) {
    master_->InitModel();
    slave_.layer->InitModel();
    this->Sync("weight");
  }
  virtual void SaveModel(utils::IStream &fo) const {
    master_->SaveModel(fo);
    slave_.layer->SaveModel(fo);
  }
  virtual void LoadModel(utils::IStream &fi) {
    master_->LoadModel(fi);
    slave_.layer->SaveModel(fi);
  }

 private:
  ILayer<xpu> *master_;
  Connection<xpu> slave_;
  std::vector<Node<xpu> > snodes_in_, snodes_out_;
  // synchronize weight or gradient
  inline void Sync(const char *dtype) {
    GetWeightVisitor<xpu> vg(dtype);
    master_->ApplyVisitor(&vg);
    SetWeightVisitor<xpu> vs(vg.data, dtype);
    slave_.layer->ApplyVisitor(&vs);
  }
  inline void Cmp(const char *method, const char *dtype) {
    GetWeightVisitor<xpu> vm(dtype), vs(dtype);
    master_->ApplyVisitor(&vm);
    slave_.layer->ApplyVisitor(&vs);
    utils::Check(vm.data.size() == vs.data.size(),
                 "%s: number of %s mismatch", method, dtype);
    for (size_t i = 0; i < vm.data.size(); ++i) {
      CmpResult(vm.data[i], vs.data[i], method, vm.fields[i].c_str());
    }
  }
  inline static void CmpResult(mshadow::Tensor<xpu, 2> dmaster,
                               mshadow::Tensor<xpu, 2> dslave,
                               const char *tag,
                               const char *tag2 = "") {
    mshadow::TensorContainer<cpu, 2> tmst(false), tslv(false);
    mshadow::Stream<xpu> stream;
    tmst.Resize(dmaster.shape_);
    tslv.Resize(dslave.shape_);
    mshadow::Copy(tmst, dmaster, &stream);
    mshadow::Copy(tslv, dslave, &stream);
    index_t count = tmst.shape_.Size();
    double diff = 0.0, ssum = 0.0, maxdiff = 0.0;
    index_t mxidx = 0;
    for (index_t i = 0; i < count; ++i) {
      double d = std::abs(tmst.dptr_[i] - tslv.dptr_[i]);
      if (d > maxdiff) {
        maxdiff = d; mxidx = i;
      }
      diff += d;
      ssum += std::abs(tmst.dptr_[i]);
    }
    // relative absolute error
    double rerr = diff / ssum;
    if (rerr > 1e-5 || diff != diff) {
      fprintf(stderr, "%s%s: err=%f, maxd[%u]=%f, diff=%f, ssum=%f\n",
              tag, tag2, rerr, mxidx, maxdiff, diff, ssum);
    }
  }  
};
}  // namespace layer
}  // namespace cxxnet
#endif  // CXXNET_LAYER_PAIRTEST_LAYER_INL_HPP_
####$$$$ cxxnet-master\cxxnet-master\src\layer/param.h
#ifndef CXXNET_LAYER_PARAM_H_
#define CXXNET_LAYER_PARAM_H_
/*!
 * \file param.h
 * \brief commonly used parameters in each layer
 *        the LayerParam is not a must for each layer but can be helpful
 * \author Bing Xu, Tianqi Chen
 */
#include <cstring>
#include <string>

namespace cxxnet {
namespace layer {
/*! \brief potential parameters for each layer */
struct LayerParam {
  /*! \brief number of hidden layers */
  int num_hidden;
  /*! \brief initialization sd for weight */
  float init_sigma;
  /*! \brief initialization sparse weight */
  int init_sparse;
  /*! \brief initialization uniform for weight */
  float init_uniform;
  /*! \brief intialization value for bias */
  float init_bias;
  /*! \brief number of output channel */
  int num_channel;
  /*! \brief type of random number generation */
  int random_type;
  /*! \brief number of parallel group */
  int num_group;
  /*! \brief kernel height */
  int kernel_height;
  /*! \brief kernel width */
  int kernel_width;
  /*! \brief stride prameter */
  int stride;
  /*! \brief padding in y dimension */
  int pad_y;
  /*! \brief padding in x dimension */
  int pad_x;
  /*! \brief whether not include bias term */
  int no_bias;
  /*! \brief maximum temp_col_size allowed in each layer, we need at least one temp col */
  int temp_col_max;
  /*! \brief shut up */
  int silent;
  /*! \brief number of input channels */
  int num_input_channel;
  /*! \brief number of input hidden nodes, used by fullc */
  int num_input_node;
  /*! \brief reserved fields, for future compatibility */
  int reserved[64];
  /*! \brief construtor */
  LayerParam(void) {
    init_sigma = 0.01f;
    init_uniform = -1.0f;
    init_sparse = 10;
    init_bias  = 0.0f;
    random_type = 0;
    num_hidden = 0;
    num_channel = 0;
    num_group = 1;
    kernel_width = 0;
    kernel_height = 0;
    stride = 1;
    pad_x = pad_y = 0;
    no_bias = 0;
    silent = 0;
    num_input_channel = 0;
    num_input_node = 0;
    // 64 MB
    temp_col_max = 64<<18;
    memset(reserved, 0, sizeof(reserved));
  }
  /*!
   * \brief Set param for the layer from string
   * \param name parameter name
   * \param val string for configuration
   */
  inline void SetParam(const char *name, const char* val) {
    if (!strcmp(name, "init_sigma")) init_sigma = (float)atof(val);
    if (!strcmp(name, "init_uniform")) init_uniform = (float)atof(val);
    if (!strcmp(name, "init_bias")) init_bias  = (float)atof(val);
    if (!strcmp(name, "init_sparse")) init_sparse = atoi(val);
    if (!strcmp(name, "random_type")) {
      if (!strcmp(val, "gaussian")) random_type = 0;
      else if (!strcmp(val, "uniform")) random_type = 1;
      else if (!strcmp(val, "xavier")) random_type = 1;
      else if (!strcmp(val, "kaiming")) random_type = 2;
      else utils::Error("invalid random_type %s", val);
      // 3: mshadow binary file
    }
    if (!strcmp(name, "nhidden")) num_hidden = atoi(val);
    if (!strcmp(name, "nchannel")) num_channel = atoi(val);
    if (!strcmp(name, "ngroup")) num_group = atoi(val);
    if (!strcmp(name, "kernel_size")) {
      kernel_width = kernel_height = atoi(val);
    }
    if (!strcmp(name, "kernel_height")) kernel_height = atoi(val);
    if (!strcmp(name, "kernel_width")) kernel_width = atoi(val);
    if (!strcmp(name, "stride")) stride = atoi(val);
    if (!strcmp(name, "pad")) {
      pad_y = pad_x  = atoi(val);
    }
    if (!strcmp(name, "pad_y")) pad_y = atoi(val);
    if (!strcmp(name, "pad_x")) pad_x = atoi(val);
    if (!strcmp(name, "no_bias")) no_bias = atoi(val);
    if (!strcmp(name, "silent")) silent = atoi(val);
    if (!strcmp(name, "temp_col_max")) temp_col_max = atoi(val) << 18;
  }

  template<int dim, typename xpu>
  inline void RandInitWeight(mshadow::Random<xpu> *prng,
                             mshadow::Tensor<xpu, dim> mat,
                             index_t in_num, index_t out_num) {
    if (random_type == 0) {
      // gaussian initialization
      prng->SampleGaussian(&mat, 0.0f, init_sigma);
    } else if (random_type == 1) {
      // uniform initialization
      real_t a = sqrt(3.0f / (in_num + out_num));
      if (init_uniform > 0) a = init_uniform;
      prng->SampleUniform(&mat, -a, a);
    } else if (random_type == 2) {
      // kaiming initalization
      float sigma = 0.01;
      if (num_hidden > 0) {
        sigma = sqrt(2.0f / num_hidden);
      } else {
        sigma = sqrt(2.0f / (num_channel * kernel_width * kernel_height));
      }
      // printf("Sigma: %f\n", sigma);
      prng->SampleGaussian(&mat, 0.0f, sigma);
    } else if (random_type == 3) {
      // mshadow::utils::LoadBinary(fi, mat, false);
    }
  }
};
}  // namespace layer
}  // namespace cxxnet
#endif   // CXXNET_LAYER_PARAM_H_
####$$$$ cxxnet-master\cxxnet-master\src\layer/pooling_layer-inl.hpp
#ifndef CXXNET_LAYER_POOLING_LAYER_INL_HPP_
#define CXXNET_LAYER_POOLING_LAYER_INL_HPP_

#include <mshadow/tensor.h>
#include "./layer.h"
#include "./param.h"

namespace cxxnet {
namespace layer {

template<typename Reducer,
         int mode,
         typename xpu,
         bool is_identity = true,
         typename ForwardOp = op::identity,
         typename BackOp = op::identity_grad>
class PoolingLayer : public ILayer<xpu> {
 public:
  virtual ~PoolingLayer(void) {}
  virtual void SetParam(const char *name, const char* val) {
    param_.SetParam(name, val);
  }
  virtual void InitConnection(const std::vector<Node<xpu>*> &nodes_in,
                              const std::vector<Node<xpu>*> &nodes_out,
                              ConnectState<xpu> *p_cstate) {
    InitNode(nodes_in, nodes_out, p_cstate);
  }
  virtual void OnBatchSizeChanged(const std::vector<Node<xpu>*> &nodes_in,
                                  const std::vector<Node<xpu>*> &nodes_out,
                                  ConnectState<xpu> *p_cstate) {
    p_cstate->states[0].Resize(nodes_out[0]->data.shape_);
  }
  virtual void Forward(bool is_train,
                       const std::vector<Node<xpu>*> &nodes_in,
                       const std::vector<Node<xpu>*> &nodes_out,
                       ConnectState<xpu> *p_cstate) {
    using namespace mshadow::expr;
    mshadow::Tensor<xpu,4> &tmp = p_cstate->states[0];
    const int ksize_y = param_.kernel_height;
    const int ksize_x = param_.kernel_width;
    mshadow::Shape<2> pshape = nodes_out[0]->data[0][0].shape_;
    if (!is_identity) {
      nodes_in[0]->data = F<ForwardOp>(nodes_in[0]->data);
    }
    if (mode == kMaxPooling || mode == kSumPooling) {
      tmp = pool<Reducer>(nodes_in[0]->data, pshape, ksize_y, ksize_x, param_.stride);
    }else if (mode == kAvgPooling) {
      tmp = pool<Reducer>(nodes_in[0]->data, pshape, ksize_y, ksize_x, param_.stride)
          * (1.0f / (ksize_y*ksize_x));
    } else {
      utils::Error("Unknown pooling mode");
    }
    mshadow::Copy(nodes_out[0]->data, tmp, nodes_out[0]->data.stream_);
  }
  virtual void Backprop(bool prop_grad,
                        const std::vector<Node<xpu>*> &nodes_in,
                        const std::vector<Node<xpu>*> &nodes_out,
                        ConnectState<xpu> *p_cstate) {
    using namespace mshadow::expr;
    mshadow::Tensor<xpu,4> &tmp = p_cstate->states[0];
    if (prop_grad) {
      const int ksize_y = param_.kernel_height;
      const int ksize_x = param_.kernel_width;
      if (is_identity) {
        if (mode == kMaxPooling || mode == kSumPooling) {
          nodes_in[0]->data = unpool<Reducer>(nodes_in[0]->data, tmp, nodes_out[0]->data, ksize_y, ksize_x, param_.stride);
        }else if (mode == kAvgPooling) {
          nodes_in[0]->data = unpool<Reducer>(nodes_in[0]->data, tmp, nodes_out[0]->data, ksize_y, ksize_x, param_.stride)
              * (1.0f / (ksize_y * ksize_x));
        } else {
          utils::Error("Unknown pooling mode");
        }
      }  else {
        if (mode == kMaxPooling || mode == kSumPooling) {
          nodes_in[0]->data = F<BackOp>(nodes_in[0]->data) *
              unpool<Reducer>(nodes_in[0]->data, tmp, nodes_out[0]->data, ksize_y, ksize_x, param_.stride);
        } else if (mode == kAvgPooling) {
          nodes_in[0]->data = F<BackOp>(nodes_in[0]->data) *
              unpool<Reducer>(nodes_in[0]->data, tmp, nodes_out[0]->data, ksize_y, ksize_x, param_.stride)
              * (1.0f / (ksize_y * ksize_x));
        } else {
          utils::Error("Unknown pooling mode");
        }
      }
    }
  }

 protected:
  inline void InitNode(const std::vector<Node<xpu>*> &nodes_in,
                       const std::vector<Node<xpu>*> &nodes_out,
                       ConnectState<xpu> *p_cstate) {
    utils::Check(nodes_in.size() == 1 && nodes_out.size() == 1,
                 "PoolingLayer: only support 1-1 connection");
    const index_t ksize_y = static_cast<index_t>(param_.kernel_height);
    const index_t ksize_x = static_cast<index_t>(param_.kernel_width);
    const index_t kstride = static_cast<index_t>(param_.stride);
    mshadow::Shape<4> ishape = nodes_in[0]->data.shape_;
    utils::Check(param_.kernel_height > 0 && param_.kernel_width > 0,
                 "must set kernel_size correctly");
    utils::Check(ksize_x <= ishape[3] && ksize_y <= ishape[2],
                 "kernel size exceed input");

    mshadow::Shape<4> oshape = mshadow::
        Shape4(ishape[0], ishape[1],
               std::min(ishape[2] - ksize_y + kstride-1, ishape[2] - 1) / kstride + 1,
               std::min(ishape[3] - ksize_x + kstride-1, ishape[3] - 1) / kstride + 1);
    nodes_out[0]->data.shape_ = oshape;
    // use 1 temp state to store pooled result
    p_cstate->states.push_back(mshadow::TensorContainer<xpu,4>(false));
    p_cstate->states[0].Resize(oshape);
  }
  /*! \brief parameters that potentially be useful */
  LayerParam param_;
};   // class PoolingLayer
}  // namespace layer
}  // namespace cxxnet
#endif  // LAYER_POOLING_LAYER_INL_HPP_

####$$$$ cxxnet-master\cxxnet-master\src\layer/prelu_layer-inl.hpp
#ifndef PRELU_LAYER_INL_HPP
#define PRELU_LAYER_INL_HPP
#pragma once

#include <mshadow/tensor.h>
#include "./layer.h"
#include "./op.h"

namespace cxxnet {
namespace op {
struct mxelu {
  MSHADOW_XINLINE static real_t Map(real_t a, real_t b) {
    return a > 0.0f ? a : a * b;
  }
};

struct mxelu_grad {
  MSHADOW_XINLINE static real_t Map(real_t a, real_t b) {
    return a > 0.0f ? 1 : b;
  }
};

struct prelu_grad {
  MSHADOW_XINLINE static real_t Map(real_t a) {
    return a > 0.0f ? 0.0f : a;
  }
};

struct min {
  MSHADOW_XINLINE static real_t Map(real_t a, real_t b) {
    return a > b ? b : a;
  }
};

struct max {
  MSHADOW_XINLINE static real_t Map(real_t a, real_t b) {
    return a > b ? a : b;
  }
};

} // namespace op
} // namespace cxxnet

namespace cxxnet {
namespace layer {

template<typename xpu>
class PReluLayer : public ILayer<xpu> {
 public:
  PReluLayer(mshadow::Random<xpu> *p_rnd) : prnd_(p_rnd) {
    // setup default value
    init_slope_ = 0.25f;
    init_random_ = 0;
    random_ = 0;
  }
  virtual void SetParam(const char *name, const char* val) {
    if (!strcmp(name, "init_slope")) init_slope_ = atof(val);
    if (!strcmp(name, "random_slope")) init_random_ = atoi(val);
    if (!strcmp(name, "random")) random_ = atof(val);
  }
  virtual void ApplyVisitor(typename ILayer<xpu>::IVisitor *pvisitor) {
    pvisitor->Visit("bias", slope_, gslope_);
  }
  virtual void InitConnection(const std::vector<Node<xpu>*> &nodes_in,
                              const std::vector<Node<xpu>*> &nodes_out,
                              ConnectState<xpu> *p_cstate) {
    utils::Check(nodes_in.size() == 1 && nodes_out.size() == 1,
                 "PReluLayer: only support 1-1 connection");

    nodes_out[0]->data.shape_ = nodes_in[0]->data.shape_;
    if (nodes_in[0]->data.size(1) == 1){
      // This is a fc layer
      channel_ = nodes_in[0]->data.size(3);
    } else {
      // This is a conv layer
      channel_ = nodes_in[0]->data.size(1);
    }
    p_cstate->states.resize(1);
    p_cstate->states[0].Resize(nodes_in[0]->data.shape_);
  }
  virtual void InitModel(void) {
    // resize to correct shape
    slope_.Resize(mshadow::Shape1(channel_));
    gslope_.Resize(mshadow::Shape1(channel_));
    if (init_random_ == 0) {
      slope_ = init_slope_;
    } else {
      slope_ = prnd_->uniform(slope_.shape_);
      slope_ = slope_ * init_slope_;
    }
    gslope_ = 0.0;
  }
  virtual void SaveModel(utils::IStream &fo) const{
    slope_.SaveBinary(fo);
  }
  virtual void LoadModel(utils::IStream &fi){
    slope_.LoadBinary(fi);
    // setup gradient weight
    gslope_.Resize(slope_.shape_);
    gslope_ = 0.0f;
  }
  virtual void SetStream(mshadow::Stream<xpu> *stream) {
    slope_.set_stream(stream);
    gslope_.set_stream(stream);
  }
  virtual void OnBatchSizeChanged(const std::vector<Node<xpu>*> &nodes_in,
                                  const std::vector<Node<xpu>*> &nodes_out,
                                  ConnectState<xpu> *p_cstate) {
    // Do nothing for now
  }
  virtual void Forward(bool is_train,
                       const std::vector<Node<xpu>*> &nodes_in,
                       const std::vector<Node<xpu>*> &nodes_out,
                       ConnectState<xpu> *p_cstate) {
    using namespace mshadow::expr;
    mshadow::Tensor<xpu, 4> &in = nodes_in[0]->data;
    mshadow::Tensor<xpu, 4> &out = nodes_out[0]->data;
    mshadow::TensorContainer<xpu,4> &mask = p_cstate->states[0];
    if (in.size(1) != 1){
      if (is_train){
        mask = broadcast<1>(slope_, in.shape_) *
          (1 + prnd_->uniform(mask.shape_) * random_ * 2.0f - random_);
      } else {
        mask = broadcast<1>(slope_, in.shape_);
      }
    } else {
      if (is_train){
        mask = broadcast<3>(slope_, in.shape_) *
          (1 + prnd_->uniform(mask.shape_) * random_ * 2.0f - random_);
      } else {
        mask = broadcast<3>(slope_, in.shape_);
      }
    }
    mask = F<op::max>(F<op::min>(1, mask), 0);
    out = F<op::mxelu>(in, mask);
  }
  virtual void Backprop(bool prop_grad,
                        const std::vector<Node<xpu>*> &nodes_in,
                        const std::vector<Node<xpu>*> &nodes_out,
                        ConnectState<xpu> *p_cstate) {
    using namespace mshadow::expr;
    mshadow::Tensor<xpu, 4> &in = nodes_in[0]->data;
    mshadow::Tensor<xpu, 4> &out = nodes_out[0]->data;
    mshadow::TensorContainer<xpu,4> &mask = p_cstate->states[0];
    if (in.size(1) != 1){
      gslope_ += sumall_except_dim<1>(F<op::prelu_grad>(in) * out);
      if (prop_grad){
        in = F<op::mxelu_grad>(in, mask) * out;
      }
    } else {
      gslope_ += sumall_except_dim<3>(F<op::prelu_grad>(in) * out);
      if (prop_grad){
        in = F<op::mxelu_grad>(in, mask) * out;
      }
    }
  }

 private:
  /*! \brief random number generator */
  mshadow::Random<xpu> *prnd_;
  /*! \brief init slope */
  float init_slope_;
  /*! \brief the number of channels */
  int channel_;
  /*! \brief slope */
  mshadow::TensorContainer<xpu,1> slope_;
  /*! \brief gradient of slope */
  mshadow::TensorContainer<xpu,1> gslope_;
  /*! \brief whether init slope to [0, init_slope] */
  int init_random_;
  /*! \brief indicate the noise injected in training */
  float random_;
};  // class PReluLayer

} // namespace layer
} // namespace cxxnet
#endif // INSANITY_LAYER_INL_HPP
####$$$$ cxxnet-master\cxxnet-master\src\layer/split_layer-inl.hpp
#ifndef CXXNET_LAYER_SPLIT_LAYER_INL_HPP_
#define CXXNET_LAYER_SPLIT_LAYER_INL_HPP_

#include "./layer.h"
#include "./op.h"


namespace cxxnet {
namespace layer {

template<typename xpu>
class SplitLayer : public ILayer<xpu> {
 public:
  virtual void InitConnection(const std::vector<Node<xpu>*> &nodes_in,
                              const std::vector<Node<xpu>*> &nodes_out,
                              ConnectState<xpu> *p_cstate) {
    //utils::Check(nodes_in.size() == 1 && nodes_out.size() > 1,
    //             "Split layer only support 1-n connection");
    mshadow::Shape<4> oshape = nodes_in[0]->data.shape_;
    for (index_t i = 0; i < nodes_out.size(); ++i){
      nodes_out[i]->data.shape_ = oshape;
    }
  }
  virtual void Forward(bool is_train,
                       const std::vector<Node<xpu>*> &nodes_in,
                       const std::vector<Node<xpu>*> &nodes_out,
                       ConnectState<xpu> *p_cstate) {
    for (index_t i = 0; i < nodes_out.size(); ++i){
      mshadow::Copy(nodes_out[i]->data, nodes_in[0]->data,
        nodes_out[i]->data.stream_);
    }
  }
  virtual void Backprop(bool prop_grad,
                        const std::vector<Node<xpu>*> &nodes_in,
                        const std::vector<Node<xpu>*> &nodes_out,
                        ConnectState<xpu> *p_cstate) {
    if (prop_grad){
      mshadow::Copy(nodes_in[0]->data, nodes_out[0]->data,
        nodes_in[0]->data.stream_);
      for (index_t i = 1; i < nodes_out.size(); ++i){
        nodes_in[0]->data += nodes_out[i]->data;
      }
    }
  }
}; //class SplitLayer
} // namespace layer
} // namespace cxxnet
#endif
####$$$$ cxxnet-master\cxxnet-master\src\layer/visitor.h
#ifndef CXXNET_LAYER_VISITOR_H_
#define CXXNET_LAYER_VISITOR_H_
#include <vector>
#include <string>
#include "./layer.h"
/*!
 * \file visitor.h
 * \brief implementation of visitor of layer weights
 *        tihs file gives util to set/get weights from/to a layer
 * \author Tianqi Chen
 */
namespace cxxnet {
namespace layer {
/*!
 * \brief visitor used to get weight/gradient
 *  from the layer the weight is flattened to 2D shape
 *
 *  Usage Example:
 *     GetWeightVisitor vs("weight");
 *     layer->ApplyVisitor(&vs);
 *     std::vector<mshadow::Tensor<xpu, 2> > weights = vs.data;
 *
 * \tparam xpu the device the data contents lies on
 */
template<typename xpu>
class GetWeightVisitor : public ILayer<xpu>::IVisitor {
 public:
  /*! \brief the weight contents of the layer */
  std::vector<mshadow::Tensor<xpu, 2> > data;
  /*! \brief shapes of ach weight */
  std::vector<std::vector<index_t> > shapes;
  /*! \brief field name of each of the data */
  std::vector<std::string> fields;
  /*!
   * \brief constructor of visitor,
   * \param data_type can only be "grad" or "weight"
   * \param prefix set the prefix to only fetch data whose field name
   *  have the prefix
   */
  GetWeightVisitor(const char *data_type, const char *prefix = "")
      : mode_(0), prefix_(prefix) {
    if (!strcmp(data_type, "weight")) mode_ = 0;
    if (!strcmp(data_type, "grad")) mode_ = 1;
    utils::Assert(mode_ == 0 || mode_ == 1,
      "GetWeightVisitor: do not support data_type %s", data_type);
  }
  // visit
  virtual void Visit(const char *field_name,
                     mshadow::Tensor<xpu, 1> weight,
                     mshadow::Tensor<xpu, 1> grad) {
    this->Visit_(field_name, weight, grad);
  }
  virtual void Visit(const char *field_name,
                     mshadow::Tensor<xpu, 2> weight,
                     mshadow::Tensor<xpu, 2> grad) {
    this->Visit_(field_name, weight, grad);
  }
  virtual void Visit(const char *field_name,
                     mshadow::Tensor<xpu, 3> weight,
                     mshadow::Tensor<xpu, 3> grad) {
    this->Visit_(field_name, weight, grad);
  }
  virtual void Visit(const char *field_name,
                     mshadow::Tensor<xpu, 4> weight,
                     mshadow::Tensor<xpu, 4> grad) {
    this->Visit_(field_name, weight, grad);
  }

 private:
  // internal mode
  int mode_;
  // prefix to match the field name
  std::string prefix_;
  // local visiting function
  template<int dim>
  inline void Visit_(const char *field_name,
                     mshadow::Tensor<xpu, dim> weight,                     
                     mshadow::Tensor<xpu, dim> grad) {
    if (strncmp(prefix_.c_str(), field_name, prefix_.length()) != 0) return;
    fields.push_back(std::string(field_name));
    shapes.push_back(std::vector<index_t>(dim));
    for (int i = 0; i < dim; ++i) {
      shapes.back()[i] = weight.size(i);
    }
    if (mode_ == 0) {
      data.push_back(weight.FlatTo2D());
    } else {
      data.push_back(grad.FlatTo2D());
    }
  }
};
/*!
 * \brief set used to set weight/gradient into layer
 *  the weight must be flattened to 2D to input
 *
 *  Usage Example:
 *     GetSetVisitor vs(data, "weight");
 *     layer->ApplyVisitor(&vs);
 *
 * \tparam xpu the device the data contents lies on
 */
template<typename xpu>
class SetWeightVisitor : public ILayer<xpu>::IVisitor {
 public:
  /*!
   * \brief constructor of visitor,
   * \param data_type can only be "grad" or "weight"
   * \param prefix set the prefix to only fetch data whose field name
   *  have the prefix
   */
  SetWeightVisitor(const std::vector<mshadow::Tensor<xpu, 2> > &data,
                   const char *data_type, const char *prefix = "")
      : data_(data), prefix_(prefix), counter_(0) {
    if (!strcmp(data_type, "weight")) mode_ = 0;
    if (!strcmp(data_type, "grad")) mode_ = 1;
    utils::Assert(mode_ == 0 || mode_ == 1,
      "SetWeightVisitor: do not support data_type %s", data_type);
  }
  // visit
  virtual void Visit(const char *field_name,
                     mshadow::Tensor<xpu, 1> weight,
                     mshadow::Tensor<xpu, 1> grad) {
    this->Visit_(field_name, weight, grad);
  }
  virtual void Visit(const char *field_name,
                     mshadow::Tensor<xpu, 2> weight,
                     mshadow::Tensor<xpu, 2> grad) {
    this->Visit_(field_name, weight, grad);
  }
  virtual void Visit(const char *field_name,
                     mshadow::Tensor<xpu, 3> weight,
                     mshadow::Tensor<xpu, 3> grad) {
    this->Visit_(field_name, weight, grad);
  }
  virtual void Visit(const char *field_name,
                     mshadow::Tensor<xpu, 4> weight,
                     mshadow::Tensor<xpu, 4> grad) {
    this->Visit_(field_name, weight, grad);
  }

 private:
  /*! \brief the weight contents of the layer */
  std::vector<mshadow::Tensor<xpu, 2> > data_;
  // internal mode
  int mode_;
  // prefix to match the field name
  std::string prefix_;
  // index counter
  size_t counter_;
  template<int dim>
  inline void Visit_(const char *field_name,
                     mshadow::Tensor<xpu, dim> weight,
                     mshadow::Tensor<xpu, dim> grad) {
    using mshadow::expr::reshape;
    if (strncmp(prefix_.c_str(), field_name, prefix_.length()) != 0) return;
    utils::Check(counter_ < data_.size(),
                 "SetWeightVisitor: not enough input data");
    if (mode_ == 0) {
      weight = reshape(data_[counter_], weight.shape_);
    } else {
      grad = reshape(data_[counter_], grad.shape_);
    }
    counter_ += 1;
  }
};
}  // namespace layer
}  // namespace cxxnet
#endif  // CXXNET_LAYER_VISITOR_H_
####$$$$ cxxnet-master\cxxnet-master\src\layer/xelu_layer-inl.hpp
#ifndef LAYER_XELU_LAYER_INL_HPP_
#define LAYER_XELU_LAYER_INL_HPP_
#pragma once

#include <mshadow/tensor.h>
#include "./layer.h"
#include "./param.h"
#include "./op.h"
#include "../utils/utils.h"

namespace cxxnet {
namespace layer {

template <typename xpu>
class XeluLayer : public ILayer<xpu> {
 public:
  XeluLayer() { b_ = 5.0f; }
  virtual ~XeluLayer(void) {}
  virtual void SetParam(const char *name, const char* val) {
    if (!strcmp(name, "b")) b_ = atof(val);
  }
  virtual void InitConnection(const std::vector<Node<xpu>*> &nodes_in,
                              const std::vector<Node<xpu>*> &nodes_out,
                              ConnectState<xpu> *p_cstate) {
    utils::Check(nodes_in.size() == 1 && nodes_out.size() == 1,
                 "ActivationLayer Layer only support 1-1 connection");
    nodes_out[0]->data.shape_ = nodes_in[0]->data.shape_;
  }
  virtual void Forward(bool is_train,
                       const std::vector<Node<xpu>*> &nodes_in,
                       const std::vector<Node<xpu>*> &nodes_out,
                       ConnectState<xpu> *p_cstate) {
    using namespace mshadow::expr;
    // InitConnection is already called, no need to check size again
    nodes_in[0]->data = F<op::xelu>(nodes_in[0]->data, b_);
    mshadow::Copy(nodes_out[0]->data, nodes_in[0]->data, nodes_out[0]->data.stream_);
  }
  virtual void Backprop(bool prop_grad,
                        const std::vector<Node<xpu>*> &nodes_in,
                        const std::vector<Node<xpu>*> &nodes_out,
                        ConnectState<xpu> *p_cstate) {
    using namespace mshadow::expr;
    if (prop_grad) {
      nodes_in[0]->data = F<op::xelu_grad>(nodes_in[0]->data, b_) * nodes_out[0]->data;
    }
  }
 private:
  /*! \brief parameters that potentially be useful */
  float b_;
};

} // namespace layer
} // namespace xelu

#endif // XELU_LAYER_INL_HPP_
####$$$$ cxxnet-master\cxxnet-master\src\layer\loss/l2_loss_layer-inl.hpp
#ifndef CXXNET_LAYER_L2_LOSS_LAYER_INL_HPP_
#define CXXNET_LAYER_L2_LOSS_LAYER_INL_HPP_

#include <mshadow/tensor.h>
#include "../layer.h"
#include "./loss_layer_base-inl.hpp"

namespace cxxnet {
namespace layer {
/*! \brief loss function layer */
template<typename xpu>
class L2LossLayer: public LossLayerBase<xpu> {
 public:
  L2LossLayer(const LabelInfo *label_info)
      : LossLayerBase<xpu>(label_info) {}
  virtual ~L2LossLayer(void) {
  }
 protected:
  virtual void Forward_(mshadow::Tensor<xpu, 2> inout_data,
                        mshadow::Stream<xpu> *stream) {
    // Do Nothing
  }
  virtual void SetGradCPU(mshadow::Tensor<cpu, 2> inout_data,
                          const LabelRecord &label) {
    mshadow::Tensor<cpu, 2> lb = label.label;
    utils::Assert(lb.size(0) == inout_data.size(0) && lb.size(1) == inout_data.size(1),
                  "L2LossLayer: label size mismatch");
    for (index_t i = 0; i < inout_data.size(0); ++i) {
      for (index_t j = 0; j < inout_data.size(1); ++j) {
        inout_data[i][j] -= lb[i][j];
      }
    }
  }
};
}  // namespace layer
}  // namespace cxxnet
#endif  // LAYER_L2_LOSS_LAYER_INL_HPP_
####$$$$ cxxnet-master\cxxnet-master\src\layer\loss/loss_layer_base-inl.hpp
#ifndef CXXNET_LAYER_LOSS_LAYER_BASE_INL_HPP_
#define CXXNET_LAYER_LOSS_LAYER_BASE_INL_HPP_

#include <mshadow/tensor.h>
#include "../layer.h"

namespace cxxnet {
namespace layer {
/*! \brief loss function layer */
template<typename xpu>
class LossLayerBase: public ILayer<xpu> {
 public:
  LossLayerBase(const LabelInfo *label_info)
      : stream_(NULL) {
    this->plabelinfo = label_info;
    this->target = "label";
    update_period = 1;
    grad_scale = 1.0f;
  }
  virtual ~LossLayerBase(void) {
  }
  virtual void SetParam(const char *name, const char *val) {
    if (!strcmp(name, "batch_size")) batch_size = atoi(val);
    if (!strcmp(name, "update_period")) update_period = atoi(val);
    if (!strcmp(name, "target")) target = val;
    if (!strcmp(name, "grad_scale")) grad_scale = atof(val);  
  }
  virtual void SetStream(mshadow::Stream<xpu> *stream) {
    this->stream_ = stream;
  }
  virtual void InitConnection(const std::vector<Node<xpu>*> &nodes_in,
                              const std::vector<Node<xpu>*> &nodes_out,
                              ConnectState<xpu> *p_cstate) {
    utils::Check(nodes_in.size() == 1 && nodes_out.size() == 1,
                 "LossLayer: only support 1-1 connection");
    utils::Check(nodes_in[0] == nodes_out[0], "LossLayer is an self-loop Layer");
    utils::Assert(plabelinfo->name2findex != NULL,
                  "LossLayer: LabelInfo.name2findex == NULL");
    std::map<std::string, size_t>::const_iterator it =
        plabelinfo->name2findex->find(target);
    utils::Check(it != plabelinfo->name2findex->end() &&
                 it->first == target, "LossLayer: unknown target=%s",
                 target.c_str());
    target_index = it->second;
  }
  virtual void Forward(bool is_train,
                       const std::vector<Node<xpu>*> &nodes_in,
                       const std::vector<Node<xpu>*> &nodes_out,
                       ConnectState<xpu> *p_cstate) {
    this->Forward_(nodes_in[0]->mat(), stream_);
  }
  virtual void Backprop(bool prop_grad,
                        const std::vector<Node<xpu>*> &nodes_in,
                        const std::vector<Node<xpu>*> &nodes_out,
                        ConnectState<xpu> *p_cstate) {
    utils::Assert(target_index < plabelinfo->fields.size(),
                  "target index exceed bound");
    this->SetGrad(nodes_in[0]->mat(),
                  plabelinfo->fields[target_index],
                  stream_);                  
    // scale gradient by dividing global batch size
    nodes_in[0]->mat() *= (grad_scale / (batch_size * update_period));
  }
  
 protected:
  // the child class can override the following functions in
  // protected fields
  /*!
   * \brief forward transformation called by loss layer
   * \param inout_data the data used as both input and output
   * \param stream the computing stream
   */
  virtual void Forward_(mshadow::Tensor<xpu, 2> inout_data,
                        mshadow::Stream<xpu> *stream) {
  }
  /*!
   * \brief set the gradient value given input data,
   *  this is the function that child class must implement in loss layer
   *
   *  when the function is called, inout_data contains the forward value
   *  This function need to set the content of the inout_data to be the
   *  gradient value given the input data and label
   * \param inout_data the data used as both input and output
   * \param label label sequence of the data
   * \param stream the computing stream
   */
  virtual void SetGrad(mshadow::Tensor<xpu, 2> inout_data,
                       const LabelRecord &label,
                       mshadow::Stream<xpu> *stream) {
    temp_.Resize(inout_data.shape_);
    mshadow::Copy(temp_, inout_data, stream);
    // wait till copy finish
    if (stream != NULL) stream->Wait();
    this->SetGradCPU(temp_, label);
    mshadow::Copy(inout_data, temp_, stream);
  }
  /*!
   * \brief same as SetGrad, but everything is now on CPU
   * normally you only need to implement this function
   *
   *  when the function is called, inout_data contains the forward value
   *  This function need to set the content of the inout_data to be the
   *  gradient value given the input data and label
   * \param inout_data the data used as both input and output
   * \param label label sequence of the data
   */
  virtual void SetGradCPU(mshadow::Tensor<cpu, 2> inout_data,
                          const LabelRecord &label) {
    utils::Error("LossLayerBase::SetGradCPU not implemented");
  }
 private:
  /*! \brief stream used for internal computation */
  mshadow::Stream<xpu> *stream_;
  /*! \brief temp memory to do CPU side computation*/
  mshadow::TensorContainer<cpu, 2> temp_;
  /*!
   * \brief global batch_size set by user, this 
   *        is not necessarily the batch_size in plabelinfo,
   *        since a batch can be divided 
   *        into subbatch to layers in different devices
   */
  int batch_size;
  /*! \brief target field of loss */
  std::string target;
  /*! \brief remembered target index in label info */
  size_t target_index;
  /*! \brief reference to label information */
  const LabelInfo *plabelinfo;
  // update period, used to do scaling
  int update_period;
  /*! \brief gradient scaling of the loss */
  float grad_scale;
};
}  // namespace layer
}  // namespace cxxnet
#endif  // LAYER_LOSS_LAYER_BASE_INL_HPP_
####$$$$ cxxnet-master\cxxnet-master\src\layer\loss/multi_logistic_layer-inl.hpp
#ifndef CXXNET_LAYER_MULTISIGMOID_LAYER_INL_HPP_
#define CXXNET_LAYER_MULTISIGMOID_LAYER_INL_HPP_

#include <mshadow/tensor.h>
#include "../layer.h"
#include "./loss_layer_base-inl.hpp"

namespace cxxnet {
namespace layer {
/*! \brief loss function layer */
template<typename xpu>
class MultiLogisticLayer: public LossLayerBase<xpu> {
 public:
  MultiLogisticLayer(const LabelInfo *label_info)
      : LossLayerBase<xpu>(label_info) {}
  virtual ~MultiLogisticLayer(void) {
  }
 protected:
  virtual void Forward_(mshadow::Tensor<xpu, 2> inout_data,
                        mshadow::Stream<xpu> *stream) {
    inout_data = mshadow::expr::F<op::sigmoid>(inout_data);
  }
  virtual void SetGradCPU(mshadow::Tensor<cpu, 2> inout_data,
                          const LabelRecord &label) {
    mshadow::Tensor<cpu, 2> lb = label.label;
    utils::Assert(lb.size(0) == inout_data.size(0) && lb.size(1) == inout_data.size(1),
                  "MultiLogisticLayer: label size mismatch");
    for (index_t i = 0; i < inout_data.size(0); ++i) {
      for (index_t j = 0; j < inout_data.size(1); ++j) {
        inout_data[i][j] -= lb[i][j];
      }
    }
  }
};
}  // namespace layer
}  // namespace cxxnet
#endif  // LAYER_MULTISIGMOID_LAYER_INL_HPP_
####$$$$ cxxnet-master\cxxnet-master\src\layer\loss/softmax_layer-inl.hpp
#ifndef CXXNET_LAYER_SOFTMAX_LAYER_INL_HPP_
#define CXXNET_LAYER_SOFTMAX_LAYER_INL_HPP_

#include <mshadow/tensor.h>
#include "../layer.h"
#include "./loss_layer_base-inl.hpp"

namespace cxxnet {
namespace layer {
/*! \brief loss function layer */
template<typename xpu>
class SoftmaxLayer: public LossLayerBase<xpu> {
 public:
  SoftmaxLayer(const LabelInfo *label_info)
      : LossLayerBase<xpu>(label_info) {}
  virtual ~SoftmaxLayer(void) {
  }
 protected:
  virtual void Forward_(mshadow::Tensor<xpu, 2> inout_data,
                        mshadow::Stream<xpu> *stream) {
    mshadow::Softmax(inout_data, inout_data);
  }
  virtual void SetGradCPU(mshadow::Tensor<cpu, 2> inout_data,
                          const LabelRecord &label) {
    mshadow::Tensor<cpu, 2> lb = label.label;
    utils::Assert(lb.size(0) == inout_data.size(0) && lb.size(1) == 1,
                  "SoftmaxLayer: label size mismatch");
    for (mshadow::index_t i = 0; i < inout_data.size(0); ++i) {
      index_t k = static_cast<index_t>(lb[i][0]);
      inout_data[i][k] -= 1.0f;
    }
  }
};
}  // namespace layer
}  // namespace cxxnet
#endif  // LAYER_SOFTMAX_LAYER_INL_HPP_
####$$$$ cxxnet-master\cxxnet-master\src\nnet/neural_net-inl.hpp
#ifndef CXXNET_NNET_NEURAL_NET_INL_HPP_
#define CXXNET_NNET_NEURAL_NET_INL_HPP_
/*!
 * \file neural_net-inl.hpp
 * \brief implementation of common neuralnet
 * \author Tianqi Chen
 */
#include <vector>
#include <utility>
#include <mshadow/tensor.h>
#include "../layer/layer.h"
#include "../layer/visitor.h"
#include "../updater/updater.h"
#include "../utils/utils.h"
#include "../utils/io.h"
#include "../utils/thread.h"
#include "./nnet_config.h"

namespace cxxnet {
namespace nnet {
/*! \brief implementation of abstract neural net */
template<typename xpu>
struct NeuralNet {
  /*! \brief network configuration configure */
  const NetConfig &cfg;
  /*! \brief maximum batch_size */
  mshadow::index_t max_batch;
  /*! \brief label information */
  layer::LabelInfo label_info;
  /*! \brief nodes in the neural net */
  std::vector<layer::Node<xpu> > nodes;
  /*! \brief layers in the neural net */
  std::vector<layer::Connection<xpu> > connections;
  /*! \brief updaters in the neural net */
  std::vector<std::vector<updater::IAsyncUpdater<xpu>*> > updaters;
  /*! \brief random number generator */
  mshadow::Random<xpu> rnd;
  /*! \brief stream for this  */
  mshadow::Stream<xpu> *stream;
  // constructor do nothing
  NeuralNet(const NetConfig &cfg,
            mshadow::index_t batch_size,
            int seed,
            mshadow::Stream<xpu> *stream)
      : cfg(cfg), rnd(seed), stream(stream) {
    // set maximum batch
    this->max_batch = batch_size;
    rnd.set_stream(stream);
    label_info.name2findex = &cfg.label_name_map;
  }
  ~NeuralNet(void) {
    this->FreeSpace();
  }
  /*! \brief save model to file */
  inline void SaveModel(utils::IStream &fo) const {
    for (index_t i = 0; i < connections.size(); ++i) {
      for (size_t j = 0; j < updaters[i].size(); ++j) {
        updaters[i][j]->UpdateWait();
      }
      if (connections[i].type != layer::kSharedLayer) {
        connections[i].layer->SaveModel(fo);
      }
    }
  }
  /*! \brief initial model parameters in the beginning */
  inline void InitModel(void) {
    this->InitNet();
    this->ConfigConntions();
    for (size_t i = 0; i < connections.size(); ++i) {
      if (this->cfg.layers[i].name != "") {
        printf("Initializing layer: %s\n", this->cfg.layers[i].name.c_str());
      } else {
        printf("Initializing layer: %d\n", static_cast<int>(i));
      }
      layer::Connection<xpu> &c = connections[i];
      c.layer->InitConnection(c.nodes_in, c.nodes_out, &c.state);
      c.SetStream(stream);
    }
    for (size_t i = 0; i < connections.size(); ++i) {
      if (connections[i].type != layer::kSharedLayer) {
        connections[i].layer->InitModel();
      }
    }
  }
  /*! \brief load model from stream */
  inline void LoadModel(utils::IStream &fi) {
    this->FreeSpace();
    this->InitNet();
    this->ConfigConntions();
    for (size_t i = 0; i < connections.size(); ++i) {
      if (connections[i].type != layer::kSharedLayer) {
        connections[i].SetStream(stream);
        connections[i].layer->LoadModel(fi);
      }
    }
    for (size_t i = 0; i < connections.size(); ++i) {
      layer::Connection<xpu> &c = connections[i];
      c.layer->InitConnection(c.nodes_in, c.nodes_out, &c.state);
      c.SetStream(stream);
    }
  }
  /*!
   * \brief forward prop
   * \param is_train whether is training phase
   * \param batch the input batch
   */
  inline void Forward(bool is_train,
                      mshadow::Tensor<cpu,4> batch,
                      std::vector<mshadow::Tensor<cpu,4> > extra_data,
                      bool need_sync) {
    // check if we need to adjust batch size according to the input
    this->AdjustBatchSize(batch.size(0));
    // copy data into node
    mshadow::Copy(nodes[0].data, batch, stream);
    for (size_t i = 0; i < extra_data.size(); ++i) {
      mshadow::Copy(nodes[i + 1].data, extra_data[i], stream);
    }
    // setup updater notification
    for (size_t i = connections.size(); i != 0; --i) {
      for (size_t j = 0; j < updaters[i - 1].size(); ++j) {
        updaters[i - 1][j]->BeforeForward();
      }
    }
    // start forward prop
    for (size_t i = 0; i < connections.size(); ++i) {
      layer::Connection<xpu> &c = connections[i];
      for (size_t j = 0; j < updaters[i].size(); ++j) {
        updaters[i][j]->UpdateWait();
      }
      c.layer->Forward(is_train, c.nodes_in, c.nodes_out, &c.state);
    }
  }
  /*!
   * \brief backprop
   * \param prop_to_input whether prop gradient to input node
   */
  inline void Backprop(bool prop_to_input,
                       bool need_update,
                       long update_epoch) {
    for (size_t i = connections.size(); i > 0; --i) {
      layer::Connection<xpu> &c = connections[i - 1];
      for (size_t j = 0; j < updaters[i - 1].size(); ++j) {
        updaters[i - 1][j]->BeforeBackprop(c.nodes_in, c.nodes_out);
      }
      c.layer->Backprop(i != 1 || prop_to_input,
                        c.nodes_in, c.nodes_out, &c.state);
      // wait backprop to complete before call update
      if (updaters[i - 1].size() != 0) stream->Wait();
      for (size_t j = 0; j < updaters[i - 1].size(); ++j) {
        updaters[i - 1][j]->AfterBackprop(need_update, update_epoch);
      }
    }
  }
  /*!
   * \brief update model parameters
   * \param epoch number of epoches
   */
  inline void Update(size_t epoch) {
    for (size_t i = 0; i < updaters.size(); ++ i) {
      for (size_t j = 0; j < updaters[i].size(); ++ j) {
        updaters[i][j]->Update(epoch);
      }
    }
  }
  /*!
   * \brief notify round start
   * \param round round counter
   */
  inline void StartRound(int round) {
    for (size_t i = 0; i < updaters.size(); ++ i) {
      for (size_t j = 0; j < updaters[i].size(); ++ j) {
        updaters[i][j]->StartRound(round);
      }
    }
  }
  // create the updaters
  inline void InitUpdaters(mshadow::ps::ISharedModel<xpu, real_t> *ps, int devid) {
    for (int i = 0; i < cfg.param.num_layers; ++i) {
      std::vector<updater::IAsyncUpdater<xpu>*> out;
      if (connections[i].type != layer::kSharedLayer) {
        updater::CreateAsyncUpdaters
            (i, devid, ps,
             cfg.updater_type.c_str(),
             &rnd, cfg.layers[i].type,
             connections[i].layer,
             &out);
        for (size_t k = 0; k < out.size(); ++k) {
          for (size_t j = 0; j < cfg.defcfg.size(); ++j) {
            out[k]->SetParam(cfg.defcfg[j].first.c_str(),
                             cfg.defcfg[j].second.c_str());
          }
          for (size_t j = 0; j < cfg.layercfg[i].size(); ++j) {
            out[k]->SetParam(cfg.layercfg[i][j].first.c_str(),
                             cfg.layercfg[i][j].second.c_str());
          }
          out[k]->SetStream(stream);
          out[k]->Init();
        }
      }
      updaters.push_back(out);
    }
    utils::Assert(updaters.size() == connections.size(),
                  "updater size do not match number of layers");
  }
  // intialize the space of nodes
  inline void InitNodes(void) {
    for (size_t i = 0; i < nodes.size(); ++ i) {
      mshadow::Shape<4> s = nodes[i].data.shape_;
      nodes[i].AllocSpace();
      printf("node[%s].shape: %u,%u,%u,%u\n", this->cfg.node_names[i].c_str(),
        s[0], s[1], s[2], s[3]);
    }
  }
 private:
  // intialize the neural net data structure
  inline void InitNet(void) {
    nodes.resize(cfg.param.num_nodes);
    mshadow::Shape<3> s = cfg.param.input_shape;
    // setup input shape
    nodes[0].data.shape_ = mshadow::Shape4(max_batch, s[0], s[1], s[2]);
    // setup extra data
    for (int i = 0; i < cfg.param.extra_data_num; ++i) {
      const std::vector<int>& extra_shape = cfg.extra_shape;
      nodes[i + 1].data.shape_ = mshadow::Shape4(
        max_batch, extra_shape[i * 3], extra_shape[i * 3 + 1], extra_shape[i * 3 + 2]);
    }
    // input layer
    for (int i = 0; i < cfg.param.num_layers; ++i) {
      const NetConfig::LayerInfo &info = cfg.layers[i];
      layer::Connection<xpu> c;
      c.type = info.type;
      for (size_t j = 0; j < info.nindex_in.size(); ++j) {
        c.nodes_in.push_back(&nodes[info.nindex_in[j]]);
      }
      for (size_t j = 0; j < info.nindex_out.size(); ++j) {
        c.nodes_out.push_back(&nodes[info.nindex_out[j]]);
      }
      if (c.type == layer::kSharedLayer) {
        utils::Assert(info.primary_layer_index >=0, "primary_layer_index problem");
        utils::Check(info.primary_layer_index < static_cast<int>(connections.size()),
                     "shared layer primary_layer_index exceed bound");
        c.layer = connections[info.primary_layer_index].layer;
        utils::Check(c.layer->AllowSharing(),
                     "some layer you set shared do not allow sharing");
      } else {
        c.layer = layer::CreateLayer(c.type, &rnd, &label_info);
      }
      connections.push_back(c);
    }
  }
  // configure the parameters of layer
  inline void ConfigConntions(void) {
    for (int i = 0; i < cfg.param.num_layers; ++ i) {
      if (connections[i].type == layer::kSharedLayer) continue;
      for (size_t j = 0; j < cfg.defcfg.size(); ++j) {
        connections[i].layer->SetParam(cfg.defcfg[j].first.c_str(),
                                       cfg.defcfg[j].second.c_str());
      }
      for (size_t j = 0; j < cfg.layercfg[i].size(); ++j) {
        connections[i].layer->SetParam(cfg.layercfg[i][j].first.c_str(),
                                       cfg.layercfg[i][j].second.c_str());
      }
    }
  }
  // adjust batch size to a new value, the batch_size must be smaller than max_batch
  inline void AdjustBatchSize(mshadow::index_t batch_size) {
    utils::Assert(max_batch >= batch_size, "cannot set batch size larger than max batch");
    if (batch_size != nodes[0].data.size(0)) {
      for (size_t i = 0; i < nodes.size(); ++i) {
        nodes[i].data.shape_[0] = batch_size;
      }
      for (size_t i = 0; i < connections.size(); ++ i) {
        layer::Connection<xpu> &c = connections[i];
        c.layer->OnBatchSizeChanged(c.nodes_in, c.nodes_out, &c.state);
      }
    }
  }
  /*! \brief free all space allocated in this struct*/
  inline void FreeSpace(void) {
    // wait all actions to complete before free
    stream->Wait();
    for (size_t i = 0; i < nodes.size(); ++i) {
      nodes[i].FreeSpace();
    }
    for (size_t i = 0; i < connections.size(); ++i) {
      if (connections[i].type != layer::kSharedLayer) {
        delete connections[i].layer;
      }
    }
    for (size_t i = 0; i < updaters.size(); ++i) {
      for (size_t j = 0; j < updaters[i].size(); ++j) {
        delete updaters[i][j];
      }
    }
    nodes.clear(); connections.clear(); updaters.clear();
  }
};

/*!
 * \brief neural net that runs with an independent thread backed by NeuralNet
 * \tparam
 */
template<typename xpu>
class NeuralNetThread {
 public:
  /*! \brief create a new neural net thread on specific device */
  NeuralNetThread(const NetConfig &cfg,
                  mshadow::ps::ISharedModel<xpu, real_t> *ps,
                  int device_id,
                  mshadow::index_t batch_size,
                  int seed,
                  bool new_thread = true)
      : cfg(cfg), pserver(ps),
        device_id(device_id), batch_size(batch_size),
        seed(seed), new_thread(new_thread) {
    net_ = NULL;
    if (new_thread) {
      destroy_signal = false;
      job_start.Init(0);
      job_end.Init(0);
      worker_thread.Start(ThreadEntry, this);
      // wait until net is created
      job_end.Wait();
    } else {
      mshadow::InitTensorEngine<xpu>(device_id);
      stream = mshadow::NewStream<xpu>();
      net_ = new NeuralNet<xpu>(cfg, batch_size, seed, stream);
    }
  }
  // destructor
  ~NeuralNetThread(void) {
    if (net_ != NULL) {
      if (new_thread) {
        destroy_signal = true;
        job_start.Post();
        worker_thread.Join();
        job_start.Destroy();
        job_end.Destroy();
      } else {
        delete net_;
        mshadow::DeleteStream(stream);
        mshadow::ShutdownTensorEngine<xpu>();
      }
    }
  }

  /*!
   * \brief wait till the the thread finishes current task
   * This function MUST be called every time before running next job
   */
  inline void WaitJob(void) {
    if (new_thread) job_end.Wait();
  }
  inline void InitModel(void) {
    this->task = kInitModel;
    this->ExecTask();
  }
  inline void SaveModel(utils::IStream &fo) {
    iparam_fp = &fo;
    this->task = kSaveModel;
    this->ExecTask();
  }
  inline void LoadModel(utils::IStream &fi) {
    iparam_fp = &fi;
    this->task = kLoadModel;
    this->ExecTask();
  }
  inline void Update(size_t epoch) {
    iparam_epoch = epoch;
    this->task = kUpdate;
    this->ExecTask();
  }
  inline void StartRound(int round) {
    iparam_epoch = static_cast<size_t>(round);
    this->task = kStartRound;
    this->ExecTask();
  }
  /*! \brief run a training forward backprop pass */
  inline void TrainForwardBackprop(mshadow::Tensor<cpu,4> batch,
                                   const std::vector<mshadow::Tensor<mshadow::cpu, 4> >& extra_data,
                                   const layer::LabelInfo &label_info,
                                   const std::vector<std::pair<int, mshadow::Tensor<cpu, 4> > >& req,
                                   bool prop_to_input,
                                   bool need_sync,
                                   bool need_update,
                                   size_t update_epoch) {
    utils::Assert(net_ != NULL, "thread must be initialized before use");
    net_->label_info = label_info;
    iparam_batch = batch;
    iparam_flag = prop_to_input;
    oparam_req = req;
    iparam_need_sync = need_sync;
    iparam_need_update = need_update;
    iparam_epoch = update_epoch;
    iparam_extra_data = extra_data;
    this->task = kTrainProp;
    this->ExecTask();
  }
  /*! \brief run a predicting forward pass, copy final layer  */
  inline void PredictForward(mshadow::Tensor<cpu, 4> batch,
                             const std::vector<mshadow::Tensor<mshadow::cpu, 4> > &extra_data) {
    iparam_batch = batch;
    iparam_extra_data = extra_data;
    this->task = kPredForward;
    this->ExecTask();
  }
  // copy node data out
  inline void CopyNodeData(int nid, mshadow::Tensor<cpu, 4> out_data) {
    iparam_nid = nid;
    oparam_node = out_data;
    this->task = kCopyNode;
    this->ExecTask();
  }
  // copy layer from a fs
  inline void CopyLayer(int lid, utils::IStream &fi) {
    iparam_fp = &fi;
    iparam_lid = lid;
    this->task = kCopyLayer;
    this->ExecTask();
  }
  // set weight into certain layer
  inline void SetWeight(int lid,
                        mshadow::Tensor<cpu, 2> weight,
                        const char *tag) {
    iparam_lid = lid;
    iparam_weight = weight;
    iparam_tag = tag;
    this->task = kSetWeight;
    this->ExecTask();
  }

  // set weight into certain layer
  inline void GetWeight(int lid,
                        mshadow::TensorContainer<cpu, 2> *out_weight,
                        std::vector<index_t> *out_shape,
                        const char *tag) {
    iparam_lid = lid;
    oparam_weight = out_weight;
    oparam_shape = out_shape;
    iparam_tag = tag;
    this->task = kGetWeight;
    this->ExecTask();
  }
  // return reference of node
  inline const NeuralNet<xpu> &net(void) const{
    return *net_;
  }

 private:
  // type of task that can be executed
  enum TaskType {
    kInitModel,
    kLoadModel,
    kSaveModel,
    kUpdate,
    kStartRound,
    kTrainProp,
    kPredForward,
    kCopyNode,
    kCopyLayer,
    kSetWeight,
    kGetWeight
  };
  // thread related code
  inline static CXXNET_THREAD_PREFIX ThreadEntry(void *pthread) {
    static_cast<NeuralNetThread<xpu>*>(pthread)->RunThread();
    utils::ThreadExit(NULL);
    return NULL;
  }
  inline void RunThread(void) {
    mshadow::InitTensorEngine<xpu>(device_id);
    stream = mshadow::NewStream<xpu>();
    // allocate net
    net_ = new NeuralNet<xpu>(cfg, batch_size, seed, stream);
    // tell the master that net is created
    job_end.Post();
    while (!destroy_signal) {
      job_start.Wait();
      if (destroy_signal) break;
      this->TaskDispatch();
      job_end.Post();
    }
    delete net_;
    mshadow::DeleteStream(stream);
    mshadow::ShutdownTensorEngine<xpu>();
  }
  inline void ExecTask(void) {
    if (new_thread) {
      job_start.Post();
    } else {
      this->TaskDispatch();
    }
  }
  inline void TaskDispatch(void) {
    utils::Assert(net_ != NULL, "thread must be initialized before use");
    switch (task) {
      case kInitModel: {
        net_->InitModel();
        net_->InitUpdaters(pserver, device_id);
        net_->InitNodes();
        stream->Wait();
        return;
      }
      case kLoadModel: {
        net_->LoadModel(*iparam_fp);
        net_->InitUpdaters(pserver, device_id);
        net_->InitNodes();
        stream->Wait();
        return;
      }
      case kSaveModel: net_->SaveModel(*iparam_fp); return;
      case kUpdate: net_->Update(iparam_epoch); return;
      case kStartRound: net_->StartRound(static_cast<int>(iparam_epoch)); return;
      case kTrainProp: {
        if (iparam_batch.size(0) == 0) return;
        net_->Forward(true, iparam_batch, iparam_extra_data, iparam_need_sync);
        for (index_t i = 0; i < oparam_req.size(); ++i) {
          index_t id = oparam_req[i].first + (oparam_req[i].first < 0 ? net_->nodes.size() : 0);
          utils::Assert(id < net_->nodes.size(), "nid out of range");
          mshadow::Copy(oparam_req[i].second, net_->nodes[id].data, stream);
        }
        net_->Backprop(iparam_flag, iparam_need_update, iparam_epoch);
        stream->Wait();
        return;
      }
      case kPredForward: {
        net_->Forward(false, iparam_batch, iparam_extra_data, true);
        return;
      }
      case kCopyNode: {
        if (iparam_nid < 0) iparam_nid += static_cast<int>(net_->nodes.size());
        utils::Assert(iparam_nid < static_cast<int>(net_->nodes.size()), "nid out of range");
        mshadow::Copy(oparam_node, net_->nodes[iparam_nid].data, stream);
        stream->Wait();
        return;
      }
      case kCopyLayer: {
        utils::Assert(iparam_lid < static_cast<int>(net_->connections.size()),
                      "lid out of range");
        net_->connections[iparam_lid].layer->LoadModel(*iparam_fp);
        return;
      }
      case kSetWeight: {
        utils::Assert(iparam_lid < static_cast<int>(net_->connections.size()),
                      "lid out of range");
        mshadow::TensorContainer<xpu, 2> tmp;
        tmp.Resize(iparam_weight.shape_);
        mshadow::Copy(tmp, iparam_weight, stream);
        stream->Wait();
        std::vector<mshadow::Tensor<xpu, 2> > data;
        data.push_back(tmp);
        layer::SetWeightVisitor<xpu> vs(data, "weight", iparam_tag.c_str());
        net_->connections[iparam_lid].layer->ApplyVisitor(&vs);
        return;
      }
      case kGetWeight: {
        utils::Assert(iparam_lid < static_cast<int>(net_->connections.size()),
                      "lid out of range");
        layer::GetWeightVisitor<xpu> vs("weight", iparam_tag.c_str());
        net_->connections[iparam_lid].layer->ApplyVisitor(&vs);
        if (vs.data.size() == 0) {
          oparam_shape->resize(0);
          oparam_weight->Resize(mshadow::Shape2(0, 0));
        } else {
          oparam_weight->Resize(vs.data[0].shape_);
          mshadow::Copy(*oparam_weight, vs.data[0]);
          *oparam_shape = vs.shapes[0];
          utils::Assert(vs.fields[0] == iparam_tag,
                        "GetWeight:shape mismatch");
        }
        return;
      }
    }
  }
  // the following are fields that are used to pass parameters in or out
  // used to copy out fields in the last layer
  mshadow::Tensor<cpu, 4> oparam_node;
  // used to copy out fields in a given layer
  std::vector<std::pair<int, mshadow::Tensor<cpu, 4> > > oparam_req;
  // output weight parameter
  mshadow::TensorContainer<cpu, 2> *oparam_weight;
  // output shape parameter
  std::vector<index_t> *oparam_shape;
  // input flag
  bool iparam_flag;
  // special input flag for update
  bool iparam_need_sync, iparam_need_update;
  // input epochs
  size_t iparam_epoch;
  // input node id
  int iparam_nid;
  // input layer id
  int iparam_lid;
  // input parameters of file pointers
  utils::IStream *iparam_fp;
  // input batch
  mshadow::Tensor<cpu, 2> iparam_weight;
  // input tag
  std::string iparam_tag;
  // input batch
  mshadow::Tensor<cpu, 4> iparam_batch;
  // input extra data
  std::vector<mshadow::Tensor<cpu,4> > iparam_extra_data;
  // current task
  TaskType task;
  // intenal net implementation
  NeuralNet<xpu> *net_;
  // configuration
  const NetConfig &cfg;
  // signal the destruction of object
  bool destroy_signal;
  // signal of jobs
  utils::Semaphore job_end, job_start;
  // thread object
  utils::Thread worker_thread;
  // parameter server
  mshadow::ps::ISharedModel<xpu, real_t> *pserver;
  // stream used for computation
  mshadow::Stream<xpu> *stream;
  // device id used to intialize tensor engine
  int device_id;
  // local batch size of this thread
  mshadow::index_t batch_size;
  // seed used to intialize this thread
  int seed;
  // whether the implementation is backed by a new thread
  const bool new_thread;
};
}  // namespace nnet
}  // namespace cxxnet
#endif  // CXXNET_NNET_NEURAL_NET_INL_HPP_
####$$$$ cxxnet-master\cxxnet-master\src\nnet/nnet.h
#ifndef CXXNET_NNET_NNET_H_
#define CXXNET_NNET_NNET_H_
/*!
 * \file nnet.h
 * \brief trainer abstraction
 * \author Bing Xu, Tianqi Chen
 */

#include <vector>
#include <mshadow/tensor.h>
#include "../global.h"
#include "../utils/io.h"
#include "../io/data.h"

namespace cxxnet {
namespace nnet {
/*! \brief interface for network */
class INetTrainer{
 public:
  virtual ~INetTrainer(void) {}
  /*!
   * \brief Set parametters
   * \param name parameter name
   * \param val string for configuration
   */
  virtual void SetParam(const char *name, const char *val) = 0;
  /*! \brief random initalize model */
  virtual void InitModel(void) = 0;
  /*! \brief save model to stream */
  virtual void SaveModel(utils::IStream &fo) = 0;
  /*! \brief load model from stream */
  virtual void LoadModel(utils::IStream &fi) = 0;
  /*!
   * \brief inform the updater that a new round has been started
   * \param round round counter
   */
  virtual void StartRound(int round) = 0;
  /*!
   * \brief update model parameter
   * \param training data batch
   */
  virtual void Update(const DataBatch &data) = 0;
  /*!
   * \brief evaluate a test statistics, output results as a string
   * \param iter_eval the iterator containing the evaluation data
   * \param data_name the name of the dataset, used to construct the returing string
   * \return a string containing the evaluation result in format data_name-metric:result
   */
  virtual std::string Evaluate(IIterator<DataBatch> *iter_eval,
                               const char *data_name) = 0;
  /*!
   * \brief predict labels for a given data batch
   * \param out_preds the prediction result for each data sample
   * \param batch the data to be predicted
   */
  virtual void Predict(mshadow::TensorContainer<mshadow::cpu, 1> *out_preds,
                       const DataBatch &batch) = 0;
  /*!
   * \brief extract the content of a node for a given data batch
   * \param out_preds the content for each data sample in the node
   * \param batch the data to be passed
   * \param node_name the name of the node to be extracted
   */
  virtual void ExtractFeature(mshadow::TensorContainer<mshadow::cpu, 4> *out_preds,
                              const DataBatch &batch,
                              const char *node_name) = 0;
  /*!
   * \brief Initialize current model from a input stream.
   *  This method will copy the weight from corresponding layers if their names match.
   * \param fi the stream that the model will be initialized from
   */
  virtual void CopyModelFrom(utils::IStream &fi) = 0;   
  /*!
   * \brief set weight of certain layer
   * \param layer_name the name of the layer
   * \param weight_tag type of weight can be "wmat" or "bias"
   */
  virtual void SetWeight(mshadow::Tensor<mshadow::cpu, 2> weight,
                         const char *layer_name,
                         const char *weight_tag) = 0;
  /*!
   * \brief set weight of certain layer
   * \param out_weight hold the output weight data, Flattened to 2D
   * \param out_shape hold the shape of the weight
   * \param 
   * \param weight_tag type of weight can be "wmat" or "bias"
   */
  virtual void GetWeight(mshadow::TensorContainer<mshadow::cpu, 2> *out_weight,
                         std::vector<index_t> *out_shape,
                         const char *layer_name,
                         const char *weight_tag) = 0;
};

/*!
 * \brief create a net implementation
 * \param net_type network type, used to select trainer variants
 * \tparam device type the net lies
 */
template<typename xpu>
INetTrainer* CreateNet(int net_type);
}  // namespace nnet
}  // namespace cxxnet
#endif // CXXNET_NNET_NNET_H_
####$$$$ cxxnet-master\cxxnet-master\src\nnet/nnet_config.h
#ifndef CXXNET_NNET_NNET_CONFIG_H_
#define CXXNET_NNET_NNET_CONFIG_H_
/*!
 * \file nnet_config.h
 * \brief network structure configuration
 * \author Tianqi Chen, Bing Xu
 */
#include <vector>
#include <utility>
#include <string>
#include <cstring>
#include <map>
#include <mshadow/tensor.h>
#include "../layer/layer.h"
#include "../utils/utils.h"
#include "../utils/io.h"
// #include "glog/logging.h"

namespace cxxnet {
namespace nnet {
/*!
 * \brief this is an object that records the configuration of a neural net
 *    it is used to store the network structure, and reads in configuration
 *    that associates with each of the layers
 */
struct NetConfig {
  /*! \brief general model parameter */
  struct NetParam {
    /*! \brief number of nodes in the network */
    int num_nodes;
    /*! \brief number of layers in the network */
    int num_layers;
    /*! \brief input shape, not including batch dimension */
    mshadow::Shape<3> input_shape;
    /*! \brief whether the configuration is finalized and the network structure is fixed */
    int init_end;
    /*! \brief the number of extra data */
    int extra_data_num;
    /*! \brief reserved fields, used to extend data structure */
    int reserved[31];
    /*! \brief constructor */
    NetParam(void) {
      memset(reserved, 0, sizeof(reserved));
      num_nodes = 0;
      num_layers = 0;
      input_shape = mshadow::Shape3(0, 0, 0);
      init_end = 0;
      extra_data_num = 0;
    }
  };
  /*! \brief information about each layer */
  struct LayerInfo {
    /*! \brief type of layer */
    layer::LayerType type;
    /*!
     * \brief the index of primary layer,
     *  this field is only used when layer type is kSharedLayer
     */
    int primary_layer_index;
    /*! \brief layer name */
    std::string name;
    /*! \brief input node index */
    std::vector<int> nindex_in;
    /*! \brief output node node index */
    std::vector<int> nindex_out;
    LayerInfo(void) : primary_layer_index(-1), name() {
    }
    /*! \brief equality check */
    inline bool operator==(const LayerInfo &b) const {
      if (type != b.type ||
          primary_layer_index != b.primary_layer_index ||
          nindex_in.size() != b.nindex_in.size() ||
          nindex_out.size() != b.nindex_out.size())  return false;
      if (name != b.name) return false;
      for (size_t i = 0; i < nindex_in.size(); ++i) {
        if (nindex_in[i] != b.nindex_in[i]) return false;
      }
      for (size_t i = 0; i < nindex_out.size(); ++i) {
        if (nindex_out[i] != b.nindex_out[i]) return false;
      }
      return true;
    }
  };
  // model parameters that defines network configuration
  /*! \brief generic parameters about net */
  NetParam param;
  /*! \brief per layer information */
  std::vector<LayerInfo> layers;
  /*! \brief name of each node */
  std::vector<std::string> node_names;
  // -----------------------------
  // Training parameters that can be changed each time, even when network is fixed
  // the training parameters will not be saved during LoadNet SaveNet
  //
  /*! \brief maps node name to node index */
  std::map<std::string, int> node_name_map;
  /*! \brief maps tag to layer index */
  std::map<std::string, int> layer_name_map;
  /*! \brief type of updater function */
  std::string updater_type;
  /*! \brief type of synchronization function */
  std::string sync_type;
  /*! \brief map to map input sequence to label name*/
  std::map<std::string, size_t> label_name_map;
  /*! \brief vector to record range of input label*/
  std::vector<std::pair<index_t, index_t> > label_range;
  /*! \brief default global configuration */
  std::vector< std::pair< std::string, std::string > > defcfg;
  /*! \brief extra parameter configuration specific to this layer */
  std::vector< std::vector< std::pair<std::string, std::string> > > layercfg;
  /*! \brief stores the shape of extra data */
  std::vector<int> extra_shape;
  // constructor
  NetConfig(void) {
    updater_type = "sgd";
    sync_type = "simple";
    label_name_map["label"] = 0;
    label_range.push_back(std::make_pair(0, 1));
  }
  /*!
   * \brief save network structure to output
   *  note: this operation does not save the training configurations
   *        such as updater_type, batch_size
   * \param fo output stream
   */
  inline void SaveNet(utils::IStream &fo) const {
    fo.Write(&param, sizeof(param));
    if (param.extra_data_num != 0) {
      fo.Write(extra_shape);
    }
    utils::Assert(param.num_layers == static_cast<int>(layers.size()),
                  "model inconsistent");
    utils::Assert(param.num_nodes == static_cast<int>(node_names.size()),
                  "num_nodes is inconsistent with node_names");
    for (int i = 0; i < param.num_nodes; ++i) {
      fo.Write(node_names[i]);
    }
    for (int i = 0; i < param.num_layers; ++i) {
      fo.Write(&layers[i].type, sizeof(layer::LayerType));
      fo.Write(&layers[i].primary_layer_index, sizeof(int));
      fo.Write(layers[i].name);
      fo.Write(layers[i].nindex_in);
      fo.Write(layers[i].nindex_out);
    }
  }
  /*!
   * \brief save network structure from input
   *  note: this operation does not load the training configurations
   *        such as updater_type, batch_size
   * \param fi output stream
   */
  inline void LoadNet(utils::IStream &fi) {
    utils::Check(fi.Read(&param, sizeof(param)) != 0,
                 "NetConfig: invalid model file");
    node_names.resize(param.num_nodes);
    if (param.extra_data_num != 0) {
      utils::Check(fi.Read(&extra_shape) != 0,
        "NetConfig: Reading extra data shape failed.");
    }
    for (int i = 0; i < param.num_nodes; ++i) {    
      utils::Check(fi.Read(&node_names[i]),
                   "NetConfig: invalid model file");
    }
    node_name_map.clear();
    for (size_t i = 0; i < node_names.size(); ++i) {
      node_name_map[node_names[i]] = static_cast<int>(i);
    }
    layers.resize(param.num_layers);
    layercfg.resize(param.num_layers);
    layer_name_map.clear();
    for (int i = 0; i < param.num_layers; ++i) {
      utils::Check(fi.Read(&layers[i].type, sizeof(layer::LayerType)) != 0,
                 "NetConfig: invalid model file");
      utils::Check(fi.Read(&layers[i].primary_layer_index, sizeof(int)) != 0,
                 "NetConfig: invalid model file");
      utils::Check(fi.Read(&layers[i].name), "NetConfig: invalid model file");
      utils::Check(fi.Read(&layers[i].nindex_in), "NetConfig: invalid model file");
      utils::Check(fi.Read(&layers[i].nindex_out), "NetConfig: invalid model file");
      if (layers[i].type == layer::kSharedLayer) {
        utils::Check(layers[i].name.length() == 0, "SharedLayer must not have name");
      } else {
        if (layers[i].name != "") {
          utils::Check(layer_name_map.count(layers[i].name) == 0,
                       "NetConfig: invalid model file, duplicated layer name: %s",
                       layers[i].name.c_str());
          layer_name_map[layers[i].name] = i;
        }
      }
    }
    this->ClearConfig();
  }
  inline void SetGlobalParam(const char *name, const char *val) {
    if (!strcmp(name, "updater")) updater_type = val;
    if (!strcmp(name, "sync")) sync_type = val;
    {
      unsigned a, b;
      if (sscanf(name, "label_vec[%u,%u)", &a, &b) == 2) {
        label_range.push_back(std::make_pair((index_t)a,
                                             (index_t)b));
        label_name_map[val] = label_range.size() - 1;
      }
    }
  }
  /*!
   * \brief setup configuration, using the config string pass in
   */
  inline void Configure(const std::vector< std::pair<std::string, std::string> > &cfg) {
    this->ClearConfig();
    if (node_names.size() == 0 && node_name_map.size() == 0) {
      node_names.push_back(std::string("in"));
      node_name_map["in"] = 0;
    }
    node_name_map["0"] = 0;
    // whether in net config mode
    int netcfg_mode = 0;
    // remembers what is the last top node
    int cfg_top_node = 0;
    // current configuration layer index
    int cfg_layer_index = 0;
    for (size_t i = 0; i < cfg.size(); ++i) {
      const char *name = cfg[i].first.c_str();
      const char *val = cfg[i].second.c_str();
      if (!strcmp(name, "extra_data_num")) {
        int num;
        sscanf(val, "%d", &num);
        for (int i = 0; i < num; ++i) {
          char name[256];
          sprintf(name, "in_%d", i + 1);
          if (node_name_map.find(name) == node_name_map.end()) {
            node_names.push_back(name);
            node_name_map[name] = i + 1;
          }
        }
        param.extra_data_num = num;
      }
      if (!strncmp(name, "extra_data_shape[", 17)) {
        int extra_num;
        int x, y, z;
        utils::Check(sscanf(name, "extra_data_shape[%d", &extra_num) == 1,
          "extra data shape config incorrect");
        utils::Check(sscanf(val, "%d,%d,%d", &x, &y, &z) == 3,
          "extra data shape config incorrect");
        extra_shape.push_back(x);
        extra_shape.push_back(y);
        extra_shape.push_back(z);
      }
      if (param.init_end == 0) {
        if (!strcmp( name, "input_shape")) {
          unsigned x, y, z;
          utils::Check(sscanf(val, "%u,%u,%u", &z, &y, &x) == 3,
                       "input_shape must be three consecutive integers without space example: 1,1,200 ");
          param.input_shape = mshadow::Shape3(z, y, x);
        }
      }
      if (netcfg_mode != 2) this->SetGlobalParam(name, val);
      if (!strcmp(name, "netconfig") && !strcmp(val, "start")) netcfg_mode = 1;
      if (!strcmp(name, "netconfig") && !strcmp(val, "end")) netcfg_mode = 0;
      if (!strncmp(name, "layer[", 6)) {
        LayerInfo info = this->GetLayerInfo(name, val, cfg_top_node, cfg_layer_index);
        netcfg_mode = 2;
        if (param.init_end == 0) {
          utils::Assert(layers.size() == static_cast<size_t>(cfg_layer_index),
                        "NetConfig inconsistent");
          layers.push_back(info);
          layercfg.resize(layers.size());
        } else {
          utils::Check(cfg_layer_index < static_cast<int>(layers.size()),
                       "config layer index exceed bound");
          utils::Check(info == layers[cfg_layer_index],
                       "config setting does not match existing network structure");
        }
        if (info.nindex_out.size() == 1) {
          cfg_top_node = info.nindex_out[0];
        } else {
          cfg_top_node = -1;
        }
        cfg_layer_index += 1;
        continue;
      }
      if (netcfg_mode == 2) {
        utils::Check(layers[cfg_layer_index - 1].type != layer::kSharedLayer,
                     "please do not set parameters in shared layer, set them in primary layer");
        layercfg[cfg_layer_index - 1].push_back(std::make_pair(std::string(name), std::string(val)));
      } else {
        defcfg.push_back(std::make_pair(std::string(name), std::string(val)));
      }
    }
    if (param.init_end == 0) this->InitNet();
  }
  inline int GetLayerIndex(const char *name) const {
    std::string key = name;
    std::map<std::string, int>::const_iterator it
        = layer_name_map.find(key);
    if (it == layer_name_map.end() || key != it->first) {
      utils::Error("unknown layer name %s", name);
    }
    return it->second;
  }

 private:
  // configuration parser to parse layer info, support one to to one connection for now
  // extend this later to support multiple connections
  inline LayerInfo GetLayerInfo(const char *name, const char *val,
                                int top_node, int cfg_layer_index) {
    LayerInfo inf;
    int inc;
    char ltype[256], tag[256];
    char src[256], dst[256];
    if (sscanf(name, "layer[+%d", &inc) == 1) {
      utils::Check(top_node >=0,
                   "ConfigError: layer[+1] is used, "\
                   "but last layer have more than one output"\
                   "use layer[input-name->output-name] instead");
      inf.nindex_in.push_back(top_node);
      if (sscanf(name, "layer[+1:%[^]]]", tag) == 1) {
        inf.nindex_out.push_back(GetNodeIndex(tag, true));
      } else {
        if (inc == 0) {
          inf.nindex_out.push_back(top_node);
        } else {
          utils::SPrintf(tag, sizeof(tag), "!node-after-%d", top_node);
          inf.nindex_out.push_back(GetNodeIndex(tag, true));
        }
      }
    } else if (sscanf(name, "layer[%[^-]->%[^]]]", src, dst) == 2) {
      this->ParseNodeIndex(src, &inf.nindex_in, false);
      this->ParseNodeIndex(dst, &inf.nindex_out, true);
    } else {
      utils::Error("ConfigError: invalid layer format %s", name);
    }
    std::string s_tag, layer_name;
    if (sscanf(val , "%[^:]:%s", ltype, tag) == 2) {
      inf.type = layer::GetLayerType(ltype);
      layer_name = tag;
    } else {
      inf.type = layer::GetLayerType(val);
    }
    if (inf.type == layer::kSharedLayer) {
      const char* layer_type_start = strchr(ltype, '[');
      utils::Check(layer_type_start != NULL,
                   "ConfigError: shared layer must specify tag of layer to share with");
      s_tag = layer_type_start + 1;
      s_tag = s_tag.substr(0, s_tag.length() - 1);
      utils::Check(layer_name_map.count(s_tag) != 0,
                   "ConfigError: shared layer tag %s is not defined before", s_tag.c_str());
      inf.primary_layer_index = layer_name_map[s_tag];
    } else {
      if (layer_name.length() != 0) {
        if (layer_name_map.count(layer_name) != 0) {
          utils::Check(layer_name_map[layer_name] == cfg_layer_index,
                       "ConfigError: layer name in the configuration file do not "\
                       "match the name stored in model");
        } else {
          layer_name_map[layer_name] = cfg_layer_index;
        }
        inf.name = layer_name;
      }
    }
    return inf;
  }
  inline void ParseNodeIndex(char *nodes,
                             std::vector<int> *p_indexs,
                             bool alloc_unknown) {
    char *pch = strtok(nodes, ",");
    while (pch != NULL) {
      p_indexs->push_back(GetNodeIndex(pch, alloc_unknown));
      pch = strtok(NULL, ",");
    }
  }
  inline int GetNodeIndex(const char *name, bool alloc_unknown) {
    std::string key = name;
    std::map<std::string, int>::iterator it
        = node_name_map.find(key);
    if (it == node_name_map.end() || key != it->first) {
      utils::Check(alloc_unknown,
                   "ConfigError: undefined node name %s,"\
                   "input node of a layer must be specified as output of another layer"\
                   "presented before the layer declaration", name);
      int value = static_cast<int>(node_names.size());
      node_name_map[key] = value;
      node_names.push_back(key);
      return value;
    } else {
      return it->second;
    }
  }
  /*! \brief guess parameters, from current setting, this will set init_end in param to be true */
  inline void InitNet(void) {
    param.num_nodes = 0;
    param.num_layers = static_cast<int>(layers.size());
    for (size_t i = 0; i < layers.size(); ++ i) {
      const LayerInfo &info = layers[i];
      for (size_t j = 0; j < info.nindex_in.size(); ++j) {
        param.num_nodes = std::max(info.nindex_in[j] + 1, param.num_nodes);
      }
      for (size_t j = 0; j < info.nindex_out.size(); ++j) {
        param.num_nodes = std::max(info.nindex_out[j] + 1, param.num_nodes);
      }
    }
    utils::Assert(param.num_nodes == static_cast<int>(node_names.size()),
                  "num_nodes is inconsistent with node_names");
    param.init_end = 1;
  }
  /*! \brief clear the configurations */
  inline void ClearConfig(void) {
    defcfg.clear();
    for (size_t i = 0; i < layercfg.size(); ++i) {
      layercfg[i].clear();
    }
  }
};
}  // namespace nnet
}  // namespace cxxnet
#endif
####$$$$ cxxnet-master\cxxnet-master\src\nnet/nnet_impl-inl.hpp
#include <utility>
#include <vector>
#include <string>
#include <cstring>
#include <cstdlib>
#include "./nnet.h"
#include "../utils/io.h"
#include "../utils/metric.h"
#include "./neural_net-inl.hpp"


namespace cxxnet {
namespace nnet {
/*! \brief implementation of neural network trainer, using multiple threads */
template<typename xpu>
class CXXNetThreadTrainer : public INetTrainer {
 public:
  CXXNetThreadTrainer(void) {
    batch_size = 100;
    update_period = 1;
    sample_counter = 0;
    eval_train = 1;
    epoch_counter = 0;
    seed = 0;
    pserver = NULL;
    type_pserver = "UNSPECIFIED";
  }
  virtual ~CXXNetThreadTrainer(void) {
    this->FreeNet();
  }
  virtual void SetParam(const char *name, const char *val) {
    if (!strcmp(name, "dev")) {
      devices_.clear();
      const char *devs = strchr(val, ':');
      if (devs != NULL) {
        int a, b;
        if (sscanf(devs + 1, "%d-%d", &a, &b) == 2) {
          for (int i = a; i <= b; ++i) {
            devices_.push_back(i);
          }
        } else {
          std::string s_dev = devs + 1;
          char *ptr = strtok(&s_dev[0], ",");
          while (ptr != NULL) {
            utils::Check(sscanf(ptr, "%d", &a) == 1, "invalid device format");
            devices_.push_back(a);
            ptr = strtok(NULL, ",");
          }
        }
      }
    }
    if (!strcmp(name, "batch_size")) batch_size = static_cast<mshadow::index_t>(atoi(val));
    if (!strcmp(name, "update_period")) update_period = atoi(val);
    if (!strcmp(name, "eval_train")) eval_train = atoi(val);
    if (!strcmp(name, "seed")) seed = atoi(val);
    if (!strcmp(name, "param_server")) type_pserver = val;
    if (!strncmp(name, "metric", 6)) {
      char label_name[256];
      char node_name[256];
      if (sscanf(name, "metric[%[^,],%[^]]", label_name, node_name) == 2) {
        metric.AddMetric(val, label_name); train_metric.AddMetric(val, label_name);
        eval_nodes.push_back(std::make_pair(node_name, 0));
      } else {
        metric.AddMetric(val, "label"); train_metric.AddMetric(val, "label");
        eval_nodes.push_back(std::make_pair("", -1));
      }
    }
    cfg.push_back(std::make_pair(std::string(name), std::string(val)));
  }
  virtual void InitModel(void) {
    this->InitNet();
    nets_[0]->InitModel();
    nets_[0]->WaitJob();
    this->Save2ModelBlob();
    for(size_t i = 1; i < nets_.size(); ++i) {
      utils::MemoryBufferStream fs(&model_blob_);
      nets_[i]->LoadModel(fs);
      nets_[i]->WaitJob();
    }
    this->InitTemp();
  }
  virtual void SaveModel(utils::IStream &fo) {
    this->Save2ModelBlob();
    net_cfg.SaveNet(fo);
    fo.Write(&epoch_counter, sizeof(epoch_counter));
    fo.Write(model_blob_);
  }
  virtual void LoadModel(utils::IStream &fi) {
    net_cfg.LoadNet(fi);
    fi.Read(&epoch_counter, sizeof(epoch_counter));
    this->FreeNet();
    this->InitNet();
    fi.Read(&model_blob_);
    for (size_t i = 0; i < nets_.size(); ++i) {
      utils::MemoryBufferStream fs(&model_blob_);
      nets_[i]->LoadModel(fs);
      nets_[i]->WaitJob();
    }
    this->InitTemp();
  }
  virtual void CopyModelFrom(utils::IStream &fi) {
    this->FreeNet();
    this->InitModel();

    // Load the original net
    NetConfig old_cfg;
    old_cfg.LoadNet(fi);
    fi.Read(&epoch_counter, sizeof(epoch_counter));
    epoch_counter = 0;
    NeuralNet<cpu> old_net(old_cfg, 0, 0, NULL);
    std::string old_model;
    fi.Read(&old_model);
    utils::MemoryBufferStream os(&old_model);
    old_net.LoadModel(os);

    // Compare original net and current net
    for (index_t i = 0; i < old_cfg.layers.size(); ++i){
      std::string& old_name = old_cfg.layers[i].name;
      for (index_t j = 0; j < net_cfg.layers.size(); ++j){
        std::string& new_name = net_cfg.layers[j].name;
        if (old_name == new_name && old_name != ""){
          printf("Copying layer %s\n", old_name.c_str());
          std::string data;
          utils::MemoryBufferStream fs(&data);
          old_net.connections[i].layer->SaveModel(fs);
          for (index_t k = 0; k < nets_.size(); ++k){
            fs.Seek(0);
            nets_[k]->CopyLayer(j, fs);
            nets_[k]->WaitJob();
          }
        }
      }
    }
  }
  virtual void StartRound(int round) {
    for (size_t i = 0; i < nets_.size(); ++i) {
      nets_[i]->StartRound(round);
    }
    this->WaitAllJobs();
  }
  virtual void Update(const DataBatch& data) {
    mshadow::Shape<4> oshape = out_temp.shape_;
    oshape[0] = data.batch_size;
    out_temp.Resize(oshape);

    const size_t ndevice = devices_.size();
    mshadow::index_t step = std::max((batch_size + ndevice - 1) / ndevice, 1UL);

    bool need_sync = sample_counter % update_period == 0;
    bool need_update = (sample_counter + 1) % update_period == 0;
    layer::LabelInfo info = GetLabelInfo(data);
    this->InitEvalReq(eval_req);
    for (mshadow::index_t i = nets_.size(); i != 0; --i) {
      mshadow::index_t begin = std::min((i - 1) * step, data.batch_size);
      mshadow::index_t end = std::min(i * step, data.batch_size);
      std::vector<mshadow::Tensor<mshadow::cpu, 4> > extra_data;
      for (mshadow::index_t j = 0; j < data.extra_data.size(); ++j){
        extra_data.push_back(data.extra_data[j].Slice(begin, end));
      }
      std::vector<std::pair<int, mshadow::Tensor<cpu, 4> > > batch_eval_req;
      for (index_t j = 0; j < eval_req.size(); ++j) {
        batch_eval_req.push_back(
          std::make_pair(eval_req[j].first, eval_req[j].second.Slice(begin, end)));
      }
      nets_[i - 1]->TrainForwardBackprop(data.data.Slice(begin, end),
                                         extra_data,
                                         info.Slice(begin, end),
                                         batch_eval_req,
                                         false, need_sync,
                                         need_update, epoch_counter);

    }
    this->WaitAllJobs();
    if (eval_train != 0) {
      std::vector<mshadow::Tensor<mshadow::cpu, 2> > scores;
      for (index_t i = 0; i < eval_req.size(); ++i) {
        scores.push_back(eval_req[i].second.FlatTo2D());
      }
      train_metric.AddEval(scores, info);
    }
    if (++sample_counter >= update_period) {
      sample_counter = 0;
      epoch_counter += 1;
    }
  }
  virtual void Predict(mshadow::TensorContainer<mshadow::cpu, 1> *out_preds,
                       const DataBatch &data) {
    mshadow::TensorContainer<mshadow::cpu, 1> &preds = *out_preds;
    std::vector<std::pair<int, mshadow::TensorContainer<cpu, 4> > > req;
    req.push_back(std::make_pair(nets_[0]->net().nodes.size() - 1, out_temp));
    mshadow::Shape<4> s = nets_[0]->net().nodes.back().data.shape_;
    s[0] = batch_size;
    req[0].second.Resize(s);
    this->ForwardTo(req, data);
    preds.Resize(mshadow::Shape1(batch_size));
    for (index_t i = 0; i < batch_size; ++i) {
      preds[i] = this->TransformPred(req[0].second[i][0][0]);
    }
  }
  virtual void ExtractFeature(mshadow::TensorContainer<mshadow::cpu, 4> *out_preds,
                              const DataBatch &batch,
                              const char *node_name_) {
    std::string node_name = node_name_;
    std::map<std::string, int> &name_map = net_cfg.node_name_map;
    int node_id, offset;
    if (sscanf(node_name.c_str(), "top[-%d]", &offset) == 1) {
      int nnode = static_cast<int>(nets_[0]->net().nodes.size());
      utils::Check(offset >= 1 && offset <= nnode,
                   "ExtractFeature: offset must be within num_node range");
      node_id = nnode - offset;
    } else {
      utils::Check(name_map.find(node_name) != name_map.end(),
                   "ExtractFeature: Cannot find node name: %s", node_name.c_str());
      node_id = name_map[node_name];
    }
    std::vector <std::pair<int, mshadow::TensorContainer<cpu, 4> > > req;
    req.push_back(std::make_pair(node_id, *out_preds));
    mshadow::Shape<4> s = nets_[0]->net().nodes[node_id].data.shape_;
    s[0] = batch_size;
    req[0].second.Resize(s);
    this->ForwardTo(req, batch);
    *out_preds = req[0].second;
  }
  virtual std::string Evaluate(IIterator<DataBatch> *iter_eval, const char *data_name) {
    std::string ret;
    if (eval_train != 0) {
      ret += train_metric.Print("train");
      train_metric.Clear();
    }
    if (iter_eval == NULL) return ret;
    metric.Clear();
    iter_eval->BeforeFirst();
    while (iter_eval->Next()) {
      const DataBatch& batch = iter_eval->Value();
      this->ForwardTo(eval_req, batch);
      std::vector<mshadow::Tensor<cpu, 2> > scores;
      for (index_t i = 0; i < eval_req.size(); ++i) {
        scores.push_back(eval_req[i].second.Slice(
          0, eval_req[i].second.size(0) - batch.num_batch_padd).FlatTo2D());
      }
      metric.AddEval(scores, GetLabelInfo(batch));
    }
    ret += metric.Print(data_name);
    return ret;
  }
  virtual void SetWeight(mshadow::Tensor<mshadow::cpu, 2> weight,
                         const char *layer_name,
                         const char *weight_tag) {
    utils::Check(!strcmp(weight_tag, "bias") ||
                 !strcmp(weight_tag, "wmat"),
                 "NNet.SetWeight: weight tag can only be bias or wmat");
    int layer_index = net_cfg.GetLayerIndex(layer_name);
    for (size_t i = 0; i < nets_.size(); ++i) {
      nets_[i]->SetWeight(layer_index, weight, weight_tag);
    }
    this->WaitAllJobs();
  }
  virtual void GetWeight(mshadow::TensorContainer<mshadow::cpu, 2> *out_weight,
                         std::vector<index_t> *out_shape,
                         const char *layer_name,
                         const char *weight_tag) {
    utils::Check(!strcmp(weight_tag, "bias") ||
                 !strcmp(weight_tag, "wmat"),
                 "NNet.GetWeight: weight tag can only be bias or wmat");
    int layer_index = net_cfg.GetLayerIndex(layer_name);
    nets_[0]->GetWeight(layer_index, out_weight, out_shape, weight_tag);
    nets_[0]->WaitJob();
  }

 private:
  inline layer::LabelInfo GetLabelInfo(const DataBatch &data) const {
    layer::LabelInfo info;
    layer::LabelRecord rec;
    info.name2findex = &net_cfg.label_name_map;
    for (size_t i = 0; i < net_cfg.label_range.size(); ++i) {
      index_t begin =  net_cfg.label_range[i].first;
      index_t end =  net_cfg.label_range[i].second;
      rec.label = mshadow::Tensor<cpu, 2>
          (data.label.dptr_ + begin,
           mshadow::Shape2(data.batch_size, end - begin),
           data.label.stride_, NULL);
      info.fields.push_back(rec);
    }
    return info;
  }
  inline float TransformPred(mshadow::Tensor<cpu,1> pred) {
    if (pred.size(0) != 1) {
      return GetMaxIndex(pred);
    } else {
      return pred[0];
    }
  }
  inline static int GetMaxIndex(mshadow::Tensor<cpu,1> pred) {
    index_t maxidx = 0;
    for(index_t i = 1; i < pred.size(0); ++ i) {
      if(pred[i] > pred[maxidx]) maxidx = i;
    }
    return maxidx;
  }
  inline void ForwardTo(std::vector<std::pair<int, mshadow::TensorContainer<cpu, 4> > >& req,
                        const DataBatch &data) {
    this->InitEvalReq(req);
    const size_t ndevice = devices_.size();
    mshadow::index_t step = std::max((batch_size + ndevice - 1) / ndevice, 1UL);
    for (mshadow::index_t i = nets_.size(); i != 0; --i) {
      mshadow::index_t begin = std::min((i - 1) * step, data.batch_size);
      mshadow::index_t end = std::min(i * step, data.batch_size);
      mshadow::Tensor<cpu, 4> mbatch = data.data.Slice(begin, end);
      std::vector<mshadow::Tensor<mshadow::cpu, 4> > extra_data;
      for (mshadow::index_t j = 0; j < data.extra_data.size(); ++j){
        extra_data.push_back(data.extra_data[j].Slice(begin, end));
      }
      nets_[i - 1]->PredictForward(mbatch, extra_data);
    }
    this->WaitAllJobs();
    // copy results out
    for (mshadow::index_t j = 0; j < req.size(); ++j) {
      for (mshadow::index_t i = nets_.size(); i != 0; --i) {
        mshadow::index_t begin = std::min((i - 1) * step, data.batch_size);
        mshadow::index_t end = std::min(i * step, data.batch_size);
        nets_[i - 1]->CopyNodeData(req[j].first, req[j].second.Slice(begin, end));
      }
      this->WaitAllJobs();
    }
  }

  inline void WaitAllJobs(void) {
    for (size_t i = nets_.size(); i != 0; --i) {
      nets_[i - 1]->WaitJob();
    }
  }
  inline void Save2ModelBlob(void) {
    // save to model blob
    model_blob_.clear();
    utils::MemoryBufferStream fs(&model_blob_);
    nets_[0]->SaveModel(fs);
    nets_[0]->WaitJob();
  }
  inline void InitNet(void) {
    utils::Assert(nets_.size() == 0, "net must be empty before this");
    net_cfg.Configure(cfg);
    if (devices_.size() == 0) devices_.push_back(0);
    size_t ndevice = devices_.size();
    mshadow::index_t step = std::max((batch_size + ndevice - 1) / ndevice, 1UL);
    while (step * (devices_.size() - 1) >= batch_size) {
      devices_.pop_back();
    }
    if (ndevice > devices_.size()) {
      ndevice = devices_.size();
      if (silent == 0) {
        printf("Warning: The number of devices is induce mini-batch=%u\n" \
               "We can equally use %lu devices to cover the batch_size\n", step, ndevice);
      }
    }
    this->InitParamServer();
    for (size_t i = 0; i < ndevice; ++i) {
      nets_.push_back(new NeuralNetThread<xpu>(net_cfg, pserver,
                                               devices_[i], step, i + seed * 100));
    }
    if (silent == 0) {
      printf("finish initialization with %lu devices\n", devices_.size());
    }
    for (index_t i = 0; i < eval_nodes.size(); ++i) {
      std::map<std::string, int> &name_map = net_cfg.node_name_map;
      if (eval_nodes[i].second < 0) {
        eval_req.push_back(std::make_pair(net_cfg.param.num_nodes - 1,
          mshadow::TensorContainer<cpu, 4>()));
      } else {
        utils::Check(name_map.find(eval_nodes[i].first) != name_map.end(),
          "Cannot find node name: %s\n", eval_nodes[i].first.c_str());
        eval_req.push_back(std::make_pair(name_map[eval_nodes[i].first],
          mshadow::TensorContainer<cpu, 4>()));
      }
    }
  }
  inline void InitParamServer(void) {
    utils::Assert(pserver == NULL, "net must be empty before this");
    if (type_pserver == "UNSPECIFIED") {
      if (devices_.size() <=1) type_pserver = "NONE";
      else type_pserver = "local";
    }
    if (type_pserver != "NONE") {
      pserver = mshadow::ps::CreateSharedModel<xpu, real_t>(type_pserver.c_str());
      for (size_t i = 0; i < cfg.size(); ++i) {
        pserver->SetParam(cfg[i].first.c_str(), cfg[i].second.c_str());
      }
      if (devices_.size() == 0) devices_.push_back(0);
      pserver->Init(devices_);
    }
  }
  inline void InitTemp(void) {
    mshadow::Shape<4> oshape = nets_[0]->net().nodes.back().data.shape_;
    oshape[0] = batch_size;
    out_temp.Resize(oshape);
  }
  inline void FreeNet(void) {
    for (size_t i = 0; i < nets_.size(); ++i) {
      delete nets_[i];
    }
    nets_.clear();
    if (pserver != NULL) {
      delete pserver;
      pserver = NULL;
    }
  }
  inline void InitEvalReq(
    std::vector<std::pair<int, mshadow::TensorContainer<cpu, 4> > >& req) {
    for (mshadow::index_t i = 0; i < req.size(); ++i) {
      index_t id = req[i].first;
      mshadow::Shape<4> oshape = nets_[0]->net().nodes[id].data.shape_;
      oshape[0] = batch_size;
      req[i].second.Resize(oshape);
    }
  }
  /*! \brief parameter server */
  mshadow::ps::ISharedModel<xpu, real_t> *pserver;
  /*! \brief type of parameter server */
  std::string type_pserver;
  /*! \brief epoch counter */
  long epoch_counter;
  /*! \brief seed to the layers */
  int seed;
  /*! \brief silent*/
  int silent;
  /*! \brief update period */
  int update_period;
  /*! \brief sample counter */
  int sample_counter;
  /*! \brief show train eval */
  int eval_train;
  /*! \brief evaluator */
  utils::MetricSet metric;
  /*! \brief evaluator for train */
  utils::MetricSet train_metric;
  /*! \brief final space of output node */
  mshadow::TensorContainer<cpu, 4> out_temp;
  /*! \brief request of copy out nodes, used in evaluation */
  std::vector<std::pair<int, mshadow::TensorContainer<cpu, 4> > > eval_req;
  /*! \brief the name of nodes used in evaluation */
  std::vector<std::pair<std::string, int > > eval_nodes;
  // ------- model part --------
  /*! \brief batch size */
  mshadow::index_t batch_size;
  /*! \brief record the devices_ used in each thread */
  std::vector<int> devices_;
  /*! \brief serialized model in CPU */
  std::string model_blob_;
  /*! \brief threads of neural nets */
  std::vector<NeuralNetThread<xpu>*> nets_;

  /*! \brief network configuration type */
  NetConfig net_cfg;
  /*! \brief history of configurations */
  std::vector< std::pair<std::string, std::string> > cfg;
};

template<typename xpu>
INetTrainer *CreateNet_(int net_type) {
  return new CXXNetThreadTrainer<xpu>();
}
}  // namespace nnet
}  // cxxnet
####$$$$ cxxnet-master\cxxnet-master\src\nnet/nnet_impl.cpp
#define _CRT_SECURE_NO_WARNINGS
#define _CRT_SECURE_NO_DEPRECATE
// this is where the actual implementations are
#include "nnet_impl-inl.hpp"
// specialize the cpu implementation
namespace cxxnet {
namespace nnet {
template<>
INetTrainer* CreateNet<cpu>(int net_type) {
  return CreateNet_<cpu>(net_type);
}
}  // namespace nnet
}  // namespace cxxnet
####$$$$ cxxnet-master\cxxnet-master\src\nnet/nnet_impl.cu
#define _CRT_SECURE_NO_WARNINGS
#define _CRT_SECURE_NO_DEPRECATE
// this is where the actual implementations are
#include "nnet_impl-inl.hpp"
// specialize the gpu implementation
namespace cxxnet {
namespace nnet {
template<>
INetTrainer* CreateNet<gpu>(int net_type) {
  return CreateNet_<gpu>(net_type);
}
}  // namespace nnet
}  // namespace cxxnet
####$$$$ cxxnet-master\cxxnet-master\src\nnet/nnet_ps_server.cpp
#define _CRT_SECURE_NO_WARNINGS
#define _CRT_SECURE_NO_DEPRECATE

#include <map>
#include <sstream>
#include <mshadow-ps/ps.h>
#include "./nnet_config.h"
#include "../layer/param.h"
#include "../utils/config.h"
#include "../updater/updater.h"

#if MSHADOW_DIST_PS
namespace PS {
DECLARE_string(app_file);
} // namespace PS
#endif

namespace cxxnet {
namespace nnet {
class CXXNetUpdater : public mshadow::ps::IModelUpdater<real_t> {
 public:
  CXXNetUpdater(void) : rnd(0) {
    seed = 0;
  }
  virtual ~CXXNetUpdater(void) {
    for (std::map<int, UpdaterEntry*>::iterator
             it = updaters.begin(); it != updaters.end(); ++it) {
      delete it->second;
    }
  }
  virtual void SetParam(const char *name, const char *val) {
    if (!strcmp(name, "seed")) seed = atoi(val);
    cfgvec.push_back(std::make_pair(std::string(name), std::string(val)));
  }
  virtual void InitUpdater(int rank, const std::string &conf) {
    // FIXME
    // std::stringstream ss(conf);
    // utils::ConfigStreamReader reader(ss);

    // if (PS::Postoffice::instance().app()->isServer()) {
#if MSHADOW_DIST_PS
    if (PS::FLAGS_app_file.size()) {
      utils::ConfigIterator reader(PS::FLAGS_app_file.c_str());
      while (reader.Next()) {
        this->SetParam(reader.name(), reader.val());
      }
    }
#endif

    // start configure settings
    cfg.Configure(cfgvec);
    rnd.Seed(seed + rank * 17);
  }
  virtual void InitModel(int key, real_t *dptr, size_t size) {
    if (updaters.find(key) != updaters.end()) {
      // already inited
      // TODO do some checks here
      return;
    }
    updaters[key] = new UpdaterEntry();
    UpdaterEntry &e = *updaters[key];
    e.key = key;
    e.weight = mshadow::Tensor<cpu, 1>
        (dptr, mshadow::Shape1(size)).FlatTo2D();
    e.updater = updater::CreateUpdater<cpu>
        (cfg.updater_type.c_str(),
         &rnd, e.weight, e.weight,
         updater::DecodeTag(key));
    e.is_bias = !strcmp(updater::DecodeTag(key), "bias");
    const int i = key / updater::kDataKeyStep;
    utils::Assert(i < cfg.param.num_layers, "layer index exceed bound");
    e.layer_type = cfg.layers[i].type;
    for (size_t j = 0; j < cfg.defcfg.size(); ++j) {
      e.SetParam(cfg.defcfg[j].first.c_str(),
                 cfg.defcfg[j].second.c_str());
    }
    for (size_t j = 0; j < cfg.layercfg[i].size(); ++j) {
      e.SetParam(cfg.layercfg[i][j].first.c_str(),
                 cfg.layercfg[i][j].second.c_str());
    }
    e.Init(&rnd);
  }
  virtual void Update(int key, real_t *dptr, size_t size) {
    std::map<int, UpdaterEntry*>::iterator it
        = updaters.find(key);
    utils::Assert(it != updaters.end() && it->first == key,
                  "must call initkey first before calling update");
    it->second->Update(dptr, size);
  }

 private:
  struct UpdaterEntry {
    int key;
    // whether this is bias
    bool is_bias;
    // type of layer
    layer::LayerType layer_type;
    // epoch we run
    long epoch;
    // parameters
    layer::LayerParam param;
    updater::IUpdater<cpu> *updater;
    mshadow::Tensor<cpu, 2> weight;
    // constructor
    UpdaterEntry(void) : epoch(0) {
      updater = NULL;
    }
    ~UpdaterEntry(void) {
      delete updater;
    }
    inline void SetParam(const char *name,
                         const char *val) {
      updater->SetParam(name, val);
      param.SetParam(name, val);
    }
    inline void Init(mshadow::Random<cpu> *p_rnd) {
      updater->Init();
      if (is_bias) {
        weight = param.init_bias;
      } else {
        if (layer_type == layer::kConv) {
          param.RandInitWeight(p_rnd, weight, param.num_channel, param.kernel_height *param.kernel_width);
        } else {
          utils::Check(param.random_type != 1 || param.init_uniform > 0.0f,
                       "xavier not supported in PS");
          param.RandInitWeight(p_rnd, weight, param.num_hidden, param.num_hidden);
        }
      }
    }
    // update given gradient
    inline void Update(real_t *grad, size_t size) {
      utils::Assert(size == weight.MSize(),
                    "PS: weight and gradient size inconsistent");
      updater->Update(epoch,
                      mshadow::Tensor<cpu, 2>(grad, weight.shape_));
      epoch += 1;
    }
  };

 private:
  int seed;
  mshadow::Random<cpu> rnd;
  // updaters
  std::map<int, UpdaterEntry*> updaters;
  /*! \brief network configuration type */
  NetConfig cfg;
  /*! \brief history of configurations */
  std::vector< std::pair<std::string, std::string> > cfgvec;
};
}  // namespace nnet
}  // namespace cxxnet

namespace mshadow {
namespace ps {
template<>
IModelUpdater<cxxnet::real_t> *CreateModelUpdater<cxxnet::real_t>(void) {
  return new cxxnet::nnet::CXXNetUpdater();
}
}  // namespace ps
}  // namespace mshadow

#if MSHADOW_DIST_PS
namespace PS {

App* CreateServerNode(const std::string& conf) {
  return new mshadow::ps::MShadowServerNode<cxxnet::real_t>(conf);
}

} // namespace PS
#endif
####$$$$ cxxnet-master\cxxnet-master\src\plugin/caffe_adapter-inl.hpp
#ifndef CXXNET_CAFFE_ADAPTER_INL_HPP
#define CXXNET_CAFFE_ADAPTER_INL_HPP
#pragma once
/*!
 * \file cxxnet_caffee_adapter-inl.hpp
 * \brief try to adapt caffe layers, this code comes as plugin of cxxnet, and by default not included in the code.
 * \author Tianqi Chen
 */
#include <climits>
#include "caffe/caffe.hpp"
#include "caffe/common.hpp"
#include "mshadow/tensor.h"
#include "mshadow/tensor_container.h"
#include <google/protobuf/text_format.h>

namespace cxxnet {
namespace layer {
using namespace caffe;
using namespace mshadow;
using namespace mshadow::expr;
using namespace mshadow::utils;
/*!
 * \brief adapter from caffe, will cost a extra blob memory,
 *        but allows some correct comparisons
 */
template<typename xpu>
class CaffeLayer: public ILayer<xpu>{
 public:
  CaffeLayer(){
    this->base_ = NULL;
    this->mode_ = -1;
    this->blb_in_ = NULL;
    this->blb_out_ = NULL;
  }
  virtual ~CaffeLayer(void) {
    this->FreeSpace();
    if (blb_in_ != NULL)  delete blb_in_;
    if (blb_out_ != NULL) delete blb_out_;
  }

  virtual void SetStream(mshadow::Stream<xpu> *stream) {
    this->stream_ = stream;
  }

  virtual void ApplyVisitor(typename ILayer<xpu>::IVisitor *pvisitor) {
    const std::vector<boost::shared_ptr<caffe::Blob<real_t> > > &blobs = base_->blobs();
    for(size_t i = 0; i < blobs.size(); ++ i) {
      // Assume that blobs do not change
      char tag[ 256 ];
      sprintf(tag, "blob%d", (int)i);
      index_t count = blobs[i]->count();
      if (xpu::kDevCPU) {
        mshadow::Tensor<xpu,1> weight(blobs[i]->mutable_cpu_data(), Shape1(count));
        mshadow::Tensor<xpu,1> grad(blobs[i]->mutable_cpu_diff(), Shape1(count));
        weight.set_stream(stream_);
        grad.set_stream(stream_);
        pvisitor->Visit(tag, weight, grad);
      }else{
        mshadow::Tensor<xpu,1> weight(blobs[i]->mutable_gpu_data(), Shape1(count));
        mshadow::Tensor<xpu,1> grad(blobs[i]->mutable_gpu_diff(), Shape1(count));
        weight.set_stream(stream_);
        grad.set_stream(stream_);
        pvisitor->Visit(tag, weight, grad);
      }
    }
  }
  virtual void Forward(bool is_train,
                       const std::vector<Node<xpu>*> &nodes_in,
                       const std::vector<Node<xpu>*> &nodes_out,
                       ConnectState<xpu> *p_cstate) {
    for (index_t i = 0; i < nodes_in.size(); ++i){
      mshadow::Shape<4> shape_in = nodes_in[i]->data.shape_;
      if (xpu::kDevCPU) {
        mshadow::Tensor<xpu,4> tbin(vec_in_[i]->mutable_cpu_data(), shape_in);
        tbin.set_stream(stream_);
        mshadow::Copy(tbin, nodes_in[i]->data, stream_);
      }else{
        mshadow::Tensor<xpu,4> tbin(blb_in_->mutable_gpu_data(), shape_in);
        tbin.set_stream(stream_);
        mshadow::Copy(tbin, nodes_in[i]->data, stream_);
      }
    }
    base_->Forward(vec_in_, &vec_out_);
    for (index_t i = 0; i < nodes_out.size(); ++i){
      mshadow::Shape<4> shape_ou = nodes_out[i]->data.shape_;
      if (xpu::kDevCPU) {
        mshadow::Tensor<xpu,4> tbout(vec_out_[i]->mutable_cpu_data(), shape_ou);
        tbout.set_stream(stream_);
        mshadow::Copy(nodes_out[i]->data, tbout, stream_);
      } else {
        mshadow::Tensor<xpu,4> tbout(vec_out_[i]->mutable_gpu_data(), shape_ou);
        tbout.set_stream(stream_);
        mshadow::Copy(nodes_out[i]->data, tbout, stream_);
      }
    }
  }
  virtual void Backprop(bool prop_grad,
                        const std::vector<Node<xpu>*> &nodes_in,
                        const std::vector<Node<xpu>*> &nodes_out,
                        ConnectState<xpu> *p_cstate) {
    for (index_t i = 0; i < nodes_out.size(); ++i){
      mshadow::Shape<4> shape_ou = nodes_out[i]->data.shape_;
      if (xpu::kDevCPU) {
        mshadow::Tensor<xpu,4> tbout(vec_out_[i]->mutable_cpu_diff(), shape_ou);
        tbout.set_stream(stream_);
        mshadow::Copy(tbout, nodes_out[i]->data, stream_);
      } else {
        mshadow::Tensor<xpu,4> tbout(vec_out_[i]->mutable_gpu_diff(), shape_ou);
        tbout.set_stream(stream_);
        mshadow::Copy(tbout, nodes_out[i]->data, stream_);
      }
    }

    base_->Backward(vec_out_, prop_grad, &vec_in_);
    if (prop_grad) {
      for (index_t i = 0; i < nodes_in.size(); ++i){
        mshadow::Shape<4> shape_in = nodes_in[i]->data.shape_;
        if (xpu::kDevCPU) {
          mshadow::Tensor<xpu,4> tbin(vec_in_[i]->mutable_cpu_diff(), shape_in);
          tbin.set_stream(stream_);
          mshadow::Copy(nodes_in[i]->data, tbin, stream_);
        } else {
          mshadow::Tensor<xpu,4> tbin(vec_in_[i]->mutable_gpu_diff(), shape_in);
          tbin.set_stream(stream_);
          mshadow::Copy(nodes_in[i]->data, tbin, stream_);
        }
      }  
    }
  }
  
  virtual void InitConnection(const std::vector<Node<xpu>*> &nodes_in,
                              const std::vector<Node<xpu>*> &nodes_out,
                              ConnectState<xpu> *p_cstate) {
    utils::Assert(mode_ != -1, "CaffeLayer: must specify mode: 0:flatten, 1:conv-channels");
    vec_in_.clear();
    Caffe::SetDevice(0);
    for (index_t i = 0; i < nodes_in.size(); ++i){
      mshadow::Shape<4> ishape = nodes_in[i]->data.shape_;
      if (mode_ == 0) {
        utils::Assert(ishape[0] == 1 && ishape[1] == 1, "the input is not flattened, forget a FlattenLayer?");
        batch_size_ = ishape[2];
        blb_in_  = new caffe::Blob<real_t>(ishape[2], ishape[3], 1, 1);
        blb_out_ = new caffe::Blob<real_t>();
      }else{
        batch_size_ = ishape[0];
        blb_in_  = new caffe::Blob<real_t>(ishape[0], ishape[1], ishape[2], ishape[3]);
        blb_out_ = new caffe::Blob<real_t>();
      }
      vec_in_.push_back(blb_in_);
    }

    vec_out_.clear();
    for (index_t i = 0; i < nodes_out.size(); ++i){
      blb_out_ = new caffe::Blob<real_t>();
      vec_out_.push_back(blb_out_);
    }    
    if (base_ == NULL) {
      base_ = caffe::GetLayer<real_t>(param_);
    }
    
    base_->SetUp(vec_in_, &vec_out_);
    utils::Assert(nodes_out.size() == vec_out_.size(), "CaffeLayer: Number of output inconsistent.");
    for (index_t i = 0; i < nodes_out.size(); ++i){
      if (mode_ == 0 || mode_ == 2) {
        nodes_out[i]->data.shape_ = mshadow::Shape4(1, 1, vec_out_[i]->num(), vec_out_[i]->channels());
      }else{
        nodes_out[i]->data.shape_ = mshadow::Shape4(vec_out_[i]->num(), vec_out_[i]->channels(),
          vec_out_[i]->height(), vec_out_[i]->width());
      }
    }
  }
  virtual void SetParam(const char *name, const char* val) {
    if (!strcmp(name, "proto")) {
      google::protobuf::TextFormat::ParseFromString(std::string(val), &param_);
    }
    if (!strcmp(name, "mode")) {
      mode_ = atoi(val);
    }
    if (!strcmp(name, "dev")) {
      if (!strcmp(val, "cpu")) caffe::Caffe::set_mode(caffe::Caffe::CPU);
      if (!strcmp(val, "gpu")) caffe::Caffe::set_mode(caffe::Caffe::GPU);
    }
  }
  virtual void InitModel(void) {
  }
  virtual void SaveModel(mshadow::utils::IStream &fo) const {
    std::vector<char> buf;
    caffe::LayerParameter lparam = base_->layer_param();
    base_->ToProto(&lparam);            
    int msize = lparam.ByteSize();
    buf.resize(msize);
    fo.Write(&msize, sizeof(int));
    utils::Assert(lparam.SerializeToArray(&buf[0], msize ), "CaffeLayer::SaveModel");
    fo.Write(&buf[0], msize);
  }
  virtual void LoadModel(mshadow::utils::IStream &fi) {
    int msize;
    std::vector<char> buf;
    fi.Read(&msize, sizeof(int));
    buf.resize(msize);
    utils::Assert(fi.Read(&buf[0], msize)!= 0, "CaffeLayer::LoadModel");
    param_.ParseFromArray(&buf[0], msize);
    this->FreeSpace();
    base_ = caffe::GetLayer<real_t>(param_);
  }
 private:
  inline void FreeSpace(void) {
    if (base_ != NULL) delete base_;
    base_ = NULL;
  }
 private:
  /*!\brief mini batch size*/
  int batch_size_;
  /*!\brief whether it is fullc or convolutional layer */
  int mode_;
  /*! \brief caffe's layer parametes */
  caffe::LayerParameter param_;
  /*! \brief caffe's impelementation */
  caffe::Layer<real_t>* base_;
  /*! \brief blob data */
  caffe::Blob<real_t>* blb_in_;
  caffe::Blob<real_t>* blb_out_;
  /*!\ brief stores blb in */
  std::vector< caffe::Blob<real_t>* > vec_in_;
  std::vector< caffe::Blob<real_t>* > vec_out_;
  /*!\ brief the stream_ used for the tensor*/
  mshadow::Stream<xpu> *stream_;
};
}  // namespace layer
}  // namespace cxxnet
#endif
####$$$$ cxxnet-master\cxxnet-master\src\updater/adam_updater-inl.hpp
#ifndef CXXNET_UPDATER_ADAM_UPDATER_INL_HPP_
#define CXXNET_UPDATER_ADAM_UPDATER_INL_HPP_
/*!
 * \file sgd_updater-inl.hpp
 * \brief implementation of SGD with momentum
 * \author Bing Xu
 */
#include <mshadow/tensor.h>
#include "./updater.h"
#include "./param.h"
#include "../layer/op.h"

namespace cxxnet {
namespace updater {
// Adam updater with momentum
template<typename xpu, int dim>
class AdamUpdater : public IUpdater<xpu> {
 public:
  AdamUpdater(mshadow::Tensor<xpu,dim> w, mshadow::Tensor<xpu,dim> dw, const char *tag)
      :w(w), dw(dw) {
    param.tag = tag;
    decay1 = 0.1f;
    decay2 = 0.001f;
  }
  virtual ~AdamUpdater(void) {}
  virtual void Init(void) {
    if (param.silent == 0) {
      printf("AdamUpdater: eta=%f, beta1=%f, beta2=%f\n", param.base_lr_, decay1, decay2);
    }
    m_w1.Resize(w.shape_, 0.0f);
    m_w2.Resize(w.shape_, 0.0f);
  }
  virtual void SetStream(mshadow::Stream<xpu> *stream) {
    w.set_stream(stream);
    dw.set_stream(stream);
    m_w1.set_stream(stream);
    m_w2.set_stream(stream);
  }
  virtual void Update(long epoch) {
    this->ApplyUpdate(epoch, dw);
    // dw accumulate gradient instead of storing them
    // updater need to reset then to 0 after each update
    dw = 0.0f;
  }
  virtual void Update(long epoch, mshadow::Tensor<xpu, 2> grad) {
    utils::Assert(grad.shape_ == w.shape_.FlatTo2D(),
                  "SGDUpdater: grad must be generated from source of same shape");
    this->ApplyUpdate(epoch, mshadow::Tensor<xpu, dim>
                      (grad.dptr_, w.shape_, grad.stride_, w.stream_));
  }
  virtual void StartRound(int round) {
    param.round = round;
  }
  virtual void SetParam(const char *name, const char *val) {
    param.SetParam(name, val);
    if (!strcmp(name, "beta1")) decay1 = atof(val);
    if (!strcmp(name, "beta2")) decay2 = atof(val);
  }
  virtual void ApplyVisitor(typename IUpdater<xpu>::IVisitor *pvisitor) {
    pvisitor->Visit(param.tag.c_str(), w, dw);
  }

 protected:
  UpdaterParam param;
  // variales
  mshadow::Tensor<xpu,dim> w, dw;
  // momentum variable
  mshadow::TensorContainer<xpu,dim> m_w1;
  mshadow::TensorContainer<xpu,dim> m_w2;
  float decay1;
  float decay2;
  // update function
  virtual void ApplyUpdate(long epoch,
                           mshadow::Tensor<xpu, dim> grad) {
    if (param.wd > 0.0f) grad -= param.wd * w;
    float fix1 = 1.0f - powf(1.0f - decay1, epoch + 1);
    float fix2 = 1.0f - powf(1.0f - decay2, epoch + 1);
    float lr_t = param.base_lr_ * sqrt(fix2) / fix1;
    m_w1 += decay1 * (grad - m_w1);
    m_w2 += decay2 * (mshadow::expr::F<op::square>(grad) - m_w2);
    w -= lr_t * (m_w1 / (mshadow::expr::F<op::square_root>(m_w2) + 1e-8f));
  }
};  // class AdamUpdater
}  // namespace updater
}  // namespace cxxnet
#endif

####$$$$ cxxnet-master\cxxnet-master\src\updater/async_updater-inl.hpp
#ifndef CXXNET_UPDATER_ASYNC_UPDATER_INL_HPP_
#define CXXNET_UPDATER_ASYNC_UPDATER_INL_HPP_
/*!
 * \file sgd_updater-inl.hpp
 * \brief implementation of asynchronize updater using SGD
 * \author Tianqi Chen
 */
#include <mshadow/tensor.h>
#include <mshadow-ps/ps.h>
#include "./updater.h"
#include "../utils/timer.h"
namespace cxxnet {
namespace updater {
template<typename xpu>
class AsyncUpdater: public IAsyncUpdater<xpu> {
 public:
  AsyncUpdater(int data_key, int devid, int priority,
               mshadow::Tensor<xpu, 2> w, mshadow::Tensor<xpu, 2> dw,
               layer::LayerType layer_type, const char *tag,
               mshadow::ps::ISharedModel<xpu, real_t> *pserver,
               IUpdater<xpu> *updater)
      : data_key(data_key), devid(devid),
        priority(priority), w(w), dw(dw),
        layer_type(layer_type), tag(tag),
        pserver(pserver), updater(updater), tnode(false) {
    fullc_gather = 0;
    local_batch_size = 0;
    total_batch_size = 0;
    pull_at_backprop = 1;
    update_on_server = 0;
    init_on_worker = 0;
    test_on_server = 0;
    bigarray_bound = 1000 * 1000;
    pull_not_issued = false;
  }
  virtual ~AsyncUpdater(void) {
    delete updater;
  }
  virtual void Init(void) {
    if (update_on_server == 0) {
      updater->Init();
    }
    if (pserver != NULL) {
      if (fullc_gather != 0) {
        char name[32];
        sprintf(name, "push_op[%d]", data_key);
        pserver->SetParam(name, "gather");
      }
      pserver->InitKey(dw.shape_, data_key, devid);
      if (test_on_server != 0|| init_on_worker != 0) {
        pserver->SetWeight_(w.FlatTo2D(), data_key, devid);
      }
      // pull back weight directly if update on server
      if (update_on_server != 0) {
        pserver->PullReq(w, data_key, devid, priority,
                         CleanGrad_, this);
      }
    } else {
      utils::Check(update_on_server == 0 && test_on_server == 0,
                   "parameter server must not be empty");
    }
  }
  virtual void SetStream(mshadow::Stream<xpu> *stream) {
    if (updater != NULL) updater->SetStream(stream);
    tnode.set_stream(stream);
  }
  virtual void BeforeBackprop(const std::vector<layer::Node<xpu>*> &nodes_in,
                              const std::vector<layer::Node<xpu>*> &nodes_out) {
    if (fullc_gather != 0) {
      utils::Check(update_on_server == 0, "GatherUpdate can not use update_on_server");
      utils::Check(nodes_in.size() == 1, "fullc_gather can only work with fullc");
      utils::Check(nodes_out.size() == 1, "fullc_gather can only work with fullc");
      mshadow::Tensor<xpu, 2> in = nodes_in[0]->mat();
      mshadow::Tensor<xpu, 2> out = nodes_out[0]->mat();
      num_in = in.size(1); num_out = out.size(1);
      tnode.Resize(mshadow::Shape2(total_batch_size, num_in + num_out));      
      // manually hslice
      mshadow::Tensor<xpu, 2> tin(tnode.dptr_,
                                  mshadow::Shape2(total_batch_size, num_in),
                                  tnode.stride_, tnode.stream_);
      mshadow::Tensor<xpu, 2> tout(tnode.dptr_ + num_in,
                                   mshadow::Shape2(total_batch_size, num_out),
                                   tnode.stride_, tnode.stream_);
      local_batch_size = in.size(0);
      utils::Check(local_batch_size <= total_batch_size,
                   "local_batch_size bigger than total_batch_size");
      utils::Check(total_batch_size % local_batch_size == 0,
                   "when you use fullc_gather mode, the batch_size "\
                   "must be multiple of number of devices");
      mshadow::Copy(tin.Slice(0, local_batch_size), in, tnode.stream_);
      mshadow::Copy(tout.Slice(0, local_batch_size), out, tnode.stream_);      
    }
  }
  virtual void AfterBackprop(bool do_update, long epoch) {
    if (fullc_gather == 0) {
      if (do_update && pserver == NULL) {
        updater->Update(epoch); return;
      }
      if (do_update) {
        this->update_epoch = epoch;
        pserver->Push(dw, data_key, devid, priority);
        if (update_on_server == 0) {
          if (pull_at_backprop != 0) {          
            pserver->PullReq(dw, data_key, devid, priority,
                             ApplyUpdate_, this);
          } else {
            pull_not_issued = true;
          }
        } else {
          // pull weight directly from server
          pserver->PullReq(w, data_key, devid, priority,
                           CleanGrad_, this);
        }
      }
    } else {
      utils::Check(update_on_server == 0, "GatherUpdate can not use update_on_server");
      this->do_update = do_update;
      this->update_epoch = epoch;
      if (do_update && pserver == NULL) {
        this->CalcDelta(dw.stream_);
        updater->Update(epoch); return;
      }
      pserver->Push(tnode.Slice(0, local_batch_size), data_key, devid, priority);
      pserver->PullReq(tnode, data_key, devid, priority,
                       ApplyGatherUpdate_, this);
    }
  }
  virtual void BeforeForward(void) {
    if (pull_not_issued) {
      if (update_on_server == 0) { 
        pserver->PullReq(dw, data_key, devid, priority,
                         ApplyUpdate_, this);
      } else {
        pserver->PullReq(w, data_key, devid, priority,
                         CleanGrad_, this);
      }
      pull_not_issued = false;
    }
  }
  virtual void UpdateWait(void) {
    if (pserver == NULL) return;
    pserver->PullWait(data_key, devid);
  }
  virtual void StartRound(int round) {
    if (updater != NULL) {
      updater->StartRound(round);
    }
    if (test_on_server != 0) {
      utils::Assert(update_on_server == 0,
                    "test_on_server must set update_on_server = 0");
      pserver->PullWait(data_key, devid);
      pserver->CheckWeight_(w.FlatTo2D(), data_key, devid);
    }
  }
  virtual void SetParam(const char *name, const char *val) {
    if (updater != NULL) updater->SetParam(name, val);
    if (!strcmp(name, "fullc_gather")) {
      if (tag == "wmat" && layer_type == layer::kFullConnect) {
        fullc_gather = atoi(val);
      }
    }
    if (!strcmp(name, "batch_size")) {
      total_batch_size = static_cast<index_t>(atoi(val));
    }
    if (!strcmp(name, "pull_at_backprop")) {
      if (!strcmp(val, "auto")) {
        pull_at_backprop = w.MSize() < bigarray_bound;
      } else {
        pull_at_backprop = atoi(val);
      }
    }
    if (!strcmp(name, "bigarray_bound")) {
      bigarray_bound = static_cast<size_t>(atol(val));
    }
    if (!strcmp(name, "update_on_server")) {
      update_on_server = atoi(val);
    }
    if (!strcmp(name, "test_on_server")) {
      test_on_server = atoi(val);
    }
    if (!strcmp(name, "init_on_worker")) {
      init_on_worker = atoi(val);
    }
  }
  virtual void ApplyVisitor(typename IUpdater<xpu>::IVisitor *pvisitor) {
    updater->ApplyVisitor(pvisitor);
  }

 protected:
  inline void CalcDelta(mshadow::Stream<xpu> *stream) {
    dw.set_stream(stream);
    mshadow::Tensor<xpu, 2> tin(tnode.dptr_,
                                mshadow::Shape2(total_batch_size, num_in),
                                tnode.stride_, stream);
    mshadow::Tensor<xpu, 2> tout(tnode.dptr_ + num_in,
                                 mshadow::Shape2(total_batch_size, num_out),
                                 tnode.stride_, stream);
    dw += dot(tout.T(), tin);
  }
  inline static void CleanGrad_(mshadow::Stream<xpu> *stream, void *arg) {
    AsyncUpdater<xpu> *up = static_cast<AsyncUpdater<xpu>*>(arg);    
    utils::Assert(up->update_on_server !=0, "update_on_server consistency");
    up->dw.set_stream(stream);
    up->dw = 0.0f;
  }
  inline static void ApplyUpdate_(mshadow::Stream<xpu> *stream, void *arg) {
    AsyncUpdater<xpu> *up = static_cast<AsyncUpdater<xpu>*>(arg);
    if (up->update_on_server == 0) {
      up->updater->SetStream(stream);
      up->updater->Update(up->update_epoch);
    }
  }  
  inline static void ApplyGatherUpdate_(mshadow::Stream<xpu> *stream, void *arg) {    
    AsyncUpdater<xpu> *up = static_cast<AsyncUpdater<xpu>*>(arg);
    utils::Check(up->update_on_server == 0, "GatherUpdate can not use update_on_server");
    up->CalcDelta(stream);
    if (up->do_update) {
      up->updater->SetStream(stream);
      up->updater->Update(up->update_epoch);
    }
  }
  int data_key, devid, priority;
  long update_epoch;
  mshadow::Tensor<xpu, 2> w, dw;
  layer::LayerType layer_type;
  std::string tag;
  mshadow::ps::ISharedModel<xpu, real_t> *pserver;
  IUpdater<xpu> *updater;
  // whether issue pull request at backprop
  int pull_at_backprop;
  // whether there is un-issued pullreq
  bool pull_not_issued;
  // big array bound
  size_t bigarray_bound;
  // perform update on server side
  int update_on_server;
  // perform test on server side
  int test_on_server;
  // perform init on slave side
  int init_on_worker;
  // the following data structure are used to support fullc_gather
  // use gather update for fullc layer
  int fullc_gather;
  // whether do update this round
  bool do_update;
  // number of input and output nodex
  index_t num_in, num_out;
  // the total batch_size across nodes
  index_t local_batch_size, total_batch_size;
  // temporal result 
  mshadow::TensorContainer<xpu, 2> tnode; 
};
}  // updater
}  // cxxnet
#endif
####$$$$ cxxnet-master\cxxnet-master\src\updater/nag_updater-inl.hpp
#ifndef CXXNET_UPDATER_NAG_UPDATER_INL_HPP_
#define CXXNET_UPDATER_NAG_UPDATER_INL_HPP_
/*!
 * \file nag_updater-inl.hpp
 * \brief implementation of NAG with momentum
 * \author Winsty
 */
#include <mshadow/tensor.h>
 #include "./updater.h"
#include "./param.h"

namespace cxxnet {
namespace updater {
// SGD updater with momentum
template<typename xpu, int dim>
class NAGUpdater : public IUpdater<xpu> {
 public:
  NAGUpdater(mshadow::Tensor<xpu,dim> w, mshadow::Tensor<xpu,dim> dw, const char *tag)
      :w(w), dw(dw) {
    param.tag = tag;
  }
  virtual ~NAGUpdater(void) {}
  virtual void Init(void) {
    if (param.silent == 0) {
      printf("NAGUpdater: eta=%f, mom=%f\n", param.base_lr_, param.momentum);
    }
    m_w.Resize(w.shape_, 0.0f);
    old_m_w.Resize(w.shape_, 0.0f);
  }
  virtual void SetStream(mshadow::Stream<xpu> *stream) {
    w.set_stream(stream);
    dw.set_stream(stream);
    m_w.set_stream(stream);
    old_m_w.set_stream(stream);
  }
  virtual void Update(long epoch) {
    this->ApplyUpdate(epoch, dw);
    // dw accumulate gradient instead of storing them
    // updater need to reset then to 0 after each update
    dw = 0.0f;
  }
  virtual void Update(long epoch, mshadow::Tensor<xpu, 2> grad) {
    utils::Assert(grad.shape_ == w.shape_.FlatTo2D(),
                  "SGDUpdater: grad must be generated from source of same shape");
    this->ApplyUpdate(epoch, mshadow::Tensor<xpu, dim>
                      (grad.dptr_, w.shape_, grad.stride_, w.stream_));
  }
  virtual void StartRound(int round) {
    param.round = round;
  }
  virtual void SetParam(const char *name, const char *val) {
    param.SetParam(name, val);
  }
  virtual void ApplyVisitor(typename IUpdater<xpu>::IVisitor *pvisitor) {
    pvisitor->Visit(param.tag.c_str(), w, dw);
  }

 protected:
  UpdaterParam param;
  // variales
  mshadow::Tensor<xpu,dim> w, dw;
  // momentum variable
  mshadow::TensorContainer<xpu,dim> m_w, old_m_w;
  
  inline void ApplyUpdate(long epoch,
                          mshadow::Tensor<xpu, dim> grad) {
    param.ScheduleEpoch(epoch);
    mshadow::Copy(old_m_w, m_w, old_m_w.stream_);
    m_w *= param.momentum;
    m_w += (-param.learning_rate) * (grad + param.wd * w);
    w += (1 + param.momentum) * m_w - param.momentum * old_m_w;
  }
};  // class SGDUpdater
}  // namespace updater
}  // namespace cxxnet
#endif

####$$$$ cxxnet-master\cxxnet-master\src\updater/param.h
#ifndef CXXNET_UPDATER_PARAM_H_
#define CXXNET_UPDATER_PARAM_H_
/*!
 * \file param.h
 * \brief common parameters for updater behavior, supports complex learning rate scheduling
 * \author Tianqi Chen
 */
#include <string>

namespace cxxnet {
namespace updater {
/*! \brief potential parameters for each layer */
struct UpdaterParam {
  /*! \brief tag of current parameter group */
  std::string tag;
  /*! \brief current round */
  int round;
  /*! \brief whether can print messages */
  int silent;
  /*! \brief learning rate */
  float learning_rate;
  /*! \brief weight decay */
  float wd;
  /*! \brief momentum */
  float momentum;
  // scheduling parameters
  /*! \brief type of learning rate schedule */
  int lr_schedule;
  /*! \brief type of momentum schedule */
  int momentum_schedule;
  /*! \brief base learning rate */
  float base_lr_;
  /*! \brief period of lr decay */
  long lr_step;
  /*! \brief decay parameter gamma */
  float lr_gamma;
  /*! \brief decay parameter gamma */
  float lr_alpha;
  /*! \brief decay parameter factor */
  float lr_factor;
  /*! \brief minimum learning rate */
  float lr_minimum;
  /*! \brief start scheduling epoch */
  long start_epoch;
  /*! \brief base momentum */
  float base_momentum_;
  /*! \brief final momentum */
  float final_momentum_;
  /*! \brief saturation momentum epoch */
  long saturation_epoch_;
  /*!
   * \brief clip gradient to this value if it is too large,
   *  do nothing if it is set to 0
   */
  float clip_gradient;
  
  /*! \brief constructor that sets default parameters */
  UpdaterParam(void) {
    base_lr_ = 0.01f;
    base_momentum_ = 0.5f;
    final_momentum_ = 0.90f;
    momentum_schedule = 0;
    lr_schedule = 0;
    lr_step = 1;
    lr_alpha = 0.5f;
    lr_gamma = 0.5f;
    lr_factor = 0.1f;
    lr_minimum = 0.00001;
    start_epoch = 0;
    wd = 0.0f;
    momentum = 0.9f;
    silent = 0;
    clip_gradient = 0.0f;
  }
  /*! \brief do learning rate or other parameter schedule at round epoch */
  inline void ScheduleEpoch(long epoch) {
    switch (lr_schedule) {
      case 0: learning_rate = base_lr_; break;
      case 1: learning_rate = base_lr_ * powf(lr_gamma, float(epoch) / lr_step); break;
      case 2: learning_rate = base_lr_ * powf(1.0f + (epoch/lr_step) * lr_gamma, -lr_alpha); break;
      case 3: learning_rate = base_lr_ * powf(lr_factor, epoch / lr_step); break;
      default: utils::Error("unknown schedule type");
    }
    if (momentum_schedule && saturation_epoch_) {
      momentum += (final_momentum_ - base_momentum_) / saturation_epoch_ * epoch + base_momentum_;
    }
    momentum = momentum < final_momentum_ ? momentum : final_momentum_;
    learning_rate = learning_rate < lr_minimum ? lr_minimum : learning_rate;
    if (epoch < start_epoch) {
      learning_rate = base_lr_;
      return;
    }

  }
  /*!
   * \brief Set param for the layer from string
   * \param name parameter name
   * \param val string for configuration
   */
  inline void SetParam(const char *name, const char* val) {
    // if we set "bias:wd = 0.0", and tag == "bias", the it will set wd in current updater param
    // but will not affect settings with other tags
    if (!strncmp(name, tag.c_str(), tag.length())) {
      if (name[tag.length()] == ':') name += tag.length() + 1;
    }
    if (!strcmp(name, "lr")) base_lr_ = (float)atof(val);
    if (!strcmp(name, "eta")) base_lr_ = (float)atof(val);
    if (!strcmp(name, "wd")) wd = (float)atof(val);
    if (!strcmp(name, "momentum")) momentum = (float)atof(val);
    if (!strcmp(name, "silent")) silent = atoi(val);
    if (!strcmp(name, "momentum_schedule")) momentum_schedule = atoi(val);
    if (!strcmp(name, "clip_gradient")) clip_gradient = (float)atof(val);
    if (!strcmp(name, "final_momentum")) final_momentum_ = atof(val);
    if (!strcmp(name, "base_momentum")) base_momentum_ = atof(val);
    if (!strcmp(name, "saturation_epoch")) saturation_epoch_ = atol(val);
    if (!strncmp(name, "lr:", 3) || !strncmp(name, "eta:",4)) {
      if (!strncmp(name, "lr:", 3)) name += 3;
      else name += 4;
      if (!strcmp(name, "schedule")) {
        if (!strcmp(val, "constant"))  lr_schedule = 0;
        if (!strcmp(val, "expdecay"))  lr_schedule = 1;
        if (!strcmp(val, "polydecay")) lr_schedule = 2;
        if (!strcmp(val, "factor"))     lr_schedule = 3;
      }
      if (!strcmp(name, "gamma")) lr_gamma = (float)atof(val);
      if (!strcmp(name, "alpha")) lr_alpha = (float)atof(val);
      if (!strcmp(name, "step"))  lr_step = atol(val);
      if (!strcmp(name, "factor")) lr_factor = (float)atof(val);
      if (!strcmp(name, "minimum_lr")) lr_minimum = (float)atof(val);
      if (!strcmp(name, "start_epoch")) start_epoch = atol(val);
    }
  }
};
}  // namespace updater
}  // namespace cxxnet
#endif  // CXXNET_UPDATER_PARAM_H_
####$$$$ cxxnet-master\cxxnet-master\src\updater/sgd_updater-inl.hpp
#ifndef CXXNET_UPDATER_SGD_UPDATER_INL_HPP_
#define CXXNET_UPDATER_SGD_UPDATER_INL_HPP_
/*!
 * \file sgd_updater-inl.hpp
 * \brief implementation of SGD with momentum
 * \author Tianqi Chen
 */
#include <mshadow/tensor.h>
 #include "./updater.h"
#include "./param.h"

namespace cxxnet {
namespace updater {
/*! \brief used for gradient clipping and nan detection */
struct clip {
  MSHADOW_XINLINE static real_t Map(real_t a, real_t b) {
    if (isnan(a)) return 0.0f;
    if (a < -b) return -b;
    if (a > b) return b;
    return a;
  }
};

// SGD updater with momentum
template<typename xpu, int dim>
class SGDUpdater : public IUpdater<xpu> {
 public:
  SGDUpdater(mshadow::Tensor<xpu,dim> w, mshadow::Tensor<xpu,dim> dw, const char *tag)
      :w(w), dw(dw) {
    param.tag = tag;
  }
  virtual ~SGDUpdater(void) {}
  virtual void Init(void) {
    if (param.silent == 0) {
      printf("SGDUpdater: eta=%f, mom=%f\n", param.base_lr_, param.momentum);
    }
    m_w.Resize(w.shape_, 0.0f);
  }
  virtual void SetStream(mshadow::Stream<xpu> *stream) {
    w.set_stream(stream);
    dw.set_stream(stream);
    m_w.set_stream(stream);
  }
  virtual void Update(long epoch) {
    this->ApplyUpdate(epoch, dw);
    // dw accumulate gradient instead of storing them
    // updater need to reset then to 0 after each update
    dw = 0.0f;
  }
  virtual void Update(long epoch, mshadow::Tensor<xpu, 2> grad) {
    utils::Assert(grad.shape_ == w.shape_.FlatTo2D(),
                  "SGDUpdater: grad must be generated from source of same shape");
    this->ApplyUpdate(epoch, mshadow::Tensor<xpu, dim>
                      (grad.dptr_, w.shape_, grad.stride_, w.stream_));
  }
  virtual void StartRound(int round) {
    param.round = round;
  }
  virtual void SetParam(const char *name, const char *val) {
    param.SetParam(name, val);
  }
  virtual void ApplyVisitor(typename IUpdater<xpu>::IVisitor *pvisitor) {
    pvisitor->Visit(param.tag.c_str(), w, dw);
  }

 protected:
  UpdaterParam param;
  // variales
  mshadow::Tensor<xpu,dim> w, dw;
  // momentum variable
  mshadow::TensorContainer<xpu,dim> m_w;
  // update function
  virtual void ApplyUpdate(long epoch,
                           mshadow::Tensor<xpu, dim> grad) {
    using namespace mshadow::expr;
    param.ScheduleEpoch(epoch);
    m_w *= param.momentum;
    if (param.clip_gradient != 0.0f) {
      m_w += (-param.learning_rate) * (F<clip>(grad, param.clip_gradient) + param.wd * w);
    } else {
      m_w += (-param.learning_rate) * (grad + param.wd * w);
    }
    w += m_w;
  }
};  // class SGDUpdater
}  // namespace updater
}  // namespace cxxnet
#endif

####$$$$ cxxnet-master\cxxnet-master\src\updater/updater.h
#ifndef CXXNET_UPDATER_UPDATER_H_
#define CXXNET_UPDATER_UPDATER_H_

#include <vector>
#include <mshadow/tensor.h>
#include <mshadow-ps/ps.h>
#include "../global.h"
#include "../layer/layer.h"

namespace cxxnet {
/*! \brief namespace of updating algorithms */
namespace updater {
/*!
 * \brief interface of parameter updater,
 *        it defines the updating behavior of parameters
 *        ILayer takes no charge of parameter update,
 *        IUpdater takes the gradient value accumulated by ILayer and the weight
 *        to perform update on the weight
 * \tparam xpu which device the data of the updater lies 
 */
template<typename xpu>
class IUpdater { 
 public:
  /*! \brief reuse layer's visitor type, can be used to access weight in updater */
  typedef typename layer::ILayer<xpu>::IVisitor IVisitor;
  /*!\brief virtual destructor */
  virtual ~IUpdater(void) {}
  /*!
   * \brief set the stream of internal computation to be stream
   * \param stream the stream to be used
   */
  virtual void SetStream(mshadow::Stream<xpu> *stream) = 0;
  /*! \brief intialize, print information about updater if not silent */
  virtual void Init(void) = 0;
  /*! 
   * \brief apply visitor to the updater,
   *   this is used to visit tha content of the updater
   */
  virtual void ApplyVisitor(IVisitor *pvisitor) = 0;
  /*!
   * \brief inform the updater that we are starting
   *        new round of iteration over data
   * \param round round counter
   */
  virtual void StartRound(int round) = 0;
  /*!
   * \brief update parameter
   * \param epoch what current epoch is.
   *        epoch is number of mini-batches passed, 
   *        while round is one pass over training data
   */
  virtual void Update(long epoch) = 0;
  /*!
   * \brief update the parameter, provides the
   *        gradient value from outside 
   * \param epoch what current epoch is
   *        epoch is number of mini-batches passed, 
   *        while round is one pass over training data
   * \param grad the pointer to pass in gradient value
   *        to minimize the interface, FlatTo2D should
   *        be called before passing in the gradient value
   */
  virtual void Update(long epoch, mshadow::Tensor<xpu, 2> grad) = 0;
  /*!\ brief set parameters that could be spefic to this updater */
  virtual void SetParam(const char *name, const char *val) = 0;
};

/*!
 * \brief asynchronize updater,
 * BeforeBackprop and AfterBackprop are asynchronize functions calls
 * and user need to call UpdateWait to wait the update to finish
 */
template<typename xpu>
class IAsyncUpdater : public IUpdater<xpu> {
 public:
  /*!
   * \brief this function is called before calling backprop
   * used by updater in case updater want to recover gradient by itself,
   * instead of calculated by the ILayer
   */
  virtual void BeforeBackprop(const std::vector<layer::Node<xpu>*> &nodes_in,
                              const std::vector<layer::Node<xpu>*> &nodes_out) = 0;
  /*!
   * \brief this function is called after calling backprop
   * \param do_update whether an update is performed in this iteration
   * \param epoch the update epoch if doing update
   */  
  virtual void AfterBackprop(bool do_update, long epoch) = 0;
  /*!
   * \brief this function will be called before
   * the forwardprop of all layers being calls
   */
  virtual void BeforeForward(void) = 0;
  /*!
   * \brief block until update is finished
   * if there were no update or update was already finished
   * this function will directly return
   */
  virtual void UpdateWait(void) = 0;
  // disable update function
  virtual void Update(long epoch) {
    utils::Error("IAsyncUpdater.Update call AfterBackprop instead");
  }
  // disable update function
  virtual void Update(long epoch, mshadow::Tensor<xpu, 2> grad) {
    utils::Error("IAsyncUpdater.Update call AfterBackprop instead");
  }
};
/*!
 * \brief factory: create an upadater algorithm of given type
 * \param type the type of updater
 * \param p_rnd the random number generator
 * \param weight the weight to be updated, Flattened to 2D
 * \param wgrad the tensor to hold the gradient value
 * \param tag the tag of the weight type
 */
template<typename xpu>
IUpdater<xpu>* CreateUpdater(const char *type,
                             mshadow::Random<xpu> *p_rnd,
                             mshadow::Tensor<xpu, 2> weight,
                             mshadow::Tensor<xpu, 2> wgrad,
                             const char *tag);
/*!
 * \brief factory: create updaters for a given layer, push_back them to out_updaters
 * \param layer_index layer index
 * \param device_id the device id where the async updater lies
 * \param param_server parameter server that could be used by async updater
 * \param type indicate the type of updater
 * \param p_rnd pointer to random number generator
 * \param layer_type the type of the layer
 * \param p_layer pointer to the layer object, where the data is going to be pulled from
 * \param out_updaters vector to hold outputs, if there is already elements in out_updaters, 
 *                     the function is going to push new updaters to the back of the vector
 */
template<typename xpu>
void CreateAsyncUpdaters(int layer_index,
                         int device_id,
                         mshadow::ps::ISharedModel<xpu, real_t> *param_server,
                         const char *type,
                         mshadow::Random<xpu> *p_rnd,
                         layer::LayerType layer_type,
                         layer::ILayer<xpu> *p_layer,
                         std::vector<IAsyncUpdater<xpu>*> *out_updaters);
/*!
 * \brief constant used to encode key index of parameter server
 *   data_key = layer_index * kDataKeyStep
 *   key(layer[i].bias) == i * kDataKeyStep + 1
 *   key(layer[i].bias) == i * kDataKeyStep + 1
 */
static const int kDataKeyStep = 4;
/*!
 * \brief encode layer index and weight tag into the unique key 
 * \param layer_index index of layer
 * \param tag the tag of weight type
 */
inline int EncodeDataKey(int layer_index, const char *tag) {
  if (!strcmp(tag, "bias")) return layer_index * kDataKeyStep + 1; 
  if (!strcmp(tag, "wmat")) return layer_index * kDataKeyStep + 0;
  utils::Error("EncodeDataKey: only support weight tag: wmat or bias");
  return 0;
}
/*!
 * \brief decode tag name from key
 * \param key the data key
 * \return tag name
 */
inline const char *DecodeTag(int key) {
  switch (key % updater::kDataKeyStep) {
    case 0: return "wmat";
    case 1: return "bias";
    default: utils::Error("invalid key"); return "";
  }
}
}  // namespace updater
}  // namespace cxxnet
#endif  // UPDATER_UPDATER_H_
####$$$$ cxxnet-master\cxxnet-master\src\updater/updater_impl-inl.hpp
#ifndef CXXNET_UPDATER_UPDATER_IMPL_INL_HPP_
#define CXXNET_UPDATER_UPDATER_IMPL_INL_HPP_
/*!
 * \file updater_impl-inl.hpp
 * \brief this file compiles all implementations of updaters together
 * \author Tianqi Chen
 */
#include "./sgd_updater-inl.hpp"
#include "./async_updater-inl.hpp"
#include "./nag_updater-inl.hpp"
#include "./adam_updater-inl.hpp"
namespace cxxnet {
namespace updater {
/*!
 * \brief factory: create an upadater algorithm of given type
 */
template<typename xpu, int dim>
inline IUpdater<xpu>* CreateUpdater_(const char *type,
                                     mshadow::Random<xpu> *p_rnd,
                                     mshadow::Tensor<xpu,dim> weight,
                                     mshadow::Tensor<xpu,dim> wgrad,
                                     const char *tag) {
  if(!strcmp(type, "sgd")) return new SGDUpdater<xpu,dim>(weight, wgrad, tag);
  if(!strcmp(type, "nag")) return new NAGUpdater<xpu, dim>(weight, wgrad, tag);
  if(!strcmp(type, "adam")) return new AdamUpdater<xpu, dim>(weight, wgrad, tag);
  utils::Error("unknown updater type %s", type);
  return NULL;
}

template<typename xpu, int dim>
inline IAsyncUpdater<xpu>*
CreateAsyncUpdater_(int layer_index,
                    int devid,
                    int priority,
                    mshadow::ps::ISharedModel<xpu, real_t> *pserver,
                    const char *type,
                    mshadow::Random<xpu> *p_rnd,
                    layer::LayerType layer_type,
                    mshadow::Tensor<xpu,dim> weight,
                    mshadow::Tensor<xpu,dim> wgrad,
                    const char *tag) {
  return new AsyncUpdater<xpu>(EncodeDataKey(layer_index, tag),
                               devid, priority,
                               weight.FlatTo2D(), wgrad.FlatTo2D(), layer_type, tag,
                               pserver, CreateUpdater_(type, p_rnd, weight, wgrad, tag));
}

template<typename xpu>
struct CreateAsyncUpdaterVisitor : public IUpdater<xpu>::IVisitor {
  // layerid
  int layerid;
  // device id
  int devid;
  // parameter server
  mshadow::ps::ISharedModel<xpu, real_t> *pserver;
  // type of updater
  const char *type;
  // random number generator
  mshadow::Random<xpu> *p_rnd;
  // layer type;
  layer::LayerType layer_type;
  // output updaters
  std::vector<IAsyncUpdater<xpu>*> *out_updaters;
  // constructor
  CreateAsyncUpdaterVisitor
  (int layerid,
   int devid,
   mshadow::ps::ISharedModel<xpu, real_t> *pserver,
   const char *type,
   mshadow::Random<xpu> *p_rnd,
   layer::LayerType layer_type,
   std::vector<IAsyncUpdater<xpu>*> *out_updaters)
      : layerid(layerid),
        devid(devid),
        pserver(pserver),
        type(type), p_rnd(p_rnd),
        layer_type(layer_type),
        out_updaters(out_updaters) {}
  virtual void Visit(const char *field_name,
                     mshadow::Tensor<xpu,1> weight,
                     mshadow::Tensor<xpu,1> grad) {
    out_updaters->push_back(CreateAsyncUpdater_(layerid, devid, -layerid, pserver,
                                                type, p_rnd, layer_type,
                                                weight, grad, field_name));
  }
  virtual void Visit(const char *field_name,
                     mshadow::Tensor<xpu,2> weight,
                     mshadow::Tensor<xpu,2> grad) {
    out_updaters->push_back(CreateAsyncUpdater_(layerid, devid, -layerid, pserver,
                                                type, p_rnd, layer_type,
                                                weight, grad, field_name));
  }
  virtual void Visit(const char *field_name,
                     mshadow::Tensor<xpu,3> weight,
                     mshadow::Tensor<xpu,3> grad) {
    out_updaters->push_back(CreateAsyncUpdater_(layerid, devid, -layerid, pserver,
                                                type, p_rnd, layer_type,
                                                weight, grad, field_name));
  }
  virtual void Visit(const char *field_name,
                     mshadow::Tensor<xpu,4> weight,
                     mshadow::Tensor<xpu,4> grad) {
    out_updaters->push_back(CreateAsyncUpdater_(layerid, devid, -layerid, pserver,
                                                type, p_rnd, layer_type,
                                                weight, grad, field_name));
  }

};

}  // namespace updater
}  // namespace cxxnet
#endif // CXXNET_UPDATER_INL_HPP
####$$$$ cxxnet-master\cxxnet-master\src\updater/updater_impl.cpp
#define _CRT_SECURE_NO_WARNINGS
#define _CRT_SECURE_NO_DEPRECATE
// this is where the actual implementations are
#include "updater_impl-inl.hpp"
// specialize the cpu implementation
namespace cxxnet {
namespace updater {
template<>
IUpdater<cpu>* CreateUpdater<>(const char *type,
                               mshadow::Random<cpu> *p_rnd,
                               mshadow::Tensor<cpu, 2> weight,
                               mshadow::Tensor<cpu, 2> wgrad,
                               const char *tag) {
  return CreateUpdater_(type, p_rnd, weight, wgrad, tag);
}
template<>
void CreateAsyncUpdaters<cpu>(int layer_index,
                              int device_id,
                              mshadow::ps::ISharedModel<cpu, real_t> *param_server,
                              const char *type,
                              mshadow::Random<cpu> *p_rnd,
                              layer::LayerType layer_type,
                              layer::ILayer<cpu> *p_layer,
                              std::vector<IAsyncUpdater<cpu>*> *out_updaters) {
  CreateAsyncUpdaterVisitor<cpu> visitor(layer_index, device_id, param_server,
                                         type, p_rnd, layer_type, out_updaters);  
  p_layer->ApplyVisitor(&visitor);  
}
}  // namespace updater
}  // namespace cxxnet
####$$$$ cxxnet-master\cxxnet-master\src\updater/updater_impl.cu
#define _CRT_SECURE_NO_WARNINGS
#define _CRT_SECURE_NO_DEPRECATE
// include the layer, this is where the actual implementations are

#include "updater_impl-inl.hpp"
// specialize the gpu implementation
namespace cxxnet {
namespace updater {
template<>
IUpdater<gpu>* CreateUpdater<gpu>(const char *type,
                                  mshadow::Random<gpu> *p_rnd,
                                  mshadow::Tensor<gpu, 2> weight,
                                  mshadow::Tensor<gpu, 2> wgrad,
                                  const char *tag) {
  return CreateUpdater_(type, p_rnd, weight, wgrad, tag);
}
template<>
void CreateAsyncUpdaters<gpu>(int layer_index,
                              int device_id,
                              mshadow::ps::ISharedModel<gpu, real_t> *param_server,
                              const char *type,
                              mshadow::Random<gpu> *p_rnd,
                              layer::LayerType layer_type,
                              layer::ILayer<gpu> *p_layer,
                              std::vector<IAsyncUpdater<gpu>*> *out_updaters) {
  CreateAsyncUpdaterVisitor<gpu> visitor(layer_index, device_id, param_server,
                                         type, p_rnd, layer_type, out_updaters);
  p_layer->ApplyVisitor(&visitor);
}
}  // namespace updater
}  // namespace cxxnet
####$$$$ cxxnet-master\cxxnet-master\src\utils/config.h
#ifndef CXXNET_UTILS_CONFIG_H_
#define CXXNET_UTILS_CONFIG_H_
/*!
 * \file config.h
 * \brief helper class to load in configures from file
 * \author Tianqi Chen
 */
#include <cstdio>
#include <cstring>
#include <string>
#include <istream>
#include <fstream>
#include "./utils.h"

namespace cxxnet {
namespace utils {
/*! 
 * \brief base implementation of config reader
 */
class ConfigReaderBase {
 public:
  /*! 
   * \brief get current name, called after Next returns true
   * \return current parameter name 
   */
  inline const char *name(void) const {
    return s_name.c_str();
  }
  /*! 
   * \brief get current value, called after Next returns true
   * \return current parameter value 
   */
  inline const char *val(void) const {
    return s_val.c_str();
  }
  /*! 
   * \brief move iterator to next position
   * \return true if there is value in next position
   */
  inline bool Next(void) {
    while (!this->IsEnd()) {
      GetNextToken(&s_name);
      if (s_name == "=") return false;
      if (GetNextToken(&s_buf) || s_buf != "=")  return false;
      if (GetNextToken(&s_val) || s_val == "=")  return false;
      return true;
    }
    return false;
  }
  // called before usage
  inline void Init(void) {
    ch_buf = this->GetChar();
  }

 protected:
  /*!
   * \brief to be implemented by subclass,
   * get next token, return EOF if end of file 
   */
  virtual char GetChar(void) = 0;
  /*! \brief to be implemented by child, check if end of stream */
  virtual bool IsEnd(void) = 0;

 private:
  char ch_buf;
  std::string s_name, s_val, s_buf;

  inline void SkipLine(void) {
    do {
      ch_buf = this->GetChar();
    } while (ch_buf != EOF && ch_buf != '\n' && ch_buf != '\r');
  }

  inline void ParseStr(std::string *tok) {
    while ((ch_buf = this->GetChar()) != EOF) {
      switch (ch_buf) {
        case '\\': *tok += this->GetChar(); break;
        case '\"': return;
        case '\r':
        case '\n': Error("ConfigReader: unterminated string");
        default: *tok += ch_buf;
      }
    }
    Error("ConfigReader: unterminated string");
  }
  inline void ParseStrML(std::string *tok) {
    while ((ch_buf = this->GetChar()) != EOF) {
      switch (ch_buf) {
        case '\\': *tok += this->GetChar(); break;
        case '\'': return;
        default: *tok += ch_buf;
      }
    }
    Error("unterminated string");
  }
  // return newline
  inline bool GetNextToken(std::string *tok) {
    tok->clear();
    bool new_line = false;
    while (ch_buf != EOF) {
      switch (ch_buf) {
        case '#' : SkipLine(); new_line = true; break;
        case '\"':
          if (tok->length() == 0) {
            ParseStr(tok); ch_buf = this->GetChar(); return new_line;
          } else {
            Error("ConfigReader: token followed directly by string");
          }
        case '\'':
          if (tok->length() == 0) {
            ParseStrML(tok); ch_buf = this->GetChar(); return new_line;
          } else {
            Error("ConfigReader: token followed directly by string");
          }
        case '=':
          if (tok->length() == 0) {
            ch_buf = this->GetChar();
            *tok = '=';
          }
          return new_line;
        case '\r':
        case '\n':
          if (tok->length() == 0) new_line = true;
        case '\t':
        case ' ' :
          ch_buf = this->GetChar();
          if (tok->length() != 0) return new_line;
          break;
        default:
          *tok += ch_buf;
          ch_buf = this->GetChar();
          break;
      }
    }
    if (tok->length() == 0) {
      return true;
    } else {
      return false;
    }
  }
};
/*!
 * \brief an iterator use stream base, allows use all types of istream
 */
class ConfigStreamReader: public ConfigReaderBase {
 public:
  /*!
   * \brief constructor
   * \param istream input stream
   */
  explicit ConfigStreamReader(std::istream &fin) : fin(fin) {}

 protected:
  virtual char GetChar(void) {
    return fin.get();
  }
  /*! \brief to be implemented by child, check if end of stream */
  virtual bool IsEnd(void) {
    return fin.eof();
  }

 private:
  std::istream &fin;
};

/*!
 * \brief an iterator that iterates over a configure file and gets the configures
 */
class ConfigIterator: public ConfigStreamReader {
 public:
  /*!
   * \brief constructor
   * \param fname name of configure file
   */
  explicit ConfigIterator(const char *fname) : ConfigStreamReader(fi) {
    fi.open(fname);
    if (fi.fail()) {
      utils::Error("cannot open file %s", fname);
    }
    ConfigReaderBase::Init();
  }
  /*! \brief destructor */
  ~ConfigIterator(void) {
    fi.close();
  }

 private:
  std::ifstream fi;
};
}  // namespace utils
}  // namespace cxxnet
#endif  // CXXNET_UTILS_CONFIG_H_
####$$$$ cxxnet-master\cxxnet-master\src\utils/decoder.h
#ifndef CXXNET_UTILS_DECODER_H_
#define CXXNET_UTILS_DECODER_H_

#include <vector>
#if CXXNET_USE_OPENCV_DECODER == 0
  #include <jpeglib.h>
  #include <setjmp.h>
  #include <jerror.h>
#endif

#include <mshadow/tensor.h>
#include "./utils.h"
#if CXXNET_USE_OPENCV
  #include <opencv2/opencv.hpp>
#endif

namespace cxxnet {
namespace utils {

#if CXXNET_USE_OPENCV_DECODER == 0
struct JpegDecoder {
public:
  JpegDecoder(void) {
    cinfo.err = jpeg_std_error(&jerr.base);
    jerr.base.error_exit = jerror_exit;
    jerr.base.output_message = joutput_message;
    jpeg_create_decompress(&cinfo);
  }
  // destructor
  ~JpegDecoder(void) {
    jpeg_destroy_decompress(&cinfo);
  }

  inline void Decode(unsigned char *ptr, size_t sz,
                     mshadow::TensorContainer<cpu, 3, unsigned char> *p_data) {
    if(setjmp(jerr.jmp)) {
      jpeg_destroy_decompress(&cinfo);
      utils::Error("Libjpeg fail to decode");
    }
    this->jpeg_mem_src(&cinfo, ptr, sz);
    utils::Check(jpeg_read_header(&cinfo, TRUE) == JPEG_HEADER_OK, "libjpeg: failed to decode");
    utils::Check(jpeg_start_decompress(&cinfo) == true, "libjpeg: failed to decode");
    p_data->Resize(mshadow::Shape3(cinfo.output_height, cinfo.output_width, cinfo.output_components));
    JSAMPROW jptr = &((*p_data)[0][0][0]);
    while (cinfo.output_scanline < cinfo.output_height) {
      utils::Check(jpeg_read_scanlines(&cinfo, &jptr, 1) == true, "libjpeg: failed to decode");
      jptr += cinfo.output_width * cinfo.output_components;
    }
    utils::Check(jpeg_finish_decompress(&cinfo) == true, "libjpeg: failed to decode");
  }
private:
  struct jerror_mgr {
    jpeg_error_mgr base;
    jmp_buf jmp;
  };

  METHODDEF(void) jerror_exit(j_common_ptr jinfo) {
    jerror_mgr* err = (jerror_mgr*)jinfo->err;
    longjmp(err->jmp, 1);
  }

  METHODDEF(void) joutput_message(j_common_ptr) {}

  static boolean mem_fill_input_buffer_ (j_decompress_ptr cinfo) {
    utils::Error("JpegDecoder: bad jpeg image");
    return true;
  }

  static void mem_skip_input_data_ (j_decompress_ptr cinfo, long num_bytes_) {
    jpeg_source_mgr *src = cinfo->src;
    size_t num_bytes = static_cast<size_t>(num_bytes_);
    if (num_bytes > 0) {
      src->next_input_byte += num_bytes;
      utils::Assert(src->bytes_in_buffer >= num_bytes, "fail to decode");
      src->bytes_in_buffer -= num_bytes;
    } else {
      utils::Error("JpegDecoder: bad jpeg image");

    }
  }

  static void mem_term_source_ (j_decompress_ptr cinfo) {}
  static void mem_init_source_ (j_decompress_ptr cinfo) {}
  static boolean jpeg_resync_to_restart_(j_decompress_ptr cinfo, int desired) {
    utils::Error("JpegDecoder: bad jpeg image");
    return true;
  }
  void jpeg_mem_src (j_decompress_ptr cinfo, void* buffer, long nbytes) {
    src.init_source = mem_init_source_;
    src.fill_input_buffer = mem_fill_input_buffer_;
    src.skip_input_data = mem_skip_input_data_;
    src.resync_to_restart = jpeg_resync_to_restart_;
    src.term_source = mem_term_source_;
    src.bytes_in_buffer = nbytes;
    src.next_input_byte = static_cast<JOCTET*>(buffer);
    cinfo->src = &src;
  }

private:
  jpeg_decompress_struct cinfo;
  jpeg_source_mgr src;
  jerror_mgr jerr;
};
#endif

#if CXXNET_USE_OPENCV
struct OpenCVDecoder {
  void Decode(unsigned char *ptr, size_t sz, mshadow::TensorContainer<cpu, 3, unsigned char> *p_data) {
    cv::Mat buf(1, sz, CV_8U, ptr);
    cv::Mat res = cv::imdecode(buf, 1);
    utils::Assert(res.data != NULL, "decoding fail");
    p_data->Resize(mshadow::Shape3(res.rows, res.cols, 3));
    for (int y = 0; y < res.rows; ++y) {
      for (int x = 0; x < res.cols; ++x) {
        cv::Vec3b bgr = res.at<cv::Vec3b>(y, x);
        // store in RGB order
        (*p_data)[y][x][2] = bgr[0];
        (*p_data)[y][x][1] = bgr[1];
        (*p_data)[y][x][0] = bgr[2];
      }
    }
    res.release();
  }
};
#endif
} // namespace utils
} // namespace cxxnet

#endif // DECODER_H
####$$$$ cxxnet-master\cxxnet-master\src\utils/io.h
#ifndef CXXNET_UTILS_IO_H_
#define CXXNET_UTILS_IO_H_
/*!
 * \file io.h
 * \brief definition of abstract stream interface for IO
 * \author Bing Xu Tianqi Chen
 */
#include "./utils.h"
#include <zlib.h>
#include <string>
#include <algorithm>
#include <cstring>

namespace cxxnet {
namespace utils {
/*!
 * \brief interface of stream I/O, used to serialize model
 */
class IStream {
 public:
  /*!
   * \brief read data from stream
   * \param ptr pointer to memory buffer
   * \param size size of block
   * \return usually is the size of data readed
   */
  virtual size_t Read(void *ptr, size_t size) = 0;
  /*!
   * \brief write data to stream
   * \param ptr pointer to memory buffer
   * \param size size of block
   */
  virtual void Write(const void *ptr, size_t size) = 0;
  /*! \brief virtual destructor */
  virtual ~IStream(void) {}

 public:
  // helper functions to write various of data structures
  /*!
   * \brief binary serialize a vector 
   * \param vec vector to be serialized
   */
  template<typename T>
  inline void Write(const std::vector<T> &vec) {
    uint64_t sz = static_cast<uint64_t>(vec.size());
    this->Write(&sz, sizeof(sz));
    if (sz != 0) {
      this->Write(&vec[0], sizeof(T) * sz);
    }
  }
  /*!
   * \brief binary load a vector 
   * \param out_vec vector to be loaded
   * \return whether load is successfull
   */
  template<typename T>
  inline bool Read(std::vector<T> *out_vec) {
    uint64_t sz;
    if (this->Read(&sz, sizeof(sz)) == 0) return false;
    out_vec->resize(sz);
    if (sz != 0) {
      if (this->Read(&(*out_vec)[0], sizeof(T) * sz) == 0) return false;
    }
    return true;
  }
  /*!
   * \brief binary serialize a string
   * \param str the string to be serialized
   */ 
  inline void Write(const std::string &str) {
    uint64_t sz = static_cast<uint64_t>(str.length());
    this->Write(&sz, sizeof(sz));
    if (sz != 0) {
      this->Write(&str[0], sizeof(char) * sz);
    }
  }
  /*!
   * \brief binary load a string
   * \param out_str string to be loaded
   * \return whether load is successful
   */
  inline bool Read(std::string *out_str) {
    uint64_t sz;
    if (this->Read(&sz, sizeof(sz)) == 0) return false;
    out_str->resize(sz);
    if (sz != 0) {
      if (this->Read(&(*out_str)[0], sizeof(char) * sz) == 0) return false;
    }
    return true;
  }
  /*!
   * \brief read a simple type and return it
   *        for example fs.ReadType<int>() will read int from the stream
   * \return the data readed
   * \tparam TRet the type of data to be readed 
   */
  template<typename TRet>
  inline TRet ReadType(void) {
    TRet ret;
    this->Read(&ret, sizeof(ret));
    return ret;
  }
}; // class IStream

/*! \brief interface of i/o stream that support seek */
class ISeekStream: public IStream {
 public:
  /*! \brief seek to certain position of the file */
  virtual void Seek(size_t pos) = 0;
  /*! \brief tell the position of the stream */
  virtual size_t Tell(void) = 0;
};

/*! \brief a in memory buffer that can be read and write as stream interface */
struct MemoryBufferStream : public ISeekStream {
 public:
  MemoryBufferStream(std::string *p_buffer) 
      : p_buffer_(p_buffer) {
    curr_ptr_ = 0;
  }
  virtual ~MemoryBufferStream(void) {}
  virtual size_t Read(void *ptr, size_t size) {
    utils::Assert(curr_ptr_ <= p_buffer_->length(),
                  "read can not have position excceed buffer length");
    size_t nread = std::min(p_buffer_->length() - curr_ptr_, size);
    if (nread != 0) memcpy(ptr, &(*p_buffer_)[0] + curr_ptr_, nread);
    curr_ptr_ += nread;
    return nread;
  }
  virtual void Write(const void *ptr, size_t size) {
    if (size == 0) return;
    if (curr_ptr_ + size > p_buffer_->length()) {
      p_buffer_->resize(curr_ptr_+size);
    }
    memcpy(&(*p_buffer_)[0] + curr_ptr_, ptr, size); 
    curr_ptr_ += size;
  }
  virtual void Seek(size_t pos) {
    curr_ptr_ = static_cast<size_t>(pos);
  }
  virtual size_t Tell(void) {
    return curr_ptr_;
  }

 private:
  /*! \brief in memory buffer */
  std::string *p_buffer_;
  /*! \brief current pointer */
  size_t curr_ptr_;
}; // class MemoryBufferStream

struct GzFile : public ISeekStream {
 public:
  GzFile(const char *path, const char *mode) {
    fp_ = gzopen(path, mode);
    utils::Check(fp_ != NULL, "Failed to open file %s\n", path);
  }
  virtual ~GzFile(void) {
    this->Close();
  }
  virtual void Close(void) {
    if (fp_ != NULL){
      gzclose(fp_); fp_ = NULL;
    }
  }
  virtual size_t Read(void *ptr, size_t size) {
    return gzread(fp_, ptr, size);
  }
  virtual void Write(const void *ptr, size_t size) {
    gzwrite(fp_, ptr, size);
  }
  virtual void Seek(size_t pos) {
    gzseek(fp_, pos, SEEK_SET);
  }
  virtual size_t Tell(void) {
    return static_cast<size_t>(gztell(fp_));
  }
 private:
  gzFile fp_;
};

/*! \brief implementation of file i/o stream */
class FileStream : public ISeekStream {
 public:
  explicit FileStream(FILE *fp) : fp(fp) {}
  explicit FileStream(void) {
    this->fp = NULL;
  }
  virtual size_t Read(void *ptr, size_t size) {
    return std::fread(ptr, size, 1, fp);
  }
  virtual void Write(const void *ptr, size_t size) {
    std::fwrite(ptr, size, 1, fp);
  }
  virtual void Seek(size_t pos) {
    std::fseek(fp, pos, SEEK_SET);
  }
  virtual size_t Tell(void) {
    return std::ftell(fp);
  }
  inline void Close(void) {
    if (fp != NULL){
      std::fclose(fp); fp = NULL;
    }
  }

 protected:
  FILE *fp;
};

/*! \brief implementation of file i/o stream */
class StdFile: public ISeekStream {
 public:
  /*! \brief constructor */
  StdFile(const char *fname, const char *mode) {
    Open(fname, mode);
  }
  StdFile() {}
  virtual ~StdFile(void) {
    this->Close();
  }
  virtual void Open(const char *fname, const char *mode) {
    fp_ = utils::FopenCheck(fname, mode);
    fseek(fp_, 0L, SEEK_END);
    sz_ = ftell(fp_);
    fseek(fp_, 0L, SEEK_SET);
  }
  virtual size_t Read(void *ptr, size_t size) {
    return fread(ptr, size, 1, fp_);
  }
  virtual void Write(const void *ptr, size_t size) {
    fwrite(ptr, size, 1, fp_);
  }
  virtual void Seek(size_t pos) {
    fseek(fp_, pos, SEEK_SET);
  }
  virtual size_t Tell(void) {
    return static_cast<size_t>(ftell(fp_));
  }
  inline void Close(void) {
    if (fp_ != NULL){
      fclose(fp_); fp_ = NULL;
    }
  }
  inline size_t Size() {
    return sz_;
  }
 private:
  FILE *fp_;
  size_t sz_;
}; // class StdFile

/*! \brief Basic page class */
class BinaryPage {
 public:
  /*! \brief page size 64 MB */
  static const size_t kPageSize = 64 << 18;
 public:
  /*! \brief memory data object */
  struct Obj{
    /*! \brief pointer to the data*/
    void  *dptr;
    /*! \brief size */
    size_t sz;
    Obj(void * dptr, size_t sz) : dptr(dptr), sz(sz){}
  };
 public:
  /*! \brief constructor of page */
  BinaryPage(void)  {
    data_ = new int[kPageSize];
    utils::Check(data_ != NULL, "fail to allocate page, out of space");
    this->Clear();
  };
  ~BinaryPage() {
    if (data_) delete [] data_;
  }
  /*!
   * \brief load one page form instream
   * \return true if loading is successful
   */
  inline bool Load(utils::IStream &fi) {
    return fi.Read(&data_[0], sizeof(int)*kPageSize) !=0;
  }
  /*! \brief save one page into outstream */
  inline void Save(utils::IStream &fo) {
    fo.Write(&data_[0], sizeof(int)*kPageSize);
  }
  /*! \return number of elements */
  inline int Size(void){
    return data_[0];
  }
  /*! \brief Push one binary object into page
   *  \param fname file name of obj need to be pushed into
   *  \return false or true to push into
   */
  inline bool Push(const Obj &dat) {
    if(this->FreeBytes() < dat.sz + sizeof(int)) return false;
    data_[ Size() + 2 ] = data_[ Size() + 1 ] + dat.sz;
    memcpy(this->offset(data_[ Size() + 2 ]), dat.dptr, dat.sz);
    ++ data_[0];
    return true;
  }
  /*! \brief Clear the page */
  inline void Clear(void) {
    memset(&data_[0], 0, sizeof(int) * kPageSize);
  }
  /*!
   * \brief Get one binary object from page
   *  \param r r th obj in the page
   */
  inline Obj operator[](int r) {
    utils::Assert(r < Size(), "index excceed bound");
    return Obj(this->offset(data_[ r + 2 ]),  data_[ r + 2 ] - data_[ r + 1 ]);
  }
 private:
  /*! \return number of elements */
  inline size_t FreeBytes(void) {
    return (kPageSize - (Size() + 2)) * sizeof(int) - data_[ Size() + 1 ];
  }
  inline void* offset(int pos) {
    return (char*)(&data_[0]) + (kPageSize*sizeof(int) - pos);
  }
 private:
  //int data_[ kPageSize ];
  int *data_;
};  // class BinaryPage
}  // namespace utils
}  // namespace cxxnet
#endif
####$$$$ cxxnet-master\cxxnet-master\src\utils/metric.h
#ifndef CXXNET_UTILS_METRIC_H_
#define CXXNET_UTILS_METRIC_H_
#pragma once
/*!
 * \file cxxnet_metric.h
 * \brief evaluation metrics, to be moved to nnet
 * \author Tianqi Chen
 */
#include <cmath>
#include <vector>
#include <algorithm>
#include <sstream>
#include "./random.h"
#include "../layer/layer.h"
namespace cxxnet {
namespace utils {

using namespace cxxnet::layer;
/*! \brief evaluator that evaluates the loss metrics */
class IMetric{
 public:
  IMetric(void) {}
  /*!\brief virtual destructor */
  virtual ~IMetric(void) {}
  /*! \brief clear statistics */
  virtual void Clear(void) = 0;
  /*!
   * \brief evaluate a specific metric, add to current statistics
   * \param preds prediction score array
   * \param labels label
   * \param n number of instances
   */
  virtual void AddEval(const mshadow::Tensor<cpu,2> &predscore, const LabelRecord& labels) = 0;
  /*! \brief get current result */
  virtual double Get(void) const = 0;
  /*! \return name of metric */
  virtual const char *Name(void) const= 0;
};

/*! \brief simple metric Base */
struct MetricBase : public IMetric{
 public:
  virtual ~MetricBase(void) {}
  virtual void Clear(void) {
    sum_metric = 0.0; cnt_inst = 0;
  }
  virtual void AddEval(const mshadow::Tensor<cpu,2> &predscore, const LabelRecord& labels) {
    for (index_t i = 0; i < predscore.size(0); ++ i) {
      sum_metric += CalcMetric(predscore[i], labels.label[i]);
      cnt_inst+= 1;
    }
  }
  virtual double Get(void) const{
    return sum_metric / cnt_inst;
  }
  virtual const char *Name(void) const{
    return name.c_str();
  }
 protected:
  MetricBase(const char *name) {
    this->name = name;
    this->Clear();
  }
  virtual float CalcMetric(const mshadow::Tensor<cpu,1> &predscore,
    const mshadow::Tensor<cpu,1> &label) = 0;
 private:
  double sum_metric;
  long   cnt_inst;
  std::string name;
};

/*! \brief RMSE */
struct MetricRMSE : public MetricBase{
 public:
  MetricRMSE(void):MetricBase("rmse") {
  }
  virtual ~MetricRMSE(void) {}
 protected:
  virtual float CalcMetric(const mshadow::Tensor<cpu,1> &predscore,
    const mshadow::Tensor<cpu,1> &label) {
    utils::Check(predscore.size(0) == label.size(0),
      "Metric: In RMSE metric, the size of prediction and label must be same.");
    float diff = 0;
    for (index_t i = 0; i < label.size(0); ++i) {
      diff += (predscore[i] - label[i]) * (predscore[i] - label[i]);
    }
    return diff;
  }
};

/*! \brief Error */
struct MetricError : public MetricBase{
 public:
  MetricError(void):MetricBase("error") {
  }
  virtual ~MetricError(void) {}
 protected:
  virtual float CalcMetric(const mshadow::Tensor<cpu,1> &pred,
    const mshadow::Tensor<cpu,1> &label) {
    index_t maxidx = 0;
    if (pred.size(0) != 1) {
      for (index_t i = 1; i < pred.size(0); ++ i) {
        if (pred[i] > pred[maxidx]) maxidx = i;
      }
    }else{
      maxidx = pred[0] > 0.0 ? 1 : 0;
    }
    return maxidx !=(index_t)label[0];
  }
};

/*! \brief Logloss */
struct MetricLogloss : public MetricBase{
 public:
  MetricLogloss(void):MetricBase("logloss") {
  }
  virtual ~MetricLogloss(void) {}
 protected:
  virtual float CalcMetric(const mshadow::Tensor<cpu,1> &pred,
    const mshadow::Tensor<cpu,1> &label) {
    int target = static_cast<int>(label[0]);
    if (pred.size(0) != 1) {
      return - std::log(std::max(std::min(pred[target], 1.0f - 1e-15f), 1e-15f));
    }else{
      const float py = std::max(std::min(pred[0], 1.0f - 1e-15f), 1e-15f);
      const float y = label[0];
      const float res = - (y * std::log(py) + (1.0f - y)*std::log(1 - py));
      utils::Check(res == res, "NaN detected!");
      return res;
    }
  }
};

/*! \brief Recall@n */
struct MetricRecall : public MetricBase{
 public:
  MetricRecall(const char *name): MetricBase(name) {
    utils::Assert(sscanf(name, "rec@%d", &topn) == 1, "must specify n for rec@n");
  }
  virtual ~MetricRecall(void) {}
 protected:
  virtual float CalcMetric(const mshadow::Tensor<cpu,1> &pred,
    const mshadow::Tensor<cpu,1> &label) {
    utils::Check(pred.size(0) >= (index_t)topn,
      "it is meaningless to take rec@n for list shorter than n, evaluating rec@%d, list=%u\n",
      topn, pred.size(0));
    vec.resize(pred.size(0));
    for (index_t i = 0; i < pred.size(0); ++ i) {
      vec[i] = std::make_pair(pred[i], i);
    }
    rnd.Shuffle(vec);
    std::sort(vec.begin(), vec.end(), CmpScore);
    int hit = 0;
    for (int i = 0; i < topn; ++ i) {
      for (index_t j = 0; j < label.size(0); ++j){
        if (vec[i].second == static_cast<index_t>(label[j])) {
          ++hit;
          break;
        }
      }
    }
    return (float)hit / label.size(0);
  }
 private:
  inline static bool CmpScore(const std::pair<float,index_t> &a, const std::pair<float,index_t> &b) {
    return a.first > b.first;
  }

  std::vector< std::pair<float,index_t> > vec;
  utils::RandomSampler rnd;
  int topn;
};

/*! \brief a set of evaluators */
struct MetricSet{
 public:
  ~MetricSet(void) {
    for (size_t i = 0; i < evals_.size(); ++ i) {
      delete evals_[i];
    }
  }
  static IMetric* Create(const char *name) {
    if (!strcmp(name, "rmse")) return new MetricRMSE();
    if (!strcmp(name, "error")) return new MetricError();
    if (!strcmp(name, "logloss")) return new MetricLogloss();
    if (!strncmp(name, "rec@",4)) return new MetricRecall(name);
    return NULL;
  }
  void AddMetric(const char *name, const char* field) {
    IMetric *metric = this->Create(name);
    if (metric != NULL){
      evals_.push_back(metric);
      label_fields_.push_back(field);
    } else {
      utils::Error("Metric: Unknown metric name: %s\n", name);
    }
  }
  inline void Clear(void) {
    for (size_t i = 0; i < evals_.size(); ++ i) {
      evals_[i]->Clear();
    }
  }
  inline void AddEval(const std::vector<mshadow::Tensor<cpu, 2> >& predscores,
    const layer::LabelInfo& labels) {
    utils::Assert(predscores.size() == evals_.size(),
      "Metric: Number of predict scores and number of metrics should be equal.");
    for (size_t i = 0; i < evals_.size(); ++ i) {
      std::map<std::string, size_t>::const_iterator it =
        labels.name2findex->find(label_fields_[i]);
      utils::Check(it != labels.name2findex->end(), "Metric: unknown target = %s",
                 label_fields_[i].c_str());
      evals_[i]->AddEval(predscores[i], labels.fields[it->second]);
    }
  }
  inline std::string Print(const char *evname) {
    std::stringstream ss;
    for (size_t i = 0; i < evals_.size(); ++ i) {
      ss << '\t' << evname << '-' << evals_[i]->Name();
      if (label_fields_[i] != "label") {
        ss << '[' << label_fields_[i] << ']';
      }
      ss << ':' << evals_[i]->Get();
    }
    return ss.str();
  }
 private:
  inline static bool CmpName(const IMetric *a, const IMetric *b) {
    return strcmp(a->Name(), b->Name()) < 0;
  }
  inline static bool EqualName(const IMetric *a, const IMetric *b) {
    return strcmp(a->Name(), b->Name()) == 0;
  }
 private:
  std::vector<IMetric*> evals_;
  std::vector<std::string> label_fields_;
};
}  // namespace utils
}  // namespace cxxnet
#endif
####$$$$ cxxnet-master\cxxnet-master\src\utils/random.h
#ifndef CXXNET_UTILS_GLOBAL_RANDOM_H_
#define CXXNET_UTILS_GLOBAL_RANDOM_H_
/*!
 * \file global_random.h
 * \brief global random number utils, used for some preprocessing
 * \author Tianqi Chen
 */
#include <cstdlib>
#include <vector>
#include <cmath>
#include "./utils.h"

namespace cxxnet {
namespace utils {
/*! \brief simple thread dependent random sampler */
class RandomSampler {
 public:
  RandomSampler(void) : rseed_(0) {
  }
  /*!
   * \brief seed random number 
   * \param seed the random number seed
   */
  inline void Seed(unsigned seed) {
    this->rseed_ = seed;
  }
  /*! \brief return a real number uniform in [0,1) */
  inline double NextDouble() {
    return static_cast<double>(rand_r(&rseed_)) /
        (static_cast<double>(RAND_MAX) + 1.0);
  }
  /*! \brief return a random number in n */
  inline uint32_t NextUInt32(uint32_t n) {
    return static_cast<uint32_t>(floor(NextDouble() * n));
  }
  /*! \brief random shuffle data */
  template<typename T>
  inline void Shuffle(T *data, size_t sz) {
    if(sz == 0) return;
    for(uint32_t i = (uint32_t)sz - 1; i > 0; i--) {
      std::swap(data[i], data[NextUInt32(i+1)]);
    } 
  }
  /*!\brief random shuffle data in */
  template<typename T>
  inline void Shuffle(std::vector<T> &data) {
    Shuffle(&data[0], data.size());
  }

 private:
  unsigned rseed_;
};
}  // namespace utils
}  // namespace cxxnet
#endif
####$$$$ cxxnet-master\cxxnet-master\src\utils/thread.h
#ifndef CXXNET_UTILS_THREAD_H_
#define CXXNET_UTILS_THREAD_H_
/*!
 * \file thread.h
 * \brief this header include the minimum necessary resource for multi-threading
 * \author Tianqi Chen
 * Acknowledgement: this file is adapted from SVDFeature project, by same author. 
 *  The MAC support part of this code is provided by Artemy Kolchinsky
 */
#ifdef _MSC_VER
#include "utils.h"
#include <windows.h>
#include <process.h>
namespace cxxnet {
namespace utils {
/*! \brief simple semaphore used for synchronization */
class Semaphore {
 public :
  inline void Init(int init_val) {
    sem = CreateSemaphore(NULL, init_val, 10, NULL);
    utils::Assert(sem != NULL, "create Semaphore error");
  }
  inline void Destroy(void) {
    CloseHandle(sem);
  }
  inline void Wait(void) {
    utils::Assert(WaitForSingleObject(sem, INFINITE) == WAIT_OBJECT_0, "WaitForSingleObject error");
  }
  inline void Post(void) {
    utils::Assert(ReleaseSemaphore(sem, 1, NULL)  != 0, "ReleaseSemaphore error");
  }
 private:
  HANDLE sem;
};
/*! \brief simple thread that wraps windows thread */
class Thread {
 private:
  HANDLE    thread_handle;
  unsigned  thread_id;            
 public:
  inline void Start(unsigned int __stdcall entry(void*), void *param) {
    thread_handle = (HANDLE)_beginthreadex(NULL, 0, entry, param, 0, &thread_id);
  }            
  inline int Join(void) {
    WaitForSingleObject(thread_handle, INFINITE);
    return 0;
  }
};
/*! \brief exit function called from thread */
inline void ThreadExit(void *status) {
  _endthreadex(0);
}
#define CXXNET_THREAD_PREFIX unsigned int __stdcall
}  // namespace utils
}  // namespace cxxnet
#else
// thread interface using g++     
#include <semaphore.h>
#include <pthread.h>
namespace cxxnet {
namespace utils {
/*!\brief semaphore class */
class Semaphore {
  #ifdef __APPLE__
 private:
  sem_t* semPtr;
  char sema_name[20];            
 private:
  inline void GenRandomString(char *s, const int len) {
    static const char alphanum[] = "0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ" ;
    for (int i = 0; i < len; ++i) {
      s[i] = alphanum[rand() % (sizeof(alphanum) - 1)];
    }
    s[len] = 0;
  }
 public:
  inline void Init(int init_val) {
    sema_name[0]='/'; 
    sema_name[1]='s'; 
    sema_name[2]='e'; 
    sema_name[3]='/'; 
    GenRandomString(&sema_name[4], 16);
    if((semPtr = sem_open(sema_name, O_CREAT, 0644, init_val)) == SEM_FAILED) {
      perror("sem_open");
      exit(1);
    }
    utils::Assert(semPtr != NULL, "create Semaphore error");
  }
  inline void Destroy(void) {
    if (sem_close(semPtr) == -1) {
      perror("sem_close");
      exit(EXIT_FAILURE);
    }
    if (sem_unlink(sema_name) == -1) {
      perror("sem_unlink");
      exit(EXIT_FAILURE);
    }
  }
  inline void Wait(void) {
    sem_wait(semPtr);
  }
  inline void Post(void) {
    sem_post(semPtr);
  }               
  #else
 private:
  sem_t sem;
 public:
  inline void Init(int init_val) {
    sem_init(&sem, 0, init_val);
  }
  inline void Destroy(void) {
    sem_destroy(&sem);
  }
  inline void Wait(void) {
    sem_wait(&sem);
  }
  inline void Post(void) {
    sem_post(&sem);
  }
  #endif  
};
/*!\brief simple thread class */
class Thread {
 private:
  pthread_t thread;                
 public :
  inline void Start(void * entry(void*), void *param) {
    pthread_attr_t attr;
    pthread_attr_init(&attr);
    pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_JOINABLE);
    pthread_create(&thread, &attr, entry, param);
  }  
  inline int Join(void) {
    void *status;
    return pthread_join(thread, &status);
  }
};
inline void ThreadExit(void *status) {
  pthread_exit(status);
}
}  // namespace utils
}  // namespace cxxnet
#define CXXNET_THREAD_PREFIX void *
#endif
#endif
####$$$$ cxxnet-master\cxxnet-master\src\utils/thread_buffer.h
#ifndef CXXNET_UTILS_THREAD_BUFFER_H_
#define CXXNET_UTILS_THREAD_BUFFER_H_
/*!
 * \file thread_buffer.h
 * \brief  multi-thread buffer, iterator, can be used to create parallel pipeline
 * \author Tianqi Chen
 */
#include <vector>
#include <cstring>
#include <cstdlib>
#include "./utils.h"
#include "./thread.h"
namespace cxxnet {
namespace utils {
/*!
 * \brief buffered loading iterator that uses multithread
 * this template method will assume the following paramters
 * \tparam Elem elememt type to be buffered
 * \tparam ElemFactory factory type to implement in order to use thread buffer
 */
template<typename Elem, typename ElemFactory>
class ThreadBuffer {
 public:
  /*!\brief constructor */
  ThreadBuffer(void) {
    this->init_end = false;
    this->buf_size = 30;
  }
  ~ThreadBuffer(void) {
    if(init_end) this->Destroy();
  }
  /*!\brief set parameter, will also pass the parameter to factory */
  inline void SetParam(const char *name, const char *val) {
    if (!strcmp( name, "buffer_size")) buf_size = atoi(val);
    factory.SetParam(name, val);
  }
  /*!
   * \brief initalize the buffered iterator
   * \param param a initialize parameter that will pass to factory, ignore it if not necessary
   * \return false if the initlization can't be done, e.g. buffer file hasn't been created 
   */
  inline bool Init(void) {
    if (!factory.Init()) return false;
    bufA.reserve(buf_size);
    bufB.reserve(buf_size);
    for (int i = 0; i < buf_size; ++i) {
      bufA.push_back(factory.Create());
      bufB.push_back(factory.Create());
    }
    this->init_end = true;
    this->StartLoader();
    return true;
  }  
  /*!\brief place the iterator before first value */
  inline void BeforeFirst(void) {
    // wait till last loader end
    loading_end.Wait();
    // critcal zone
    current_buf = 1;
    factory.BeforeFirst();
    // reset terminate limit
    endA = endB = buf_size;
    // wake up loader for first part
    loading_need.Post();
    // wait til first part is loaded
    loading_end.Wait();
    // set current buf to right value
    current_buf = 0;
    // wake loader for next part
    data_loaded = false;
    loading_need.Post();
    // set buffer value
    buf_index = 0;
  }  
  /*! \brief destroy the buffer iterator, will deallocate the buffer */
  inline void Destroy(void) {
    // wait until the signal is consumed
    this->destroy_signal = true;
    loading_need.Post();
    loader_thread.Join();
    loading_need.Destroy();
    loading_end.Destroy();    
    for (size_t i = 0; i < bufA.size(); ++i) {
      factory.FreeSpace(bufA[i]);
    }
    for (size_t i = 0; i < bufB.size(); ++i) {
      factory.FreeSpace(bufB[i]);
    }
    bufA.clear(); bufB.clear();
    factory.Destroy();
    this->init_end = false;
  }  
  /*!
   * \brief get the next element needed in buffer
   * \param elem element to store into
   * \return whether reaches end of data
   */
  inline bool Next(Elem &elem) {
    // end of buffer try to switch
    if (buf_index == buf_size) {
      this->SwitchBuffer();
      buf_index = 0;
    }
    if (buf_index >= (current_buf ? endA : endB)) { 
      return false;
    }
    std::vector<Elem> &buf = current_buf ? bufA : bufB;
    elem = buf[buf_index];
    ++buf_index;
    return true;
  }      
  /*!
   * \brief get the factory object
   */
  inline ElemFactory &get_factory(void) {
    return factory;
  }
  inline const ElemFactory &get_factory(void) const{
    return factory;
  }
  // size of buffer
  int  buf_size;
 private:
  // factory object used to load configures
  ElemFactory factory;
  // index in current buffer
  int buf_index;
  // indicate which one is current buffer
  int current_buf;
  // max limit of visit, also marks termination
  int endA, endB;
  // double buffer, one is accessed by loader
  // the other is accessed by consumer
  // buffer of the data
  std::vector<Elem> bufA, bufB;
  // initialization end
  bool init_end;
  // singal whether the data is loaded
  bool data_loaded;
  // signal to kill the thread
  bool destroy_signal;
  // thread object
  Thread loader_thread;
  // signal of the buffer
  Semaphore loading_end, loading_need;
  /*!
   * \brief slave thread
   * this implementation is like producer-consumer style
   */
  inline void RunLoader(void) {
    while(!destroy_signal) {
      // sleep until loading is needed
      loading_need.Wait();      
      std::vector<Elem> &buf = current_buf ? bufB : bufA;
      int i;
      for (i = 0; i < buf_size ; ++i) {
        if (!factory.LoadNext(buf[i])) {
          int &end = current_buf ? endB : endA;
          end = i; // marks the termination
          break;
        }
      }
      // signal that loading is done
      data_loaded = true;
      loading_end.Post();
    }
  }
  /*!\brief entry point of loader thread */
  inline static CXXNET_THREAD_PREFIX LoaderEntry(void *pthread) {
    static_cast< ThreadBuffer<Elem,ElemFactory>* >(pthread)->RunLoader();
    ThreadExit(NULL);
    return NULL;
  }
  /*!\brief start loader thread */
  inline void StartLoader(void) {
    destroy_signal = false;
    // set param
    current_buf = 1;    
    loading_need.Init(1);
    loading_end .Init(0);
    // reset terminate limit
    endA = endB = buf_size;
    loader_thread.Start(LoaderEntry, this);
    // wait until first part of data is loaded
    loading_end.Wait();
    // set current buf to right value
    current_buf = 0;
    // wake loader for next part
    data_loaded = false;
    loading_need.Post();    
    buf_index = 0; 
  }
  /*!\brief switch double buffer */
  inline void SwitchBuffer(void) {
    loading_end.Wait();
    // loader shall be sleep now, critcal zone!
    current_buf = !current_buf;
    // wake up loader
    data_loaded = false;
    loading_need.Post();
  }
};
}  // namespace utils
}  // namespace cxxnet
#endif
####$$$$ cxxnet-master\cxxnet-master\src\utils/timer.h
/*!
* \file timer.h
* \brief This file defines the utils for timing
* \author Tianqi Chen
*/
#ifndef CXXNET_TIMER_H_
#define CXXNET_TIMER_H_
#include <time.h>
#include <string>
#ifdef __MACH__
#include <mach/clock.h>
#include <mach/mach.h>
#endif
namespace cxxnet {
namespace utils {
inline double GetTime(void) {
  // Adapted from: https://gist.github.com/jbenet/1087739
  #ifdef __MACH__ // OS X does not have clock_gettime, use clock_get_time
  clock_serv_t cclock;
  mach_timespec_t mts;
  host_get_clock_service(mach_host_self(), CALENDAR_CLOCK, &cclock);
  utils::Check(clock_get_time(cclock, &mts) == 0, "failed to get time");
  mach_port_deallocate(mach_task_self(), cclock);
  return static_cast<double>(mts.tv_sec) + static_cast<double>(mts.tv_nsec) * 1e-9;
  #else
  timespec ts;
  utils::Check(clock_gettime(CLOCK_REALTIME, &ts) == 0, "failed to get time");
  return static_cast<double>(ts.tv_sec) + static_cast<double>(ts.tv_nsec) * 1e-9;
  #endif
}
}
}
#endif
####$$$$ cxxnet-master\cxxnet-master\src\utils/utils.h
#ifndef CXXNET_UTILS_UTILS_H_
#define CXXNET_UTILS_UTILS_H_
/*!
 * \file utils.h
 * \brief simple utils to support the code, adopted from xgboost
 * \author Tianqi Chen
 */
#define _CRT_SECURE_NO_WARNINGS
#include <cstdio>
#include <string>
#include <cstdlib>
#include <vector>
#include <cstdarg>

#if !defined(__GNUC__)
#define fopen64 std::fopen
#endif
#ifdef _MSC_VER
// NOTE: sprintf_s is not equivalent to snprintf,
// they are equivalent when success, which is sufficient for our case
#define snprintf sprintf_s
#define vsnprintf vsprintf_s
#else
#ifdef _FILE_OFFSET_BITS
#if _FILE_OFFSET_BITS == 32
#pragma message ("Warning: FILE OFFSET BITS defined to be 32 bit")
#endif
#endif

#ifdef __APPLE__
#define off64_t off_t
#define fopen64 std::fopen
#endif

extern "C" {
#include <sys/types.h>
}
#endif

#ifdef _MSC_VER
typedef unsigned char uint8_t;
typedef unsigned short int uint16_t;
typedef unsigned int uint32_t;
typedef unsigned long uint64_t;
typedef long int64_t;
#else
#include <inttypes.h>
#endif

#ifndef CHECK
#define CHECK(ARGS) cxxnet::utils::Check(ARGS, "Assert Error at %s: %d", __FILE__, __LINE__);
#endif
#define CUDA_CHECK(ARGS) CHECK(ARGS==0);

namespace cxxnet {
/*! \brief namespace for helper utils of the project */
namespace utils {

/*! \brief error message buffer length */
const int kPrintBuffer = 1 << 12;

#ifndef CXXNET_CUSTOMIZE_MSG_
/*!
 * \brief handling of Assert error, caused by in-apropriate input
 * \param msg error message
 */
inline void HandleAssertError(const char *msg) {
  fprintf(stderr, "AssertError:%s\n", msg);
  exit(-1);
}
/*!
 * \brief handling of Check error, caused by in-apropriate input
 * \param msg error message
 */
inline void HandleCheckError(const char *msg) {
  fprintf(stderr, "%s\n", msg);
  exit(-1);
}
inline void HandlePrint(const char *msg) {
  printf("%s", msg);
}
#else
// include declarations, some one must implement this
void HandleAssertError(const char *msg);
void HandleCheckError(const char *msg);
void HandlePrint(const char *msg);
#endif

/*! \brief printf, print message to the console */
inline void Printf(const char *fmt, ...) {
  std::string msg(kPrintBuffer, '\0');
  va_list args;
  va_start(args, fmt);
  vsnprintf(&msg[0], kPrintBuffer, fmt, args);
  va_end(args);
  HandlePrint(msg.c_str());
}
/*! \brief portable version of snprintf */
inline int SPrintf(char *buf, size_t size, const char *fmt, ...) {
  va_list args;
  va_start(args, fmt);
  int ret = vsnprintf(buf, size, fmt, args);
  va_end(args);
  return ret;
}

/*! \brief assert an condition is true, use this to handle debug information */
inline void Assert(bool exp, const char *fmt, ...) {
  if (!exp) {
    std::string msg(kPrintBuffer, '\0');
    va_list args;
    va_start(args, fmt);
    vsnprintf(&msg[0], kPrintBuffer, fmt, args);
    va_end(args);
    HandleAssertError(msg.c_str());
  }
}

/*!\brief same as assert, but this is intended to be used as message for user*/
inline void Check(bool exp, const char *fmt, ...) {
  if (!exp) {
    std::string msg(kPrintBuffer, '\0');
    va_list args;
    va_start(args, fmt);
    vsnprintf(&msg[0], kPrintBuffer, fmt, args);
    va_end(args);
    HandleCheckError(msg.c_str());
  }
}

/*! \brief report error message, same as check */
inline void Error(const char *fmt, ...) {
  {
    std::string msg(kPrintBuffer, '\0');
    va_list args;
    va_start(args, fmt);
    vsnprintf(&msg[0], kPrintBuffer, fmt, args);
    va_end(args);
    HandleCheckError(msg.c_str());
  }
}

/*! \brief replace fopen, report error when the file open fails */
inline std::FILE *FopenCheck(const char *fname, const char *flag) {
  std::FILE *fp = fopen64(fname, flag);
  Check(fp != NULL, "can not open file \"%s\"\n", fname);
  return fp;
}
}  // namespace utils
// easy utils that can be directly acessed in xgboost
/*! \brief get the beginning address of a vector */
template<typename T>
inline T *BeginPtr(std::vector<T> &vec) {
  if (vec.size() == 0) {
    return NULL;
  } else {
    return &vec[0];
  }
}
/*! \brief get the beginning address of a vector */
template<typename T>
inline const T *BeginPtr(const std::vector<T> &vec) {
  if (vec.size() == 0) {
    return NULL;
  } else {
    return &vec[0];
  }
}
}  // namespace cxxnet
#endif  // CXXNET_UTILS_UTILS_H_
####$$$$ cxxnet-master\cxxnet-master\tools/im2bin.cpp
#include "src/utils/io.h"
#include <cstdio>
#include <string>
#include <vector>

int main(int argc, char **argv) {
    using namespace cxxnet::utils;
    if (argc != 4) {
        fprintf(stderr, "Usage: imbin image.lst image_root_dir output_file\n");
        exit(-1);
    }
    char fname[ 256 ];
    unsigned int index = 0;
    float label = 0.0f;
    std::string root_path = argv[2];
    BinaryPage pg;

    StdFile writer(argv[3], "wb");
    std::vector<unsigned char> buf( BinaryPage::kPageSize * sizeof(int), 0 );

    FILE *fplst = FopenCheck(argv[1], "r");
    time_t start = time( NULL );
    long imcnt = 0, pgcnt = 0;
    long elapsed;

    printf( "create image binary pack from %s, this will take some time...\n", argv[1] );

    while( fscanf( fplst,"%u%f %[^\n]\n", &index, &label, fname ) == 3 ) {
        std::string path = fname;
        path = root_path + path;
        StdFile reader(path.c_str(), "rb");
        BinaryPage::Obj fobj(&buf[0], reader.Size());

        if( reader.Size() > buf.size() ){
            fprintf( stderr, "image %s is too large to fit into a single page, considering increase kPageSize\n", path.c_str() );
            Error("image size too large");
        }

        reader.Read(fobj.dptr, fobj.sz);
        reader.Close();

        ++ imcnt;
        if (!pg.Push(fobj)) {
            pg.Save(writer);
            pg.Clear();
            if( !pg.Push(fobj) ){
                fprintf( stderr, "image %s is too large to fit into a single page, considering increase kPageSize\n", path.c_str() );
                Error("image size too large");
            }
            pgcnt += 1;
        }
        if( imcnt % 1000 == 0 ){
            elapsed = (long)(time(NULL) - start);
            printf("\r                                                               \r");
            printf("[%8lu] images processed to %lu pages, %ld sec elapsed", imcnt, pgcnt, elapsed );
            fflush( stdout );
        }
    }
    if( pg.Size() != 0 ){
        pg.Save(writer);
        pgcnt += 1;
    }
    elapsed = (long)(time(NULL) - start);
    printf("\nfinished [%8lu] images processed to %lu pages, %ld sec elapsed\n", imcnt, pgcnt, elapsed );
    writer.Close();
    return 0;
}
####$$$$ cxxnet-master\cxxnet-master\tools/imgbin-partition-maker.py
import sys
import os
import random
import argparse

random.seed(888)


parser = argparse.ArgumentParser(description='Generate a Makfile to make partition imgbin file for cxxnet')
parser.add_argument('--img_list', required=True, help="path to list of all images")
parser.add_argument('--img_root', required=True, help="prefix path to the file path in img_list")
parser.add_argument('--im2bin', default='./im2bin', help="path to im2bin tools")
parser.add_argument('--partition_size', default="256", help="max size of single bin file")
parser.add_argument('--shuffle', default='0', help="Shuffle the list or not")
parser.add_argument('--prefix', required=True, help="Prefix of output image lists and bins")
parser.add_argument('--out', required=True, help="Output folder for image bins and lists")
parser.add_argument('--makefile', default="Gen.mk", help="name of generated Makefile")

if len(sys.argv) < 5:
    print parser.print_help()
    exit(-1)

args = parser.parse_args()
# im2bin path
IM2BIN = args.im2bin

fi = file(args.img_list)
lst = [line for line in fi]

img_root = args.img_root

if args.shuffle == "1":
    random.shuffle(lst)

prefix = args.prefix
output_dir = args.out
if output_dir[-1] != '/':
    output_dir += '/'

fo = open(args.makefile, "w")

objs = []
cmds = []
fw = None
sz = 0
img_cnt = 1;
cnt = 1

for item in lst:
    if sz + 10240 > (int(args.partition_size)<<20) or fw == None:
        lst_name = output_dir + (prefix % cnt) + '.lst'
        bin_name = output_dir + (prefix % cnt) + '.bin'
        objs.append(bin_name)
        if fw != None:
            fw.close()
        fw = open(lst_name, "w")
        cmd = "%s: %s\n\t%s %s %s %s" % (bin_name, lst_name,
                IM2BIN, lst_name, img_root, bin_name)
        cmds.append(cmd)
        sz = 0
        cnt += 1
        img_cnt = 1
    path = item.split('\t')[2][:-1]
    sz += os.path.getsize(img_root + path) + (img_cnt + 2) * 4
    fw.write(item)
    img_cnt += 1

obj = "all: " + ' '.join(objs) + '\n'
fo.write(obj)
fo.write('\n\n'.join(cmds))
fo.close()
fw.close()









####$$$$ cxxnet-master\cxxnet-master\tools/Makefile
# set LD_LIBRARY_PATH
export CC  = gcc
export CXX = g++
export NVCC =nvcc

export CFLAGS = -Wall -O3 -msse3 -Wno-unknown-pragmas -funroll-loops -I../mshadow/ -I.. -DMSHADOW_USE_MKL=0

export LDFLAGS=
export NVCCFLAGS = -g -O3 -ccbin $(CXX)

# specify tensor path
BIN = im2bin
OBJ =
CUOBJ =
CUBIN =
.PHONY: clean all

all: $(BIN) $(OBJ) $(CUBIN) $(CUOBJ)

im2bin: im2bin.cpp

$(BIN) :
	$(CXX) $(CFLAGS) -o $@ $(filter %.cpp %.o %.c, $^)  $(LDFLAGS)

$(OBJ) :
	$(CXX) -c $(CFLAGS) -o $@ $(firstword $(filter %.cpp %.c, $^) )

$(CUOBJ) :
	$(NVCC) -c -o $@ $(NVCCFLAGS) -Xcompiler "$(CFLAGS)" $(filter %.cu, $^)

$(CUBIN) :
	$(NVCC) -o $@ $(NVCCFLAGS) -Xcompiler "$(CFLAGS)" -Xlinker "$(LDFLAGS)" $(filter %.cu %.cpp %.o, $^)

clean:
	$(RM) $(OBJ) $(BIN) $(CUBIN) $(CUOBJ) *~

####$$$$ cxxnet-master\cxxnet-master\wrapper/cxxnet.py
"""
CXXNet python ctypes wrapper
Author: Tianqi Chen, Bing Xu

"""
import ctypes
import os
import sys
import numpy
import numpy.ctypeslib

# set this line correctly
if os.name == 'nt':
    # TODO windows
    CXXNET_PATH = os.path.dirname(__file__) + '/libcxxnetwrapper.dll'
else:
    CXXNET_PATH = os.path.dirname(__file__) + '/libcxxnetwrapper.so'

# load in xgboost library
cxnlib = ctypes.cdll.LoadLibrary(CXXNET_PATH)
cxnlib.CXNIOCreateFromConfig.restype = ctypes.c_void_p
cxnlib.CXNIONext.restype = ctypes.c_int
cxnlib.CXNIOGetData.restype = ctypes.POINTER(ctypes.c_float)
cxnlib.CXNIOGetLabel.restype = ctypes.POINTER(ctypes.c_float)
cxnlib.CXNNetCreate.restype = ctypes.c_void_p
cxnlib.CXNNetPredictBatch.restype = ctypes.POINTER(ctypes.c_float)
cxnlib.CXNNetPredictIter.restype = ctypes.POINTER(ctypes.c_float)
cxnlib.CXNNetExtractBatch.restype = ctypes.POINTER(ctypes.c_float)
cxnlib.CXNNetExtractIter.restype = ctypes.POINTER(ctypes.c_float)
cxnlib.CXNNetGetWeight.restype = ctypes.POINTER(ctypes.c_float)
cxnlib.CXNNetEvaluate.restype = ctypes.c_char_p


def ctypes2numpy(cptr, length, dtype=numpy.float32):
    """convert a ctypes pointer array to numpy array """
    #assert isinstance(cptr, ctypes.POINTER(ctypes.c_float))
    res = numpy.zeros(length, dtype=dtype)
    assert ctypes.memmove(res.ctypes.data, cptr, length * res.strides[0])
    return res

def ctypes2numpyT(cptr, shape, dtype=numpy.float32, stride = None):
    """convert a ctypes pointer array to numpy array """
    size = 1
    for x in shape:
        size *= x
    if stride is None:
        res = numpy.zeros(size, dtype=dtype)
        assert ctypes.memmove(res.ctypes.data, cptr, size * res.strides[0])
    else:
        dsize = size / shape[-1] * stride
        res = numpy.zeros(dsize, dtype=dtype)
        assert ctypes.memmove(res.ctypes.data, cptr, dsize * res.strides[0])
        res = res.reshape((dsize / shape[-1], shape[-1]))
        res = res[:, 0 :shape[-1]]
    return res.reshape(shape)

def shape2ctypes(data):
    shape = (ctypes.c_uint * data.ndim)()
    for i in range(data.ndim):
        shape[i] = data.shape[i]
    return shape


class DataIter:
    """data iterator of cxxnet"""
    def __init__(self, cfg):
        self.handle = cxnlib.CXNIOCreateFromConfig(ctypes.c_char_p(cfg.encode('utf-8')))
        self.head = True
        self.tail = False
    def __del__(self):
        """destructor"""
        cxnlib.CXNIOFree(self.handle)
    def next(self):
        """next batch in iter"""
        ret = cxnlib.CXNIONext(self.handle)
        self.head = False
        self.tail = ret == 0
        return ret != 0
    def before_first(self):
        """reset iterator"""
        cxnlib.CXNIOBeforeFirst(self.handle)
        self.head = True
        self.tail = False
    def check_valid(self):
        """check iterator state"""
        if self.head:
            raise Exception('iterator was at head state, call next to get to valid state')
        if self.tail:
            raise Exception('iterator reaches end')
    def get_data(self):
        """get current batch data"""
        oshape = (ctypes.c_uint * 4)()
        ostride = ctypes.c_uint()
        ret = cxnlib.CXNIOGetData(self.handle,
                                  oshape, ctypes.byref(ostride))
        return ctypes2numpyT(ret, [x for x in oshape], 'float32', ostride.value)
    def get_label(self):
        """get current batch label"""
        oshape = (ctypes.c_uint * 2)()
        ostride = ctypes.c_uint()
        ret = cxnlib.CXNIOGetLabel(self.handle,
                                   oshape, ctypes.byref(ostride))
        return ctypes2numpyT(ret, [x for x in oshape], 'float32', ostride.value)

class Net:
    """neural net object"""
    def __init__(self, dev = 'cpu', cfg = ''):
        self.handle = cxnlib.CXNNetCreate(ctypes.c_char_p(dev.encode('utf-8')),
                                          ctypes.c_char_p(cfg.encode('utf-8')))

    def __del__(self):
        """destructor"""
        cxnlib.CXNNetFree(self.handle)

    def set_param(self, name, value):
        """set paramter to the trainer"""
        name = str(name)
        value = str(value)
        cxnlib.CXNNetSetParam(self.handle,
                              ctypes.c_char_p(name.encode('utf-8')),
                              ctypes.c_char_p(value.encode('utf-8')))

    def init_model(self):
        """ initialize the network structure
        """
        cxnlib.CXNNetInitModel(self.handle)

    def load_model(self, fname):
        """ load model from file
        Parameters
            fname: str
                name of model
        """
        cxnlib.CXNNetLoadModel(self.handle, fname)

    def save_model(self, fname):
        """ save model to file
        Parameters
            fname: str
                name of model
        """
        cxnlib.CXNNetSaveModel(self.handle, fname)

    def start_round(self, round_counter):
        """ notify the net the training phase of round counter begins
        Parameters
            round_counter: int
                current round counter
        """
        cxnlib.CXNNetStartRound(self.handle, round_counter)

    def update(self, data, label = None):
        """ update the net using the data
        Parameters
            data: input can be DataIter or numpy.ndarray
            label: the label of the data batch
        """
        if isinstance(data, DataIter):
            data.check_valid()
            cxnlib.CXNNetUpdateIter(self.handle, data.handle)
        elif isinstance(data, numpy.ndarray):
            if data.ndim != 4:
                raise Exception('Net.update: need 4 dimensional tensor (batch, channel, height, width)')
            if label is None:
                raise Exception('Net.update: need label to use update')
            if not isinstance(label, numpy.ndarray):
                raise Exception('Net.update: label need to be ndarray')
            if label.ndim == 1:
                label = label.reshape((label.size(0),1))
            if label.ndim != 2:
                raise Exception('Net.update: label need to be 2 dimension or one dimension ndarray')
            if label.shape[0] != data.shape[0]:
                raise Exception('Net.update: data size mismatch')
            cxnlib.CXNNetUpdateBatch(self.handle,
                                     data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),
                                     shape2ctypes(data),
                                     label.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),
                                     shape2ctypes(label))
        else:
            raise Exception('update do not support type %s' % str(type(data)))

    def evaluate(self, data, name):
        """ evaluate the model using data iterator
        Parameters
            data: input can be DataIter
            name: str
                name of the input data
        Return:
            Evaluation string
        """
        if isinstance(data, DataIter):
            return cxnlib.CXNNetEvaluate(self.handle, data.handle, name)
        else:
            raise Exception('update do not support type %s' % str(type(data)))

    def predict(self, data):
        """ make prediction from data
        Parameters
            data: iter or numpy ndarray
        Return
            prediction in numpy array
        """
        olen = ctypes.c_uint()
        if isinstance(data, DataIter):
            data.check_valid()
            ret = cxnlib.CXNNetPredictIter(self.handle,
                                           data.handle,
                                           ctypes.byref(olen));
        elif isinstance(data, numpy.ndarray):
            if data.ndim != 4:
                raise Exception('need 4 dimensional tensor to use predict')

            ret = cxnlib.CXNNetPredictBatch(self.handle,
                                            data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),
                                            shape2ctypes(data),
                                            ctypes.byref(olen));
        return ctypes2numpy(ret, olen.value, 'float32')

    def extract(self, data, name):
        """Extract feature from data
        Parameters
            data: iter or numpy ndarray
            name: node name to be extracted
        Return
            feature in numpy array
        """
        oshape = (ctypes.c_uint * 4)()
        if isinstance(data, DataIter):
            data.check_valid()
            ret = cxnlib.CXNNetExtractIter(self.handle,
                                           data.handle,
                                           ctypes.c_char_p(name.encode('utf-8')),
                                           oshape);
        elif isinstance(data, numpy.ndarray):
            if data.ndim != 4:
                raise Exception('need 4 dimensional tensor to use extract')
            ret = cxnlib.CXNNetExtractBatch(self.handle,
                                            data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),
                                            shape2ctypes(data),
                                            ctypes.c_char_p(name.encode('utf-8')),
                                            oshape)
        return ctypes2numpyT(ret, [x for x in oshape], 'float32')

    def set_weight(self, weight, layer_name, tag):
        """Set weight for special layer
        Parameters
            weight: new weight array
            layer_name: layer to be set
            tag: bias or wmat
        """
        if tag != 'bias' and tag != 'wmat':
            raise Exception('tag must be bias or wmat')
        cxnlib.CXNNetSetWeight(self.handle,
                               weight.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),
                               weight.size,
                               ctypes.c_char_p(layer_name.encode('utf-8')),
                               ctypes.c_char_p(tag.encode('utf-8')))

    def get_weight(self, layer_name, tag):
        """Get weight array from layer
           Parameter
                layer_name: name of layer
                tag: bias or wmat
           return
                weight array

        """

        if tag != 'bias' and tag != 'wmat':
            raise Exception('tag must be bias or wmat')
        oshape = (ctypes.c_uint * 4)()
        odim = ctypes.c_uint()
        ret = cxnlib.CXNNetGetWeight(self.handle,
                                     ctypes.c_char_p(layer_name.encode('utf-8')),
                                     ctypes.c_char_p(tag.encode('utf-8')),
                                     oshape, ctypes.byref(odim))
        if odim.value == 0 or ret is None:
            return None
        return ctypes2numpyT(ret, [oshape[i] for i in range(odim.value)], 'float32')

def train(cfg, data, num_round, param, eval_data = None):
    net = Net(cfg = cfg)
    if isinstance(param, dict):
        param = param.items()
    for k, v in param:
        net.set_param(k, v)
    net.init_model()
    for r in range(num_round):
        net.start_round(r)
        data.before_first()
        scounter = 0
        while data.next():
            net.update(data)
            scounter += 1
            if scounter % 100  == 0:
                print '[%d] %d batch passed' % (r, scounter)
        if eval_data is not None:
            seval = net.evaluate(eval_data, 'eval')
        sys.stderr.write(seval + '\n')
    return net
def train(cfg, data, label, num_round, param):
    net = Net(cfg = cfg)
    if isinstance(param, dict):
        param = param.items()
    for k, v in param:
        net.set_param(k, v)
    net.init_model()
    for r in range(num_round):
        print "Training in round %d" % r
        net.start_round(r)
        net.update(data=data, label=label)
    return net


####$$$$ cxxnet-master\cxxnet-master\wrapper/cxxnet_wrapper.cpp
#include <sstream>
#include <string>
#include <mshadow/tensor.h>
#include "./cxxnet_wrapper.h"
#include "../src/utils/config.h"
#include "../src/nnet/nnet.h"
#include "../src/io/data.h"

namespace cxxnet {
class WrapperIterator {
 public:
  WrapperIterator(const char *s_cfg) : iter_(NULL) {
    std::string cstr(s_cfg); cstr += "\n";
    std::stringstream stream(cstr);
    utils::ConfigStreamReader cfg(stream);
    cfg.Init();
    int flag = 1;
    std::vector<std::pair<std::string, std::string> > itcfg;
    std::vector<std::pair<std::string, std::string> > defcfg;

    while (cfg.Next()) {
      const char *name = cfg.name();
      const char *val  = cfg.val();
      if (!strcmp(name, "iter") && !strcmp(val, "end")) {
        utils::Assert(flag != 0, "wrong configuration file");
        iter_  = cxxnet::CreateIterator(itcfg);
        flag = 0; itcfg.clear(); continue;
      }
      if (flag == 0) {
        defcfg.push_back(std::make_pair(std::string(name),
                                        std::string(val)));
      } else {
        itcfg.push_back(std::make_pair(std::string(name),
                                       std::string(val)));
      }
    }
    if (iter_ == NULL) {
      iter_  = cxxnet::CreateIterator(itcfg);
    }
    for (size_t i = 0; i < defcfg.size(); ++i) {
      iter_->SetParam(defcfg[i].first.c_str(),
                      defcfg[i].second.c_str());
    }
    iter_->Init();
  }
  ~WrapperIterator(void) {
    delete iter_;
  }
  inline void BeforeFirst() {
    iter_->BeforeFirst();
  }
  inline bool Next() {
    return iter_->Next();
  }
  inline const cxx_real_t *GetData(cxx_uint dshape[4], cxx_uint *p_stride) const {
    const DataBatch &batch = iter_->Value();
    for (index_t i = 0; i < 4; ++i) {
      dshape[i] = batch.data.size(i);
    }
    *p_stride = batch.data.stride_;
    return batch.data.dptr_;
  }
  inline const cxx_real_t *GetLabel(cxx_uint lshape[4], cxx_uint *p_stride) const {
    const DataBatch &batch = iter_->Value();
    for (index_t i = 0; i < 2; ++i) {
      lshape[i] = batch.label.size(i);
    }
    *p_stride = batch.label.stride_;
    return batch.label.dptr_;    
  }

 private:
  friend class WrapperNet;
  IIterator<DataBatch> *iter_;
};

class WrapperNet {
 public:
  WrapperNet(const char *device, const char *s_cfg)
      : res_pred(false), temp2(false),
        temp4(false), net_(NULL) {
    device = "gpu";
    net_type = 0;
    silent = 0;
    print_step = 100;
    this->Configure(s_cfg);
    if (device != NULL && device[0] != '\0') {
      this->SetParam("dev", device);
    }
  }
  ~WrapperNet(void) {
    delete net_;
  }
  inline void SetParam(const char *name, const char *val) {
    if (!strcmp(name, "dev")) device_type_ = val;
    if (!strcmp(name,"net_type") && net_ != NULL) {
      net_type = atoi(val); return;
    }
    if (!strcmp(name, "silent")) {
      silent = atoi(val); return;
    }
    if (!strcmp(name, "print_step")) {
      print_step = atoi(val); return;
    }
    if (net_ != NULL) net_->SetParam(name, val);
    cfg.push_back(std::make_pair(std::string(name),
                                 std::string(val)));
  }
  inline void InitModel(void) {
    if (net_ != NULL) delete net_;
    net_ = this->CreateNet();
    net_->InitModel();
  }
  // load model from file
  inline void LoadModel(const char *fname) {
    if (net_ != NULL) delete net_;
    FILE *fi = utils::FopenCheck(fname, "rb");
    utils::FileStream fs(fi);
    utils::Check(fs.Read(&net_type, sizeof(int)) != 0, "LoadModel");
    net_ = this->CreateNet();
    net_->LoadModel(fs);
    fclose(fi);
  }
  // save model into file
  inline void SaveModel(const char *fname) {
    FILE *fo  = utils::FopenCheck(fname, "wb");
    utils::FileStream fs(fo);
    fs.Write(&net_type, sizeof(int));
    net_->SaveModel(fs);
    fclose(fo);
  }
  inline void StartRound(int round) {
    round_counter = round;
  }
  inline cxx_real_t *GetWeight(const char *layer_name,
                               const char *wtag,
                               cxx_uint wshape[4],
                               cxx_uint *out_dim) {
    std::vector<index_t> shape;
    net_->GetWeight(&temp2, &shape, layer_name, wtag);
    *out_dim = static_cast<cxx_uint>(shape.size());
    if (shape.size() == 0) return NULL;
    utils::Check(shape.size() <= 4, "GetWeight only works for dim<=4");
    for (size_t i = 0; i < shape.size(); ++i) {
      wshape[i] = shape[i];
    }
    return temp2.dptr_;
  }
  inline cxx_real_t *Extract(const DataBatch &batch,
                             const char *node_name,
                             cxx_uint oshape[4]) {
    net_->ExtractFeature(&temp4, batch, node_name);
    for (int i = 0; i < 4; ++i) {
      oshape[i] = temp4.size(i);
    }
    return temp4.dptr_;
  }
  inline cxx_real_t *Extract(WrapperIterator *iter,
                             const char *node_name,
                             cxx_uint oshape[4]) {
    return this->Extract(iter->iter_->Value(), node_name, oshape);
  }
  inline void UpdateIter(WrapperIterator *iter) {
    net_->Update(iter->iter_->Value());
  }
  inline cxx_real_t *Predict(const DataBatch &batch, cxx_uint *out_size) {
    net_->Predict(&res_pred, batch);
    *out_size = static_cast<cxx_uint>(res_pred.size(0));
    return res_pred.dptr_;
  }
  inline cxx_real_t *PredictIter(WrapperIterator *iter, cxx_uint *out_size) {
    return Predict(iter->iter_->Value(), out_size);
  }
  inline const char *Evaluate(WrapperIterator *iter, const char *data_name) {
    res_eval = net_->Evaluate(iter->iter_, data_name);
    return res_eval.c_str();
  }
  // return the net
  inline nnet::INetTrainer *net(void) {
    return net_;
  }

 protected:
  // returning cache
  std::string res_eval;
  mshadow::TensorContainer<mshadow::cpu, 1> res_pred;
  mshadow::TensorContainer<mshadow::cpu, 2> temp2;
  mshadow::TensorContainer<mshadow::cpu, 4> temp4;
 private:
  // the internal net
  nnet::INetTrainer *net_;
  /*! \brief all the configurations */
  std::vector<std::pair< std::string, std::string> > cfg;
  /*! \brief  device of the trainer */
  std::string device_type_;
  /*! \brief type of net implementation */
  int net_type;
  // silence sign
  int silent;
  // print step
  int print_step;
  // rounter counter
  int round_counter;

  inline void Configure(const char *s_cfg) {
    std::string cstr(s_cfg); cstr += "\n";
    std::stringstream sstream(cstr);
    utils::ConfigStreamReader cfg(sstream);
    cfg.Init();
    while (cfg.Next()) {
      this->SetParam(cfg.name(), cfg.val());
    }
  }
  // create a neural net
  inline nnet::INetTrainer *CreateNet(void) {
    nnet::INetTrainer *net;
    if (!strncmp(device_type_.c_str(), "gpu", 3)) {
#if MSHADOW_USE_CUDA
      net = nnet::CreateNet<mshadow::gpu>(net_type);
#else
      utils::Error("MSHADOW_USE_CUDA was not enabled");
#endif
    } else {
      net = nnet::CreateNet<mshadow::cpu>(net_type);
    }
    for (size_t i = 0; i < cfg.size(); ++ i) {
      net->SetParam(cfg[i].first.c_str(), cfg[i].second.c_str());
    }
    return net;
  }
};
}  // namespace cxxnet

using namespace cxxnet;

extern "C" {
  void *CXNIOCreateFromConfig(const char *cfg) {
    return new WrapperIterator(cfg);
  }
  int CXNIONext(void *handle) {
    return static_cast<WrapperIterator*>(handle)->Next();
  }
  void CXNIOBeforeFirst(void *handle) {
    static_cast<WrapperIterator*>(handle)->BeforeFirst();
  }
  const cxx_real_t *CXNIOGetData(void *handle,
                             cxx_uint oshape[4],
                             cxx_uint *ostride) {
    return static_cast<WrapperIterator*>(handle)->GetData(oshape, ostride);
  }
  const cxx_real_t *CXNIOGetLabel(void *handle,
                              cxx_uint oshape[2],
                              cxx_uint *ostride) {
    return static_cast<WrapperIterator*>(handle)->GetLabel(oshape, ostride);
  }
  void CXNIOFree(void *handle) {
    delete static_cast<WrapperIterator*>(handle);
  }
  void *CXNNetCreate(const char *device, const char *cfg) {
    return new WrapperNet(device, cfg);
  }
  void CXNNetFree(void *handle) {
    delete static_cast<WrapperNet*>(handle);
  }
  void CXNNetSetParam(void *handle, const char *name, const char *val) {
    static_cast<WrapperNet*>(handle)->SetParam(name, val);
  }
  void CXNNetInitModel(void *handle) {
    static_cast<WrapperNet*>(handle)->InitModel();
  }
  void CXNNetSaveModel(void *handle, const char *fname) {
    static_cast<WrapperNet*>(handle)->SaveModel(fname);
  }
  void CXNNetLoadModel(void *handle, const char *fname) {
    static_cast<WrapperNet*>(handle)->LoadModel(fname);
  }
  void CXNNetStartRound(void *handle, int round) {
    static_cast<WrapperNet*>(handle)->StartRound(round);
  }
  void CXNNetSetWeight(void *handle,
                       cxx_real_t *p_weight,
                       cxx_uint size_weight,
                       const char *layer_name,
                       const char *wtag) {
    mshadow::Tensor<cpu, 2> weight(p_weight, mshadow::Shape2(1, size_weight));
    static_cast<WrapperNet*>(handle)->net()->SetWeight(weight, layer_name, wtag);
  }
  const cxx_real_t *CXNNetGetWeight(void *handle,
                                    const char *layer_name,
                                    const char *wtag,
                                    cxx_uint wshape[4],
                                    cxx_uint *out_dim) {
    return static_cast<WrapperNet*>(handle)->GetWeight(layer_name, wtag, wshape, out_dim);
  }
  void CXNNetUpdateIter(void *handle, void *data_handle) {
    static_cast<WrapperNet*>(handle)->
        UpdateIter(static_cast<WrapperIterator*>(data_handle));
  }
  void CXNNetUpdateBatch(void *handle,
                         cxx_real_t *p_data,
                         const cxx_uint dshape[4],
                         cxx_real_t *p_label,
                         const cxx_uint lshape[2]) {
    DataBatch batch;
    batch.label = mshadow::Tensor<cpu, 2>
        (p_label, mshadow::Shape2(lshape[0], lshape[1]));
    batch.batch_size = dshape[0];
    batch.data = mshadow::Tensor<cpu, 4>
        (p_data, mshadow::Shape4(dshape[0], dshape[1], dshape[2], dshape[3]));
    static_cast<WrapperNet*>(handle)->net()->Update(batch);
  }
  const cxx_real_t *CXNNetPredictBatch(void *handle,
                                       cxx_real_t *p_data,
                                       const cxx_uint dshape[4],
                                       cxx_uint *out_size) {
    DataBatch batch;
    batch.batch_size = dshape[0];
    batch.data = mshadow::Tensor<cpu, 4>
        (p_data, mshadow::Shape4(dshape[0], dshape[1], dshape[2], dshape[3]));
    return static_cast<WrapperNet*>(handle)->Predict(batch, out_size);
  }
  const cxx_real_t *CXNNetPredictIter(void *handle,
                                       void *data_handle,
                                       cxx_uint *out_size) {
    WrapperIterator* iter = static_cast<WrapperIterator*>(data_handle);
    return static_cast<WrapperNet*>(handle)->PredictIter(iter, out_size);
  }
  const cxx_real_t *CXNNetExtractBatch(void *handle,
                                       cxx_real_t *p_data,
                                       const cxx_uint dshape[4],
                                       const char *node_name,
                                       cxx_uint oshape[4]) {
    DataBatch batch;
    batch.batch_size = dshape[0];
    batch.data = mshadow::Tensor<cpu, 4>
        (p_data, mshadow::Shape4(dshape[0], dshape[1], dshape[2], dshape[3]));
    return static_cast<WrapperNet*>(handle)->Extract(batch, node_name, oshape);    
  }
  const cxx_real_t *CXNNetExtractIter(void *handle,
                                      void *data_handle,
                                      const char *node_name,
                                      cxx_uint oshape[4]) {
    return static_cast<WrapperNet*>(handle)->Extract
        (static_cast<WrapperIterator*>(data_handle), node_name, oshape);    
  }
  const char *CXNNetEvaluate(void *handle,
                             void *data_handle,
                             const char *data_name) {
    return static_cast<WrapperNet*>(handle)->
        Evaluate(static_cast<WrapperIterator*>(data_handle), data_name);
  }
}
####$$$$ cxxnet-master\cxxnet-master\wrapper/cxxnet_wrapper.h
#ifndef CXXNET_WRAPPER_H_
#define CXXNET_WRAPPER_H_
/*!
 * \file cxxnet_wrapper.h
 * \author Tianqi Chen
 * \brief a C style wrapper of cxxnet
 *  can be used to create wrapper of other languages
 */
#include "../src/global.h"
#ifdef _MSC_VER
#define CXXNET_DLL __declspec(dllexport)
#else
#define CXXNET_DLL
#endif
// manually define unsign long
typedef unsigned long cxx_ulong;
typedef unsigned int cxx_uint;
typedef cxxnet::real_t cxx_real_t;

#ifdef __cplusplus
extern "C" {
#endif
  /*!
   * \brief create an cxxnet io iterator from config string
   * \param cfg config string that contains the configuration about
   *        the iterator
   * \return the handle pointer to the iterator
   */
  CXXNET_DLL void *CXNIOCreateFromConfig(const char *cfg);
  /*!
   * \brief move iterator to next position
   * \param handle the handle to iterator
   * \return whether it can be moved
   */
  CXXNET_DLL int CXNIONext(void *handle);
  /*!
   * \brief call iterator.BeforeFirst
   * \param handle the handle to iterator
   */
  CXXNET_DLL void CXNIOBeforeFirst(void *handle);
  /*!
   * \brief call iterator.Value().data
   * \param handle the handle to iterator
   * \param oshape the shape of output
   * \param ostride the stride of the output tensor
   */
  CXXNET_DLL const cxx_real_t *CXNIOGetData(void *handle,
                                            cxx_uint oshape[4],
                                            cxx_uint *ostride);
  /*!
   * \brief call iterator.Value().label
   * \param handle the handle to iterator
   * \param oshape the shape of output
   * \param ostride the stride of the output tensor
   */
  CXXNET_DLL const cxx_real_t *CXNIOGetLabel(void *handle,
                                             cxx_uint oshape[2],
                                             cxx_uint *ostride);
  /*!
   * \brief free the cxxnet io iterator handle
   * \param handle the handle pointer to the data iterator
   */
  CXXNET_DLL void CXNIOFree(void *handle);
  /*!
   * \brief create a cxxnet neural net object
   * \param devcie the device type of the net, corresponds to parameter devices
   *        can be NULL, if it is NULL, device type wil be decided by config
   * \param cfg configuration string of the net
   */
  CXXNET_DLL void *CXNNetCreate(const char *device, const char *cfg);
  /*!
   * \brief free the cxxnet neural net handle
   * \param handle net handle
   */
  CXXNET_DLL void CXNNetFree(void *handle);
  /*!
   * \brief set additional parameter to cxxnet
   * \param handle net handle
   * \param name name of parameter
   * \param val the value of parameter
   */
  CXXNET_DLL void CXNNetSetParam(void *handle, const char *name, const char *val);
  /*!
   * \brief initialize
   * \param handle net handle
   */
  CXXNET_DLL void CXNNetInitModel(void *handle);
  /*!
   * \brief save model into existing file
   * \param handle handle
   * \param fname file name
   */
  CXXNET_DLL void CXNNetSaveModel(void *handle, const char *fname);
  /*!
   * \brief load model from model file
   * \param handle net handle
   * \param fname file name
   */
  CXXNET_DLL void CXNNetLoadModel(void *handle, const char *fname);
  /*!
   * \brief inform the updater that a new round has been started
   * \param handle net handle
   * \param round round counter
   */
  CXXNET_DLL void CXNNetStartRound(void *handle, int round);
  /*!
   * \brief set weight by inputing an flattened array with same layout as original weight
   * \param handle net handle
   * \param p_weight pointer to the weight
   * \param size_weight size of the weight
   * \param layer_name the name of the layer
   * \param wtag the tag of weight, can be bias or wmat
   */
  CXXNET_DLL void CXNNetSetWeight(void *handle,
                                  cxx_real_t *p_weight,
                                  cxx_uint size_weight,
                                  const char *layer_name,
                                  const char *wtag);
  /*!
   * \brief get weight out
   * \param handle net handle
   * \param layer_name the name of the layer
   * \param wtag the tag of weight, can be bias or wmat
   * \param wshape the array holding output shape, weight can be maximumly 4 dim
   * \param out_dim the place holding dimension of output
   * \return the pointer to contiguous space of weight,
   *    can be NULL if weight do not exist
   */
  CXXNET_DLL const cxx_real_t *
  CXNNetGetWeight(void *handle,
                  const char *layer_name,
                  const char *wtag,
                  cxx_uint wshape[4],
                  cxx_uint *out_dim);
                                  
  /*!
   * \brief update the model, using current position on iterator
   * \param handle net handle
   * \param data_handle the data iterator handle
   */
  CXXNET_DLL void CXNNetUpdateIter(void *handle,
                                   void *data_handle);
  /*!
   * \brief update the model using one batch of image
   * \param handle net handle
   * \param p_data pointer to the data tensor, shape=(nbatch, nchannel, height, width)
   * \param dshape shape of input batch
   * \param p_label pointer to the label field, shape=(nbatch, label_width)
   * \param lshape shape of input label
   */
  CXXNET_DLL void CXNNetUpdateBatch(void *handle,
                                    cxx_real_t *p_data,
                                    const cxx_uint dshape[4],
                                    cxx_real_t *p_label,
                                    const cxx_uint lshape[2]);
  /*!
   * \brief make a prediction
   * \param handle net handle
   * \param p_data pointer to the data tensor, shape=(nbatch, nchannel, height, width)
   * \param dshape shape of input batch
   * \param out_size the final size of output label
   *
   * \return the pointer to the result field, the caller must copy the result out
   *         before calling any other cxxnet functions
   */
  CXXNET_DLL const cxx_real_t *
  CXNNetPredictBatch(void *handle,
                     cxx_real_t *p_data,
                     const cxx_uint dshape[4],
                     cxx_uint *out_size);
  /*!
   * \brief make a prediction based on iterator input
   * \param handle net handle
   * \param data_handle
   *
   * \return the pointer to the result field, the caller must copy the result out
   *         before calling any other cxxnet functions
   */  
  CXXNET_DLL const cxx_real_t *CXNNetPredictIter(void *handle,
                                                 void *data_handle,
                                                 cxx_uint *out_size);
  /*!
   * \brief make a feature extraction based on node name
   * \param handle net handle
   * \param p_data pointer to the data tensor, shape=(nbatch, nchannel, height, width)
   * \param dshape shape of input batch
   * \param out_size the final size of output label
   * \param node_name name of the node to be get feature from
   * \param oshape the shape out extracted data
   *
   * \return the pointer to the result field, the caller must copy the result out
   *         before calling any other cxxnet functions
   */
  CXXNET_DLL const cxx_real_t *
  CXNNetExtractBatch(void *handle,
                     cxx_real_t *p_data,
                     const cxx_uint dshape[4],
                     const char *node_name,
                     cxx_uint oshape[4]);
  /*!
   * \brief make a prediction based on iterator input
   * \param handle net handle
   * \param data_handle 
   * \param node_name name of the node to be get feature from
   * \param oshape the shape out extracted data

   * \return the pointer to the result field, the caller must copy the result out
   *         before calling any other cxxnet functions
   */  
  CXXNET_DLL const cxx_real_t *CXNNetExtractIter(void *handle,
                                                 void *data_handle,
                                                 const char *node_name,
                                                 cxx_uint oshape[4]);
  /*!
   * \brief evaluate the net using the data source
   * \param handle net handle
   * \param data_handle the data iterator handle
   * \param data_name the name of data, used to attach to the result
   *
   * \return a string representing the evaluation result, user need to copy the result out
   *         before claling any other cxxnet function
   */
  CXXNET_DLL const char* CXNNetEvaluate(void *handle,
                                        void *data_handle,
                                        const char *data_name);
#ifdef __cplusplus
}
#endif
#endif  // CXXNET_WRAPPER_H_
